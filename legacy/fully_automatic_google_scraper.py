#!/usr/bin/env python3
"""
–£–°–ò–õ–ï–ù–ù–´–ô –ü–û–õ–ù–û–°–¢–¨–Æ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô GOOGLE –ü–ê–†–°–ï–†
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ Google
–ü–†–ò–û–†–ò–¢–ï–¢: –ú–∞–∫—Å–∏–º—É–º —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –∏–∑ –Ω–∏–∑–∫–æ–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π
"""

import os
import json
import time
import random
import logging
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
from urllib.parse import quote_plus, urljoin, urlparse, parse_qs
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
import urllib3
import hashlib
import base64
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    import undetected_chromedriver as uc  # type: ignore
    UC_AVAILABLE = True
except ImportError:
    UC_AVAILABLE = False

class FullyAutomaticGoogleScraper:
    def __init__(self):
        self.session = requests.Session()
        self.results = []
        self.processed_urls = set()
        self.failed_searches = []
        self.blocked_count = 0
        self.success_count = 0
        self.logger = self._setup_logging()
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò
        self.MIN_PHONE_PERCENTAGE = 85  # 85% —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏
        self.MAX_PAGES_PER_QUERY = 5    # –°—Ç—Ä–∞–Ω–∏—Ü—ã 1-5 (—Ñ–æ–∫—É—Å –Ω–∞ 2-5)
        self.MIN_RESULTS_PER_CITY = 50  # –ú–∏–Ω–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –≥–æ—Ä–æ–¥
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
        self.phone_patterns = [
            # tel: —Å—Å—ã–ª–∫–∏ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
            re.compile(r'tel:[\s]*\+?1?[\s]*\(?(\d{3})\)?[\s]*[-.\s]*(\d{3})[\s]*[-.\s]*(\d{4})', re.IGNORECASE),
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ US —Ñ–æ—Ä–º–∞—Ç—ã
            re.compile(r'\(?(\d{3})\)?[-.\s]*(\d{3})[-.\s]*(\d{4})(?!\d)'),
            re.compile(r'(\d{3})[-.\s]+(\d{3})[-.\s]+(\d{4})(?!\d)'),
            re.compile(r'1[-.\s]*\(?(\d{3})\)?[-.\s]*(\d{3})[-.\s]*(\d{4})(?!\d)'),
            
            # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            re.compile(r'(?:phone|call|tel|contact)[\s:]*\(?(\d{3})\)?[-.\s]*(\d{3})[-.\s]*(\d{4})', re.IGNORECASE),
            
            # –°–∫—Ä—ã—Ç—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –∏ data –ø–æ–ª—è
            re.compile(r'(?:data-phone|data-tel)[\s]*=[\s]*["\'][\s]*\(?(\d{3})\)?[-.\s]*(\d{3})[-.\s]*(\d{4})', re.IGNORECASE),
        ]
        
        # –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –æ—Ö–≤–∞—Ç–∞
        self.search_queries = [
            'scrap metal buyers',
            'metal recycling center', 
            'scrap yard near me',
            'junk yard',
            'auto salvage yard',
            'copper scrap buyers',
            'aluminum recycling',
            'steel scrap dealers',
            'metal scrap pickup',
            'recycling center',
            'salvage auto parts',
            'scrap metal dealers'
        ]
        
        # –¶–µ–ª–µ–≤—ã–µ –≥–æ—Ä–æ–¥–∞ (—Å—Ä–µ–¥–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ —Å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º)
        self.target_cities = [
            'Akron OH', 'Toledo OH', 'Dayton OH', 'Youngstown OH', 'Canton OH',
            'Rochester NY', 'Syracuse NY', 'Buffalo NY', 'Albany NY', 'Schenectady NY',
            'Scranton PA', 'Allentown PA', 'Reading PA', 'Erie PA', 'Bethlehem PA',
            'Flint MI', 'Lansing MI', 'Kalamazoo MI', 'Grand Rapids MI', 'Saginaw MI',
            'Rockford IL', 'Peoria IL', 'Decatur IL', 'Springfield IL', 'Champaign IL',
            'Fort Wayne IN', 'Evansville IN', 'South Bend IN', 'Gary IN', 'Muncie IN',
            'Green Bay WI', 'Appleton WI', 'Oshkosh WI', 'Racine WI', 'Kenosha WI',
            'Cedar Rapids IA', 'Davenport IA', 'Sioux City IA', 'Waterloo IA',
            'Springfield MO', 'Columbia MO', 'Joplin MO', 'St. Joseph MO',
            'Little Rock AR', 'Fayetteville AR', 'Jonesboro AR', 'Pine Bluff AR'
        ]
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ User-Agents —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –±—Ä–∞—É–∑–µ—Ä–∞–º–∏
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0'
        ]
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏ (fallback)
        self.search_engines = [
            {
                'name': 'Google',
                'url': 'https://www.google.com/search',
                'params': {'q': '', 'start': 0},
                'priority': 1
            },
            {
                'name': 'Bing',
                'url': 'https://www.bing.com/search',
                'params': {'q': '', 'first': 0},
                'priority': 2
            },
            {
                'name': 'DuckDuckGo',
                'url': 'https://duckduckgo.com/html',
                'params': {'q': '', 's': 0},
                'priority': 3
            }
        ]
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏ —Å cookies
        self._init_session()
        # –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫–æ–≤–∏–∫ (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –Ω–∞ 'Google' –∏–ª–∏ 'DuckDuckGo')
        self.primary_engine = 'Bing'

    def _init_session(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏ —Å cookies –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞"""
        self.session.cookies.update({
            'NID': self._generate_google_cookie(),
            'CONSENT': 'YES+cb.20210328-17-p0.en+FX+667',
            'SOCS': 'CAESEwgDEgk0NzM5NDAzMjYaAmVuIAEaBgiA1YC4Bg',
            '1P_JAR': datetime.now().strftime('%Y-%m-%d-0%H'),
            'AEC': 'Ae3NU9O' + self._generate_random_string(32),
            'OGPC': '19031980-1:',
            'DV': self._generate_random_string(24)
        })

    def _generate_google_cookie(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ Google NID cookie"""
        timestamp = str(int(time.time()))
        random_part = self._generate_random_string(32)
        return f"511={timestamp}-{random_part}"

    def _generate_random_string(self, length):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
        import string
        return ''.join(random.choices(string.ascii_letters + string.digits, k=length))

    def _setup_logging(self):
        logger = logging.getLogger('FullyAutomaticGoogleScraper')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # Console handler
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
            
            # File handler
            file_handler = logging.FileHandler('google_scraper.log', encoding='utf-8')
            file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)
        
        return logger

    def run_complete_automation(self, target_businesses=1000):
        """–ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä –∏–∑ Google"""
        self.logger.info(f"ü§ñ –ó–ê–ü–£–°–ö –£–°–ò–õ–ï–ù–ù–û–ì–û –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ì–û –°–ë–û–†–ê")
        self.logger.info(f"üéØ –¶–ï–õ–¨: {target_businesses} –±–∏–∑–Ω–µ—Å–æ–≤ —Å {self.MIN_PHONE_PERCENTAGE}% —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤")
        self.logger.info(f"üõ°Ô∏è –ó–ê–©–ò–¢–ê: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫")
        
        start_time = time.time()
        
        # –≠—Ç–∞–ø 1: –ú–∞—Å—Å–æ–≤—ã–π —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∏–∑ Google —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
        all_google_links = self._enhanced_google_search()
        
        # –≠—Ç–∞–ø 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫
        prioritized_links = self._prioritize_links(all_google_links)
        
        # –≠—Ç–∞–ø 3: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
        businesses_with_contacts = self._parallel_contact_extraction(prioritized_links, target_businesses)
        
        # –≠—Ç–∞–ø 4: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å
        self.results = self._finalize_results(businesses_with_contacts, target_businesses)
        
        elapsed = time.time() - start_time
        phone_percentage = self._calculate_phone_percentage()
        
        self.logger.info(f"‚úÖ –£–°–ò–õ–ï–ù–ù–´–ô –°–ë–û–† –ó–ê–í–ï–†–®–ï–ù –∑–∞ {elapsed/60:.1f} –º–∏–Ω—É—Ç")
        self.logger.info(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢: {len(self.results)} –±–∏–∑–Ω–µ—Å–æ–≤, {phone_percentage:.1f}% —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏")
        self.logger.info(f"üõ°Ô∏è –ë–õ–û–ö–ò–†–û–í–û–ö: {self.blocked_count}, –£–°–ü–ï–®–ù–û: {self.success_count}")
        
        return self.results

    def _enhanced_google_search(self):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π Google –ø–æ–∏—Å–∫ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
        self.logger.info(f"üîç –ù–∞—á–∞–ª–æ —É—Å–∏–ª–µ–Ω–Ω–æ–≥–æ Google –ø–æ–∏—Å–∫–∞")
        self.logger.info(f"üìç –ì–æ—Ä–æ–¥–∞: {len(self.target_cities)}")
        self.logger.info(f"üîé –ó–∞–ø—Ä–æ—Å—ã: {len(self.search_queries)}")
        self.logger.info(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü—ã: 2-5 (—Ñ–æ–∫—É—Å –Ω–∞ –Ω–∏–∑–∫–∏–µ –ø–æ–∑–∏—Ü–∏–∏)")
        
        all_links = []
        
        # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –≥–æ—Ä–æ–¥+–∑–∞–ø—Ä–æ—Å
            for city in self.target_cities[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ—Ä–æ–¥–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                for query in self.search_queries[:6]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
                    future = executor.submit(self._fast_search_city_query, city, query)
                    futures.append(future)
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
            completed = 0
            total_tasks = len(futures)
            
            for future in as_completed(futures, timeout=1800):  # 30 –º–∏–Ω—É—Ç –æ–±—â–∏–π —Ç–∞–π–º–∞—É—Ç
                try:
                    city_query_links = future.result(timeout=60)  # 1 –º–∏–Ω—É—Ç–∞ –Ω–∞ –∑–∞–¥–∞—á—É
                    all_links.extend(city_query_links)
                    completed += 1
                    
                    # –ü—Ä–æ–≥—Ä–µ—Å—Å
                    progress = (completed / total_tasks) * 100
                    block_rate = (self.blocked_count / (self.blocked_count + self.success_count)) * 100 if (self.blocked_count + self.success_count) > 0 else 0
                    self.logger.info(f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}% | –ë–ª–æ–∫–∏—Ä–æ–≤–æ–∫: {block_rate:.1f}% | –°—Å—ã–ª–æ–∫: {len(all_links)}")
                    
                    # –ï—Å–ª–∏ —Å–æ–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Å—ã–ª–æ–∫, –ø—Ä–µ—Ä—ã–≤–∞–µ–º
                    if len(all_links) >= 200:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è 50 –±–∏–∑–Ω–µ—Å–æ–≤
                        self.logger.info("üéØ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Å—ã–ª–æ–∫ —Å–æ–±—Ä–∞–Ω–æ, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–∏—Å–∫")
                        break
                        
                except Exception as e:
                    self.logger.error(f"Task failed: {e}")
                    continue
        
        unique_links = self._deduplicate_links(all_links)
        self.logger.info(f"üîó –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(unique_links)}")
        
        return unique_links

    def _fast_search_city_query(self, city, query):
        """–ë–´–°–¢–†–´–ô –ø–æ–∏—Å–∫ –ø–æ –æ–¥–Ω–æ–º—É –≥–æ—Ä–æ–¥—É –∏ –∑–∞–ø—Ä–æ—Å—É"""
        full_query = f"{query} {city}"
        links = []
        
        # –¢–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã 2-3 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ (—Å–∞–º—ã–µ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ã–µ)
        for page in [2, 3]:
            try:
                if self.primary_engine == 'Google':
                    page_links = self._fast_search_google_page(full_query, page)
                elif self.primary_engine == 'DuckDuckGo':
                    page_links = self._fast_search_duckduckgo_page(full_query, page)
                else:
                    page_links = self._fast_search_bing_page(full_query, page)
                
                if page_links:
                    links.extend(page_links)
                    self.success_count += 1
                else:
                    self.blocked_count += 1
                    # –ü—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–µ —Å—Ä–∞–∑—É –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤–∏–∫
                    alt_links = self._fast_alternative_search(full_query, page)
                    if alt_links:
                        links.extend(alt_links)
                
                # –ö–û–†–û–¢–ö–ò–ï –ø–∞—É–∑—ã –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                time.sleep(random.uniform(0.5, 1.5))
                
            except Exception as e:
                self.logger.debug(f"Fast search error {city} {query} p{page}: {e}")
                continue
        
        return links

    def _fast_search_bing_page(self, query, page):
        """–ë–´–°–¢–†–´–ô –ø–æ–∏—Å–∫ Bing"""
        try:
            start = (page - 1) * 10 + 1
            search_url = f"https://www.bing.com/search?q={quote_plus(query)}&first={start}"
            headers = self._get_fast_headers()
            
            response = self.session.get(search_url, headers=headers, timeout=15, verify=False)
            if response.status_code == 200:
                return self._extract_links_from_alternative_engine(response.text, query, page, 'Bing')
            else:
                return []
        except Exception:
            return []

    def _fast_search_google_page(self, query, page):
        """–ë–´–°–¢–†–´–ô –ø–æ–∏—Å–∫ Google"""
        try:
            start = (page - 1) * 10
            search_url = f"https://www.google.com/search?q={quote_plus(query)}&start={start}&hl=en&gl=us"
            headers = self._get_fast_headers()
            
            response = self.session.get(search_url, headers=headers, timeout=15, verify=False)
            if response.status_code == 200 and not self._is_captcha_page(response.text):
                return self._extract_links_from_google_page(response.text, query, page)
            else:
                return []
        except Exception:
            return []

    def _fast_search_duckduckgo_page(self, query, page):
        """–ë–´–°–¢–†–´–ô –ø–æ–∏—Å–∫ DuckDuckGo"""
        try:
            start = (page - 2) * 50
            search_url = f"https://duckduckgo.com/html/?q={quote_plus(query)}&s={start}"
            headers = self._get_fast_headers()
            
            response = self.session.get(search_url, headers=headers, timeout=15, verify=False)
            if response.status_code == 200:
                return self._extract_links_from_alternative_engine(response.text, query, page, 'DuckDuckGo')
            else:
                return []
        except Exception:
            return []

    def _fast_alternative_search(self, query, page):
        """–ë–´–°–¢–†–´–ô –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫"""
        # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏ –±—ã—Å—Ç—Ä–æ
        for engine_name in ['DuckDuckGo', 'Bing', 'Google']:
            if engine_name == self.primary_engine:
                continue
            try:
                if engine_name == 'Bing':
                    links = self._fast_search_bing_page(query, page)
                elif engine_name == 'DuckDuckGo':
                    links = self._fast_search_duckduckgo_page(query, page)
                else:
                    links = self._fast_search_google_page(query, page)
                
                if links:
                    return links
            except Exception:
                continue
        return []

    def _get_fast_headers(self):
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache'
        }

    def _page_delay(self):
        """–ë–´–°–¢–†–ê–Ø –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏"""
        time.sleep(random.uniform(0.2, 0.8))  # –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–∞—É–∑—ã

    def _enhanced_adaptive_delay(self):
        """–ë–´–°–¢–†–ê–Ø –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞"""
        time.sleep(random.uniform(0.1, 0.5))  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—É–∑—ã

    def _prioritize_links(self, all_links):
        """–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        self.logger.info(f"üìä –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è {len(all_links)} —Å—Å—ã–ª–æ–∫")
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        unique_links = {}
        for link in all_links:
            url = link['url']
            if url not in unique_links:
                unique_links[url] = link
        
        links_list = list(unique_links.values())
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        def priority_score(link):
            score = 0
            title_desc = (link['title'] + ' ' + link['description']).lower()
            
            # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –ø—Ä—è–º—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            if any(word in title_desc for word in ['phone', 'call', 'contact']):
                score += 10
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ —Ç–∏–ø—É –±–∏–∑–Ω–µ—Å–∞
            if 'scrap metal' in title_desc:
                score += 8
            elif any(word in title_desc for word in ['scrap', 'recycling', 'salvage']):
                score += 5
                
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü 2-3 (–±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ, –Ω–æ –Ω–µ —Ç–æ–ø)
            if link['page'] in [2, 3]:
                score += 3
            elif link['page'] in [4, 5]:
                score += 1
                
            return score
        
        prioritized = sorted(links_list, key=priority_score, reverse=True)
        
        self.logger.info(f"üìà –°—Å—ã–ª–∫–∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã: {len(prioritized)}")
        return prioritized

    def _parallel_contact_extraction(self, links, target_businesses):
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤"""
        self.logger.info(f"üìû –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –∏–∑ {len(links)} —Å—Å—ã–ª–æ–∫")
        
        businesses = []
        processed_count = 0
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        with ThreadPoolExecutor(max_workers=8) as executor:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏
            futures = {
                executor.submit(self._extract_business_contacts, link): link 
                for link in links[:min(len(links), target_businesses * 3)]  # –ë–µ—Ä–µ–º —Å –∑–∞–ø–∞—Å–æ–º
            }
            
            for future in as_completed(futures):
                try:
                    business = future.result(timeout=30)
                    processed_count += 1
                    
                    if business and business.get('phone'):
                        businesses.append(business)
                        self.logger.info(f"‚úÖ [{len(businesses)}] {business['name']}: {business['phone']}")
                    else:
                        link = futures[future]
                        self.logger.debug(f"‚ùå [{processed_count}] {link.get('title', 'Unknown')}: –Ω–µ—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–∞")
                    
                    # –ü—Ä–æ–≥—Ä–µ—Å—Å
                    if processed_count % 50 == 0:
                        phone_rate = len(businesses) / processed_count * 100 if processed_count > 0 else 0
                        self.logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}, —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏: {len(businesses)} ({phone_rate:.1f}%)")
                    
                    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ —Ü–µ–ª–∏
                    if len(businesses) >= target_businesses:
                        self.logger.info(f"üéØ –¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞: {len(businesses)} –±–∏–∑–Ω–µ—Å–æ–≤ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏")
                        break
                        
                except Exception as e:
                    self.logger.debug(f"Future processing error: {e}")
                    continue
        
        final_phone_rate = len(businesses) / processed_count * 100 if processed_count > 0 else 0
        self.logger.info(f"üìä –ò–¢–û–ì–û: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count}, –Ω–∞–π–¥–µ–Ω–æ {len(businesses)} —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏ ({final_phone_rate:.1f}%)")
        
        return businesses

    def _extract_business_contacts(self, link_data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ —Å –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞ –±–∏–∑–Ω–µ—Å–∞"""
        url = link_data['url']
        
        try:
            # –ó–∞–ø—Ä–æ—Å –∫ —Å–∞–π—Ç—É —Å —Ä–æ—Ç–∞—Ü–∏–µ–π –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            headers = self._get_rotating_headers()
            response = self.session.get(url, headers=headers, timeout=20, verify=False)
            
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            page_text = response.text
            
            # –¢–û–ß–ù–´–ô –ø–æ–∏—Å–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π
            phone_results = self._ultra_aggressive_phone_search(page_text, soup)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Ç–µ–ª–µ—Ñ–æ–Ω
            valid_phone = None
            best_confidence = 0
            phone_method = 'unknown'
            
            for phone_data in phone_results:
                if phone_data['confidence'] > best_confidence:
                    valid_phone = phone_data['phone']
                    best_confidence = phone_data['confidence']
                    phone_method = phone_data['method']
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –í–ê–õ–ò–î–ù–û–ì–û —Ç–µ–ª–µ—Ñ–æ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if not valid_phone:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –±–∏–∑–Ω–µ—Å–µ
            business = {
                'name': self._extract_business_name(link_data, soup),
                'phone': valid_phone,
                'website': url,
                'email': self._extract_email_advanced(page_text, soup),
                'address': self._extract_address_comprehensive(soup, page_text),
                'city': self._extract_city_advanced(soup, page_text),
                'state': self._extract_state_advanced(soup, page_text),
                'zip_code': self._extract_zip_advanced(soup, page_text),
                'business_hours': self._extract_hours_comprehensive(soup),
                'services': self._extract_services_advanced(page_text),
                'materials': self._extract_materials_advanced(page_text),
                'description': self._extract_description_advanced(soup),
                'google_title': link_data.get('title', ''),
                'google_description': link_data.get('description', ''),
                'google_query': link_data.get('query', ''),
                'google_page': link_data.get('page', 0),
                'google_position': link_data.get('position', 0),
                'phone_extraction_method': phone_method,
                'phone_confidence': best_confidence,
                'source': 'Google_Auto',
                'scraped_at': datetime.now().isoformat(),
                'quality_score': self._calculate_quality_score({})
            }
            
            # –û–±–Ω–æ–≤–ª—è–µ–º quality_score –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
            business['quality_score'] = self._calculate_quality_score(business)
            
            return business
            
        except Exception as e:
            self.logger.debug(f"Error extracting from {url}: {e}")
            return None

    def _ultra_aggressive_phone_search(self, page_text, soup):
        """–£–ª—å—Ç—Ä–∞-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å confidence scores"""
        phone_results = []
        
        # –ú–µ—Ç–æ–¥ 1: tel: —Å—Å—ã–ª–∫–∏ (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        tel_links = soup.find_all('a', href=lambda x: x and x.startswith('tel:'))
        for link in tel_links:
            tel_value = link.get('href', '').replace('tel:', '').strip()
            phone = self._clean_phone(tel_value)
            if phone and self._validate_phone_advanced(phone):
                phone_results.append({
                    'phone': phone,
                    'confidence': 95,
                    'method': 'tel_link'
                })
        
        # –ú–µ—Ç–æ–¥ 2: –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        phone = self._extract_from_structured_data(soup)
        if phone and self._validate_phone_advanced(phone):
            phone_results.append({
                'phone': phone,
                'confidence': 90,
                'method': 'structured_data'
            })
        
        # –ú–µ—Ç–æ–¥ 3: data-* –∞—Ç—Ä–∏–±—É—Ç—ã
        data_phone_elements = soup.find_all(attrs=lambda x: x and any('phone' in attr.lower() or 'tel' in attr.lower() for attr in x))
        for element in data_phone_elements:
            for attr, value in element.attrs.items():
                if 'phone' in attr.lower() or 'tel' in attr.lower():
                    phone = self._clean_phone(str(value))
                    if phone and self._validate_phone_advanced(phone):
                        phone_results.append({
                            'phone': phone,
                            'confidence': 85,
                            'method': 'data_attribute'
                        })
        
        # –ú–µ—Ç–æ–¥ 4: JavaScript –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        script_tags = soup.find_all('script')
        for script in script_tags:
            if script.string:
                script_text = script.string
                js_phone_patterns = [
                    re.compile(r'phone["\'\s]*[:=]["\'\s]*(\d{3}[-.\s]*\d{3}[-.\s]*\d{4})', re.IGNORECASE),
                    re.compile(r'telephone["\'\s]*[:=]["\'\s]*(\d{3}[-.\s]*\d{3}[-.\s]*\d{4})', re.IGNORECASE),
                ]
                
                for pattern in js_phone_patterns:
                    matches = pattern.findall(script_text)
                    for match in matches:
                        phone = self._clean_phone(match)
                        if phone and self._validate_phone_advanced(phone):
                            phone_results.append({
                                'phone': phone,
                                'confidence': 80,
                                'method': 'javascript_var'
                            })
        
        # –ú–µ—Ç–æ–¥ 5: –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        phone_containers = soup.find_all(class_=re.compile(r'phone|tel|contact', re.IGNORECASE))
        for container in phone_containers:
            text = container.get_text()
            phone = self._extract_phone_from_text(text)
            if phone and self._validate_phone_advanced(phone):
                phone_results.append({
                    'phone': phone,
                    'confidence': 75,
                    'method': 'css_class'
                })
        
        # –ú–µ—Ç–æ–¥ 6: –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –≤ HTML
        for i, pattern in enumerate(self.phone_patterns):
            matches = pattern.findall(page_text)
            for match in matches:
                phone = self._format_phone_match(match)
                if phone and self._validate_phone_advanced(phone):
                    phone_results.append({
                        'phone': phone,
                        'confidence': 70,
                        'method': f'html_pattern_{i+1}'
                    })
        
        # –ú–µ—Ç–æ–¥ 7: OCR-–ø–æ–¥–æ–±–Ω—ã–π –ø–æ–∏—Å–∫ (–ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞)
        text_only = soup.get_text()
        cleaned_text = re.sub(r'[^\d\s\-\(\)\.]+', ' ', text_only)
        potential_phones = re.findall(r'(\d{3}[\s\-\(\)\.]*\d{3}[\s\-\(\)\.]*\d{4})', cleaned_text)
        
        for potential in potential_phones:
            phone = self._clean_phone(potential)
            if phone and self._validate_phone_advanced(phone):
                phone_results.append({
                    'phone': phone,
                    'confidence': 60,
                    'method': 'text_mining'
                })
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ confidence
        unique_phones = {}
        for result in phone_results:
            phone_key = result['phone']
            if phone_key not in unique_phones or result['confidence'] > unique_phones[phone_key]['confidence']:
                unique_phones[phone_key] = result
        
        return list(unique_phones.values())

    def _extract_from_structured_data(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        # JSON-LD
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                phone = self._extract_phone_from_json_ld(data)
                if phone:
                    cleaned_phone = self._clean_phone(phone)
                    if cleaned_phone:
                        return cleaned_phone
            except:
                continue
        
        # Microdata
        microdata_elements = soup.find_all(attrs={'itemprop': True})
        for element in microdata_elements:
            itemprop = element.get('itemprop', '').lower()
            if 'telephone' in itemprop or 'phone' in itemprop:
                content = element.get('content') or element.get_text()
                phone = self._clean_phone(content)
                if phone:
                    return phone
        
        return None

    def _extract_phone_from_json_ld(self, data):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            # –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫
            for key in ['telephone', 'phone', 'contactPoint']:
                if key in data:
                    value = data[key]
                    if isinstance(value, str):
                        return value  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º raw –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ—á–∏—Å—Ç–∫–∏
                    elif isinstance(value, dict) and 'telephone' in value:
                        return value['telephone']  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º raw –∑–Ω–∞—á–µ–Ω–∏–µ
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
            for value in data.values():
                if isinstance(value, (dict, list)):
                    phone = self._extract_phone_from_json_ld(value)
                    if phone:
                        return phone
        
        elif isinstance(data, list):
            for item in data:
                phone = self._extract_phone_from_json_ld(item)
                if phone:
                    return phone
        
        return None

    def _validate_phone_advanced(self, phone):
        """–°–¢–†–û–ì–ê–Ø –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        if not phone:
            return False
        
        digits = re.sub(r'\D', '', phone)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 10 –∏–ª–∏ 11 —Ü–∏—Ñ—Ä)
        if len(digits) not in [10, 11]:
            return False
        
        # –ï—Å–ª–∏ 11 —Ü–∏—Ñ—Ä, –ø–µ—Ä–≤–∞—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 1 (US country code)
        if len(digits) == 11:
            if not digits.startswith('1'):
                return False
            digits = digits[1:]  # –£–±–∏—Ä–∞–µ–º country code
        
        # –¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–æ–≤–Ω–æ 10 —Ü–∏—Ñ—Ä
        if len(digits) != 10:
            return False
        
        area_code = digits[:3]
        exchange_code = digits[3:6]
        subscriber_number = digits[6:]
        
        # –°–¢–†–û–ì–ò–ï –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è US –Ω–æ–º–µ—Ä–æ–≤
        
        # Area code –Ω–µ –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å 0 –∏–ª–∏ 1
        if area_code[0] in ['0', '1']:
            return False
        
        # Exchange code –Ω–µ –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å 0 –∏–ª–∏ 1
        if exchange_code[0] in ['0', '1']:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ area codes
        invalid_area_codes = [
            '000', '111', '222', '333', '444', '555', '666', '777', '888', '999',
            '123', '321', '456', '654', '789', '987',
            # Toll-free numbers (–Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –º–µ—Å—Ç–Ω–æ–≥–æ –±–∏–∑–Ω–µ—Å–∞)
            '800', '833', '844', '855', '866', '877', '888'
        ]
        if area_code in invalid_area_codes:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–ª—É–∂–µ–±–Ω—ã–µ –Ω–æ–º–µ—Ä–∞
        if exchange_code + subscriber_number in ['911', '411', '511', '611', '711', '811', '311']:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã–µ –Ω–æ–º–µ—Ä–∞ (555-01xx)
        if exchange_code == '555' and subscriber_number.startswith('01'):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
        full_number = area_code + exchange_code + subscriber_number
        
        # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Ü–∏—Ñ—Ä –ø–æ–¥—Ä—è–¥
        for i in range(len(full_number) - 3):
            if len(set(full_number[i:i+4])) == 1:  # 4 –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ü–∏—Ñ—Ä—ã –ø–æ–¥—Ä—è–¥
                return False
        
        # –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–∏—Ñ—Ä –≤ –Ω–æ–º–µ—Ä–µ
        if len(set(full_number)) < 4:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—á–µ–≤–∏–¥–Ω–æ —Ñ–∞–ª—å—à–∏–≤—ã–µ –Ω–æ–º–µ—Ä–∞
        fake_numbers = [
            '1234567890', '0123456789', '9876543210',
            '1111111111', '0000000000', '2222222222'
        ]
        if full_number in fake_numbers:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–∏—Ñ—Ä—ã (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–æ –ø–æ—Ä—è–¥–∫—É)
        sequential_count = 0
        for i in range(len(full_number) - 1):
            if int(full_number[i+1]) == int(full_number[i]) + 1:
                sequential_count += 1
                if sequential_count >= 4:  # 5 —Ü–∏—Ñ—Ä –ø–æ–¥—Ä—è–¥ –ø–æ –ø–æ—Ä—è–¥–∫—É
                    return False
            else:
                sequential_count = 0
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤—Å–µ—Ö –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Ü–∏—Ñ—Ä –≤ area code
        if len(set(area_code)) == 1:
            return False
        
        return True

    def _extract_phone_from_text(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        for pattern in self.phone_patterns:
            matches = pattern.findall(text)
            for match in matches:
                phone = self._format_phone_match(match)
                if phone and self._validate_phone_advanced(phone):
                    return phone
        return None

    def _format_phone_match(self, match):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        if isinstance(match, tuple) and len(match) >= 3:
            # –°–æ–µ–¥–∏–Ω—è–µ–º —á–∞—Å—Ç–∏ tuple –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
            phone_str = ''.join(str(part) for part in match[:3])
            return self._clean_phone(phone_str)
        elif isinstance(match, str):
            return self._clean_phone(match)
        return None

    def _clean_phone(self, phone):
        """–û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        if not phone:
            return None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã
        digits = re.sub(r'\D', '', str(phone))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É
        if len(digits) == 10:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π US —Ñ–æ—Ä–º–∞—Ç
            area_code = digits[:3]
            exchange = digits[3:6]
            number = digits[6:]
        elif len(digits) == 11 and digits[0] == '1':
            # US —Å –∫–æ–¥–æ–º —Å—Ç—Ä–∞–Ω—ã
            area_code = digits[1:4]
            exchange = digits[4:7]
            number = digits[7:]
        else:
            return None
        
        # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        if area_code[0] in ['0', '1'] or exchange[0] in ['0', '1']:
            return None
            
        return f"({area_code}) {exchange}-{number}"

    def _extract_business_name(self, link_data, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –±–∏–∑–Ω–µ—Å–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏"""
        # 1. –ò–∑ Google title (—á–∞—Å—Ç–æ —Å–∞–º–æ–µ —Ç–æ—á–Ω–æ–µ)
        google_title = link_data.get('title', '').strip()
        if google_title and len(google_title) > 3:
            # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–µ–≥–æ
            clean_title = re.sub(r'\s*[-|]\s*.+$', '', google_title)  # –£–±–∏—Ä–∞–µ–º " - –≥–æ—Ä–æ–¥" –∏ —Ç.–ø.
            if len(clean_title) > 3:
                return clean_title[:100]
        
        # 2. –ò–∑ title —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        title_tag = soup.find('title')
        if title_tag:
            title_text = title_tag.get_text().strip()
            clean_title = re.sub(r'\s*[-|]\s*.+$', '', title_text)
            if len(clean_title) > 3:
                return clean_title[:100]
        
        # 3. –ò–∑ H1
        h1 = soup.find('h1')
        if h1:
            h1_text = h1.get_text().strip()
            if len(h1_text) > 3:
                return h1_text[:100]
        
        # 4. –ò–∑ structured data
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                name = self._extract_name_from_json_ld(data)
                if name:
                    return name[:100]
            except:
                continue
        
        return "Unknown Business"

    def _extract_name_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            for key in ['name', 'legalName', 'alternateName']:
                if key in data and isinstance(data[key], str):
                    return data[key].strip()
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    name = self._extract_name_from_json_ld(value)
                    if name:
                        return name
        
        elif isinstance(data, list):
            for item in data:
                name = self._extract_name_from_json_ld(item)
                if name:
                    return name
        
        return ""

    def _extract_email_advanced(self, page_text, soup):
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ email"""
        # 1. –ò–∑ structured data
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                email = self._extract_email_from_json_ld(data)
                if email:
                    return email
            except:
                continue
        
        # 2. –ò–∑ mailto —Å—Å—ã–ª–æ–∫
        mailto_links = soup.find_all('a', href=lambda x: x and x.startswith('mailto:'))
        for link in mailto_links:
            email = link.get('href', '').replace('mailto:', '').strip()
            if self._validate_email(email):
                return email
        
        # 3. –ò–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        emails = email_pattern.findall(page_text)
        
        for email in emails:
            if self._validate_email(email):
                return email
        
        return None

    def _extract_email_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ email –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            for key in ['email', 'contactPoint']:
                if key in data:
                    value = data[key]
                    if isinstance(value, str) and '@' in value:
                        return value
                    elif isinstance(value, dict) and 'email' in value:
                        return value['email']
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    email = self._extract_email_from_json_ld(value)
                    if email:
                        return email
        
        elif isinstance(data, list):
            for item in data:
                email = self._extract_email_from_json_ld(item)
                if email:
                    return email
        
        return None

    def _validate_email(self, email):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è email"""
        if not email or '@' not in email:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        exclude_domains = [
            'example.com', 'test.com', 'google.com', 'facebook.com',
            'twitter.com', 'linkedin.com', 'youtube.com', 'instagram.com'
        ]
        
        email_lower = email.lower()
        for domain in exclude_domains:
            if domain in email_lower:
                return False
        
        # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
        if re.match(r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}$', email):
            return True
        
        return False

    def _extract_address_comprehensive(self, soup, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞"""
        # 1. –ò–∑ structured data
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                address = self._extract_address_from_json_ld(data)
                if address:
                    return address
            except:
                continue
        
        # 2. –ò–∑ microdata
        address_elements = soup.find_all(attrs={'itemprop': re.compile(r'address|streetAddress', re.I)})
        for element in address_elements:
            address = element.get('content') or element.get_text().strip()
            if len(address) > 10:
                return address[:200]
        
        # 3. –ò–∑ CSS –∫–ª–∞—Å—Å–æ–≤
        address_selectors = [
            '.address', '.location', '.contact-address', '.street-address',
            '.addr', '.business-address', '.company-address'
        ]
        
        for selector in address_selectors:
            elements = soup.select(selector)
            for element in elements:
                address = element.get_text().strip()
                if len(address) > 10 and any(word in address.lower() for word in ['street', 'ave', 'road', 'drive', 'blvd']):
                    return address[:200]
        
        return None

    def _extract_address_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            # –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –∞–¥—Ä–µ—Å–∞
            if 'address' in data:
                addr = data['address']
                if isinstance(addr, str):
                    return addr
                elif isinstance(addr, dict):
                    # –°–æ–±–∏—Ä–∞–µ–º –∞–¥—Ä–µ—Å –∏–∑ —á–∞—Å—Ç–µ–π
                    parts = []
                    for key in ['streetAddress', 'addressLocality', 'addressRegion', 'postalCode']:
                        if key in addr and addr[key]:
                            parts.append(str(addr[key]))
                    if parts:
                        return ', '.join(parts)
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
            for value in data.values():
                if isinstance(value, (dict, list)):
                    address = self._extract_address_from_json_ld(value)
                    if address:
                        return address
        
        elif isinstance(data, list):
            for item in data:
                address = self._extract_address_from_json_ld(item)
                if address:
                    return address
        
        return None

    def _extract_city_advanced(self, soup, page_text):
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞"""
        # 1. –ò–∑ structured data
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                city = self._extract_city_from_json_ld(data)
                if city:
                    return city
            except:
                continue
        
        # 2. –ò–∑ microdata
        city_elements = soup.find_all(attrs={'itemprop': 'addressLocality'})
        for element in city_elements:
            city = element.get('content') or element.get_text().strip()
            if city:
                return city
        
        # 3. –ò–∑ CSS –∫–ª–∞—Å—Å–æ–≤
        city_selectors = ['.city', '.locality', '.address-city']
        for selector in city_selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        
        return None

    def _extract_city_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            if 'address' in data and isinstance(data['address'], dict):
                addr = data['address']
                if 'addressLocality' in addr:
                    return addr['addressLocality']
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    city = self._extract_city_from_json_ld(value)
                    if city:
                        return city
        
        elif isinstance(data, list):
            for item in data:
                city = self._extract_city_from_json_ld(item)
                if city:
                    return city
        
        return None

    def _extract_state_advanced(self, soup, page_text):
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —à—Ç–∞—Ç–∞"""
        # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ city, –Ω–æ –∏—â–µ–º addressRegion
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                state = self._extract_state_from_json_ld(data)
                if state:
                    return state
            except:
                continue
        
        state_elements = soup.find_all(attrs={'itemprop': 'addressRegion'})
        for element in state_elements:
            state = element.get('content') or element.get_text().strip()
            if state:
                return state
        
        return None

    def _extract_state_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —à—Ç–∞—Ç–∞ –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            if 'address' in data and isinstance(data['address'], dict):
                addr = data['address']
                if 'addressRegion' in addr:
                    return addr['addressRegion']
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    state = self._extract_state_from_json_ld(value)
                    if state:
                        return state
        
        elif isinstance(data, list):
            for item in data:
                state = self._extract_state_from_json_ld(item)
                if state:
                    return state
        
        return None

    def _extract_zip_advanced(self, soup, page_text):
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ ZIP –∫–æ–¥–∞"""
        # –ò–∑ structured data
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                zip_code = self._extract_zip_from_json_ld(data)
                if zip_code:
                    return zip_code
            except:
                continue
        
        # –ò–∑ microdata
        zip_elements = soup.find_all(attrs={'itemprop': 'postalCode'})
        for element in zip_elements:
            zip_code = element.get('content') or element.get_text().strip()
            if zip_code:
                return zip_code
        
        # –ò–∑ —Ç–µ–∫—Å—Ç–∞ (–ø–æ–∏—Å–∫ ZIP –∫–æ–¥–æ–≤)
        zip_pattern = re.compile(r'\b\d{5}(?:-\d{4})?\b')
        matches = zip_pattern.findall(page_text)
        if matches:
            return matches[0]
        
        return None

    def _extract_zip_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ZIP –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            if 'address' in data and isinstance(data['address'], dict):
                addr = data['address']
                if 'postalCode' in addr:
                    return addr['postalCode']
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    zip_code = self._extract_zip_from_json_ld(value)
                    if zip_code:
                        return zip_code
        
        elif isinstance(data, list):
            for item in data:
                zip_code = self._extract_zip_from_json_ld(item)
                if zip_code:
                    return zip_code
        
        return None

    def _extract_hours_comprehensive(self, soup):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–±–æ—á–∏—Ö —á–∞—Å–æ–≤"""
        # 1. –ò–∑ structured data
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                hours = self._extract_hours_from_json_ld(data)
                if hours:
                    return hours
            except:
                continue
        
        # 2. –ò–∑ microdata
        hours_elements = soup.find_all(attrs={'itemprop': 'openingHours'})
        for element in hours_elements:
            hours = element.get('content') or element.get_text().strip()
            if hours:
                return hours[:200]
        
        # 3. –ò–∑ CSS –∫–ª–∞—Å—Å–æ–≤
        hours_selectors = [
            '.hours', '.business-hours', '.opening-hours', '.schedule',
            '.operation-hours', '.store-hours'
        ]
        
        for selector in hours_selectors:
            element = soup.select_one(selector)
            if element:
                hours = element.get_text().strip()
                if len(hours) > 10:
                    return hours[:200]
        
        return None

    def _extract_hours_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞—Å–æ–≤ –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            if 'openingHours' in data:
                hours = data['openingHours']
                if isinstance(hours, str):
                    return hours
                elif isinstance(hours, list):
                    return ', '.join(str(h) for h in hours)
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    hours = self._extract_hours_from_json_ld(value)
                    if hours:
                        return hours
        
        elif isinstance(data, list):
            for item in data:
                hours = self._extract_hours_from_json_ld(item)
                if hours:
                    return hours
        
        return ""

    def _extract_services_advanced(self, page_text):
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —É—Å–ª—É–≥"""
        services = []
        text_lower = page_text.lower()
        
        service_keywords = {
            'pickup': ['pickup', 'pick up', 'collection'],
            'container': ['container', 'dumpster', 'roll off'],
            'demolition': ['demolition', 'demo', 'tear down'],
            'processing': ['processing', 'sorting', 'separation'],
            'weighing': ['weighing', 'scale', 'certified scales'],
            'cash payment': ['cash', 'cash payment', 'pay cash'],
            'commercial': ['commercial', 'business', 'industrial'],
            'residential': ['residential', 'home', 'homeowner'],
            'auto dismantling': ['auto dismantling', 'car dismantling', 'vehicle dismantling']
        }
        
        for service, keywords in service_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                services.append(service)
        
        return services

    def _extract_materials_advanced(self, page_text):
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤"""
        materials = []
        text_lower = page_text.lower()
        
        material_keywords = {
            'copper': ['copper', 'cu '],
            'aluminum': ['aluminum', 'aluminium', 'al '],
            'steel': ['steel', 'iron', 'ferrous'],
            'brass': ['brass', 'bronze'],
            'stainless steel': ['stainless', 'stainless steel', 'ss '],
            'lead': ['lead', 'pb '],
            'zinc': ['zinc', 'zn '],
            'nickel': ['nickel', 'ni '],
            'wire': ['wire', 'cable', 'copper wire'],
            'auto parts': ['auto', 'car', 'vehicle', 'automotive'],
            'batteries': ['battery', 'batteries', 'car battery'],
            'radiators': ['radiator', 'radiators', 'cooling'],
            'catalytic converters': ['catalytic', 'converter', 'cat converter']
        }
        
        for material, keywords in material_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                materials.append(material)
        
        return materials

    def _extract_description_advanced(self, soup):
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è"""
        # 1. Meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            desc = meta_desc.get('content', '').strip()
            if len(desc) > 20:
                return desc[:300]
        
        # 2. –ò–∑ structured data
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                desc = self._extract_description_from_json_ld(data)
                if desc:
                    return desc[:300]
            except:
                continue
        
        # 3. –ü–µ—Ä–≤—ã–π —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        paragraphs = soup.find_all('p')
        for p in paragraphs:
            text = p.get_text().strip()
            if len(text) > 50 and any(word in text.lower() for word in ['scrap', 'metal', 'recycling']):
                return text[:300]
        
        return ""

    def _extract_description_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            for key in ['description', 'about']:
                if key in data and isinstance(data[key], str):
                    return data[key]
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    desc = self._extract_description_from_json_ld(value)
                    if desc:
                        return desc
        
        elif isinstance(data, list):
            for item in data:
                desc = self._extract_description_from_json_ld(item)
                if desc:
                    return desc
        
        return ""

    def _calculate_quality_score(self, business):
        """–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        score = 0
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –ø–æ–ª—è
        if business.get('phone'):
            score += 30
        if business.get('name') and len(business['name']) > 3:
            score += 20
        if business.get('website'):
            score += 15
        
        # –í–∞–∂–Ω—ã–µ –ø–æ–ª—è
        if business.get('email'):
            score += 10
        if business.get('address'):
            score += 10
        if business.get('city'):
            score += 5
        if business.get('state'):
            score += 5
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        if business.get('business_hours'):
            score += 3
        if business.get('services') and business['services']:
            score += 2
        
        return min(score, 100)

    def _get_rotating_headers(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–æ—Ç–∏—Ä—É–µ–º—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }

    def _get_enhanced_headers(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞"""
        ua = random.choice(self.user_agents)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –±—Ä–∞—É–∑–µ—Ä–∞ –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        if 'Chrome' in ua:
            sec_ch_ua = '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"'
            sec_ch_ua_platform = '"Windows"'
        elif 'Firefox' in ua:
            sec_ch_ua = None
            sec_ch_ua_platform = None
        else:
            sec_ch_ua = '"Not A(Brand";v="99", "Safari";v="17"'
            sec_ch_ua_platform = '"macOS"'
        
        headers = {
            'User-Agent': ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
            'Sec-GPC': '1'
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º Chrome-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        if sec_ch_ua:
            headers['sec-ch-ua'] = sec_ch_ua
            headers['sec-ch-ua-mobile'] = '?0'
            headers['sec-ch-ua-platform'] = sec_ch_ua_platform
        
        return headers

    def _is_captcha_page(self, html):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ CAPTCHA —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        captcha_indicators = [
            'captcha',
            'unusual traffic',
            'verify you are not a robot',
            'recaptcha',
            'g-recaptcha',
            'please complete the security check',
            'blocked by cloudflare',
            'ray id'
        ]
        
        html_lower = html.lower()
        return any(indicator in html_lower for indicator in captcha_indicators)

    def _adaptive_delay(self):
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏"""
        # –ë–∞–∑–æ–≤–∞—è –ø–∞—É–∑–∞
        base_delay = random.uniform(2, 5)
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É –µ—Å–ª–∏ –º–Ω–æ–≥–æ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if len(self.failed_searches) > 10:
            base_delay *= 2
        
        time.sleep(base_delay)

    def _enhanced_adaptive_delay(self):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
        # –ë–∞–∑–æ–≤–∞—è –ø–∞—É–∑–∞
        base_delay = random.uniform(2, 5)
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É –µ—Å–ª–∏ –º–Ω–æ–≥–æ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if len(self.failed_searches) > 10:
            base_delay *= 2
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É, –µ—Å–ª–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Å—Ç–∞–ª–∏ —á–∞—Å—Ç—ã–º–∏
        if self.blocked_count > 5:
            base_delay *= 1.5
        
        time.sleep(base_delay)

    def _page_delay(self):
        """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É, –µ—Å–ª–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Å—Ç–∞–ª–∏ —á–∞—Å—Ç—ã–º–∏
        if self.blocked_count > 5:
            time.sleep(random.uniform(5, 10))
        else:
            time.sleep(random.uniform(3, 8))

    def _deduplicate_links(self, links):
        """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫"""
        seen_urls = set()
        unique_links = []
        
        for link in links:
            url = link['url'].lower().strip()
            if url not in seen_urls:
                seen_urls.add(url)
                unique_links.append(link)
        
        return unique_links

    def _finalize_results(self, businesses, target_count):
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        sorted_businesses = sorted(businesses, key=lambda x: x.get('quality_score', 0), reverse=True)
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É
        final_results = sorted_businesses[:target_count]
        
        return final_results

    def _calculate_phone_percentage(self):
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–∞ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏"""
        if not self.results:
            return 0
        return (sum(1 for b in self.results if b.get('phone')) / len(self.results)) * 100

    def export_automated_results(self, output_dir="output"):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞"""
        if not self.results:
            return None
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total = len(self.results)
        with_phones = sum(1 for b in self.results if b.get('phone'))
        phone_percentage = (with_phones / total) * 100 if total > 0 else 0
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        extraction_methods = defaultdict(int)
        for business in self.results:
            if business.get('phone'):
                method = business.get('phone_extraction_method', 'unknown')
                extraction_methods[method] += 1
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ Google —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        page_distribution = defaultdict(int)
        for business in self.results:
            page = business.get('google_page', 0)
            page_distribution[f'Page {page}'] += 1
        
        # CSV —ç–∫—Å–ø–æ—Ä—Ç
        df = pd.DataFrame(self.results)
        csv_file = os.path.join(output_dir, f"automated_google_results_{timestamp}.csv")
        df.to_csv(csv_file, index=False)
        
        # Excel —Å –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π
        excel_file = os.path.join(output_dir, f"automated_google_results_{timestamp}.xlsx")
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            df.to_excel(writer, sheet_name='All Results', index=False)
            
            # –¢–æ–ª—å–∫–æ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏
            df_phones = df[df['phone'].notna() & (df['phone'] != '')]
            if not df_phones.empty:
                df_phones.to_excel(writer, sheet_name='With Phones', index=False)
            
            # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (score > 70)
            df_quality = df[df['quality_score'] > 70]
            if not df_quality.empty:
                df_quality.to_excel(writer, sheet_name='High Quality', index=False)
            
            # –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –º–µ—Ç–æ–¥–æ–≤
            methods_df = pd.DataFrame(list(extraction_methods.items()), columns=['Method', 'Count'])
            methods_df.to_excel(writer, sheet_name='Extraction Methods', index=False)
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            pages_df = pd.DataFrame(list(page_distribution.items()), columns=['Page', 'Count'])
            pages_df.to_excel(writer, sheet_name='Page Distribution', index=False)
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        report_file = os.path.join(output_dir, f"automated_report_{timestamp}.txt")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("ü§ñ –£–°–ò–õ–ï–ù–ù–´–ô –ü–û–õ–ù–û–°–¢–¨–Æ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô GOOGLE –ü–ê–†–°–ï–† - –û–¢–ß–ï–¢\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–ú–µ—Ç–æ–¥: –£—Å–∏–ª–µ–Ω–Ω—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π Google –ø–æ–∏—Å–∫\n")
            f.write(f"–ì–æ—Ä–æ–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(self.target_cities)}\n")
            f.write(f"–ü–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(self.search_queries)}\n")
            f.write(f"–°—Ç—Ä–∞–Ω–∏—Ü—ã: 2-5 (–Ω–∏–∑–∫–∏–µ –ø–æ–∑–∏—Ü–∏–∏)\n\n")
            
            f.write("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")
            f.write(f"–í—Å–µ–≥–æ –±–∏–∑–Ω–µ—Å–æ–≤: {total}\n")
            f.write(f"–° —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏: {with_phones} ({phone_percentage:.1f}%)\n")
            f.write(f"–¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞: {'‚úÖ –î–ê' if phone_percentage >= self.MIN_PHONE_PERCENTAGE else '‚ùå –ù–ï–¢'}\n\n")
            
            f.write("üõ°Ô∏è –ú–ï–¢–û–î–´ –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –¢–ï–õ–ï–§–û–ù–û–í:\n")
            for method, count in sorted(extraction_methods.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / with_phones) * 100 if with_phones > 0 else 0
                f.write(f"  {method}: {count} ({percentage:.1f}%)\n")
            f.write("\n")
            
            f.write("üìÑ –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú GOOGLE:\n")
            for page, count in sorted(page_distribution.items()):
                percentage = (count / total) * 100 if total > 0 else 0
                f.write(f"  {page}: {count} –±–∏–∑–Ω–µ—Å–æ–≤ ({percentage:.1f}%)\n")
            f.write("\n")
            
            f.write("üéØ –°–¢–†–ê–¢–ï–ì–ò–ß–ï–°–ö–ò–ï –í–´–í–û–î–´:\n")
            f.write("‚Ä¢ –£—Å–∏–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ —É—Å–ø–µ—à–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã\n")
            f.write("‚Ä¢ –§–æ–∫—É—Å –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã 2-5 –æ–ø—Ä–∞–≤–¥–∞–Ω\n")
            f.write("‚Ä¢ tel: —Å—Å—ã–ª–∫–∏ - —Å–∞–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥\n")
            f.write("‚Ä¢ –ù–∏–∑–∫–æ–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ –¥–∞—é—Ç –ª—É—á—à–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã\n\n")
            
            if len(self.failed_searches) > 0:
                f.write("‚ö†Ô∏è –ù–ï–£–î–ê–ß–ù–´–ï –ü–û–ò–°–ö–ò:\n")
                for failed in self.failed_searches[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10
                    f.write(f"  ‚Ä¢ {failed}\n")
                f.write(f"–í—Å–µ–≥–æ –Ω–µ—É–¥–∞—á: {len(self.failed_searches)}\n\n")
            
            if phone_percentage >= self.MIN_PHONE_PERCENTAGE:
                f.write("üéâ –ú–ò–°–°–ò–Ø –í–´–ü–û–õ–ù–ï–ù–ê!\n")
                f.write("–ë–∞–∑–∞ –≥–æ—Ç–æ–≤–∞ –¥–ª—è outreach –∫–∞–º–ø–∞–Ω–∏–∏!\n")
            else:
                f.write("üìà –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø:\n")
                f.write("‚Ä¢ –£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ—Ä–æ–¥–æ–≤\n")
                f.write("‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤\n")
                f.write("‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∫—Å–∏/VPN\n")
        
        self.logger.info(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω:")
        self.logger.info(f"  ‚Ä¢ CSV: {csv_file}")
        self.logger.info(f"  ‚Ä¢ Excel: {excel_file}")
        self.logger.info(f"  ‚Ä¢ –û—Ç—á–µ—Ç: {report_file}")
        
        return {
            'csv_file': csv_file,
            'excel_file': excel_file,
            'report_file': report_file,
            'total_businesses': total,
            'businesses_with_phones': with_phones,
            'phone_percentage': phone_percentage,
            'success': phone_percentage >= self.MIN_PHONE_PERCENTAGE,
            'extraction_methods': dict(extraction_methods),
            'failed_searches': len(self.failed_searches)
        }

    def _try_alternative_search_engines(self, query, page):
        """–ü–æ–ø—ã—Ç–∫–∞ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏"""
        for engine in self.search_engines:
            if engine['priority'] == 1: # Google is the primary engine
                continue
                
            try:
                self.logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ {engine['name']}")
                start = (page - 1) * 10
                
                if engine['name'] == 'Bing':
                    url = f"{engine['url']}?q={quote_plus(query)}&first={start}"
                elif engine['name'] == 'DuckDuckGo':
                    url = f"{engine['url']}?q={quote_plus(query)}&s={start}"
                else:
                    url = f"{engine['url']}?q={quote_plus(query)}&start={start}"
                
                headers = self._get_rotating_headers()
                response = self.session.get(url, headers=headers, timeout=30, verify=False)
                
                if response.status_code == 200:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤–∏–∫–æ–≤
                    if engine['name'] == 'Google':
                        return self._extract_links_from_google_page(response.text, query, page)
                    else:
                        return self._extract_links_from_alternative_engine(response.text, query, page, engine['name'])
                elif response.status_code == 429:
                    self.logger.warning(f"üö´ Rate limit detected for {engine['name']}")
                    time.sleep(random.uniform(60, 120))
                    continue
                else:
                    self.logger.warning(f"‚ùå HTTP {response.status_code} for {engine['name']}")
                    continue
                    
            except Exception as e:
                self.logger.error(f"Error searching with {engine['name']}: {e}")
                continue
        
        return []

    def _extract_links_from_alternative_engine(self, html, query, page, engine_name):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏–∑ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤–∏–∫–æ–≤"""
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        try:
            if engine_name == 'Bing':
                # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è Bing
                results = soup.select('.b_algo')
                for i, result in enumerate(results):
                    link_elem = result.select_one('h2 a')
                    if link_elem:
                        href = link_elem.get('href')
                        title = link_elem.get_text().strip()
                        desc_elem = result.select_one('.b_caption p')
                        description = desc_elem.get_text().strip() if desc_elem else ''
                        
                        if self._is_relevant_for_scrap_metal(title, description):
                            links.append({
                                'url': href,
                                'title': title[:150],
                                'description': description[:300],
                                'query': query,
                                'page': page,
                                'position': i + 1,
                                'collected_at': datetime.now().isoformat(),
                                'source': 'Bing'
                            })
                            
            elif engine_name == 'DuckDuckGo':
                # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è DuckDuckGo
                results = soup.select('.result')
                for i, result in enumerate(results):
                    link_elem = result.select_one('.result__title a')
                    if link_elem:
                        href = link_elem.get('href')
                        title = link_elem.get_text().strip()
                        desc_elem = result.select_one('.result__snippet')
                        description = desc_elem.get_text().strip() if desc_elem else ''
                        
                        if self._is_relevant_for_scrap_metal(title, description):
                            links.append({
                                'url': href,
                                'title': title[:150],
                                'description': description[:300],
                                'query': query,
                                'page': page,
                                'position': i + 1,
                                'collected_at': datetime.now().isoformat(),
                                'source': 'DuckDuckGo'
                            })
                            
        except Exception as e:
            self.logger.error(f"Error parsing {engine_name} results: {e}")
        
        return links

    def _extract_links_from_google_page(self, html, query, page):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã Google"""
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        # –ü–æ–∏—Å–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º Google
        result_selectors = [
            '.MjjYud',  # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç Google
            '.g',       # –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
            '.tF2Cxc',  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            '.rc'       # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
        ]
        
        results = []
        for selector in result_selectors:
            results = soup.select(selector)
            if results:
                break
        
        for i, result in enumerate(results):
            try:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É
                link_element = result.select_one('a')
                if not link_element:
                    continue
                
                href = link_element.get('href') or link_element.get('data-href')
                if not href or not href.startswith('http'):
                    continue
                
                # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_element = result.select_one('h3')
                title = title_element.get_text().strip() if title_element else 'No title'
                
                # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                desc_element = result.select_one('.VwiC3b, .s3v9rd, .x54gtf')
                description = desc_element.get_text().strip() if desc_element else ''
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è scrap metal
                if self._is_relevant_for_scrap_metal(title, description):
                    link_data = {
                        'url': href,
                        'title': title[:150],
                        'description': description[:300],
                        'query': query,
                        'page': page,
                        'position': i + 1,
                        'collected_at': datetime.now().isoformat()
                    }
                    links.append(link_data)
                
            except Exception as e:
                self.logger.debug(f"Error extracting link {i}: {e}")
                continue
        
        return links

    def _is_relevant_for_scrap_metal(self, title, description):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è scrap metal –±–∏–∑–Ω–µ—Å–∞"""
        text = (title + ' ' + description).lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è scrap metal
        relevant_keywords = [
            'scrap', 'metal', 'recycling', 'salvage', 'junk', 'yard',
            'steel', 'copper', 'aluminum', 'brass', 'iron', 'auto',
            'buyer', 'dealer', 'pickup', 'demolition'
        ]
        
        # –ò—Å–∫–ª—é—á–µ–Ω–∏—è (–Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)
        exclude_keywords = [
            'software', 'app', 'game', 'music', 'movie', 'book',
            'news', 'blog', 'wikipedia', 'facebook', 'linkedin'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        relevant_count = sum(1 for keyword in relevant_keywords if keyword in text)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏—Å–∫–ª—é—á–∞—é—â–∏—Ö —Å–ª–æ–≤
        has_exclusions = any(keyword in text for keyword in exclude_keywords)
        
        return relevant_count >= 1 and not has_exclusions

    def _prioritize_links(self, all_links):
        """–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        self.logger.info(f"üìä –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è {len(all_links)} —Å—Å—ã–ª–æ–∫")
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        unique_links = {}
        for link in all_links:
            url = link['url']
            if url not in unique_links:
                unique_links[url] = link
        
        links_list = list(unique_links.values())
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        def priority_score(link):
            score = 0
            title_desc = (link['title'] + ' ' + link['description']).lower()
            
            # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –ø—Ä—è–º—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            if any(word in title_desc for word in ['phone', 'call', 'contact']):
                score += 10
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ —Ç–∏–ø—É –±–∏–∑–Ω–µ—Å–∞
            if 'scrap metal' in title_desc:
                score += 8
            elif any(word in title_desc for word in ['scrap', 'recycling', 'salvage']):
                score += 5
                
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü 2-3 (–±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ, –Ω–æ –Ω–µ —Ç–æ–ø)
            if link['page'] in [2, 3]:
                score += 3
            elif link['page'] in [4, 5]:
                score += 1
                
            return score
        
        prioritized = sorted(links_list, key=priority_score, reverse=True)
        
        self.logger.info(f"üìà –°—Å—ã–ª–∫–∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã: {len(prioritized)}")
        return prioritized

def main():
    print("ü§ñ –ë–´–°–¢–†–´–ô –ü–û–õ–ù–û–°–¢–¨–Æ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô GOOGLE –ü–ê–†–°–ï–†")
    print("=" * 60)
    print("‚ö° –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô —Å–±–æ—Ä –∏–∑ Bing/Google —Å—Ç—Ä–∞–Ω–∏—Ü 2-3")
    print("üìû 85% –±–∏–∑–Ω–µ—Å–æ–≤ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏")
    print("üîç –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤")
    print("üöÄ –ë–ï–ó –†–£–ß–ù–û–ì–û –í–ú–ï–®–ê–¢–ï–õ–¨–°–¢–í–ê | –ë–´–°–¢–†–û")
    
    scraper = FullyAutomaticGoogleScraper()
    
    try:
        target = int(input("\n–¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏–∑–Ω–µ—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 500): ") or "500")
        
        print(f"\nüöÄ –ó–∞–ø—É—Å–∫ –ë–´–°–¢–†–û–ì–û –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ –¥–ª—è {target} –±–∏–∑–Ω–µ—Å–æ–≤...")
        print("‚ö° –í–ù–ò–ú–ê–ù–ò–ï: –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–π–º–µ—Ç 10-30 –º–∏–Ω—É—Ç (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)")
        print("üìä –ü—Ä–æ–≥—Ä–µ—Å—Å –±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω –≤ –∫–æ–Ω—Å–æ–ª–∏")
        
        confirm = input("\n–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (y/N): ").lower()
        if confirm != 'y':
            print("‚ùå –û—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            return
        
        start_time = time.time()
        results = scraper.run_complete_automation(target)
        elapsed = time.time() - start_time
        
        if results:
            phone_count = sum(1 for b in results if b.get('phone'))
            phone_percentage = (phone_count / len(results)) * 100
            
            print(f"\nüéâ –£–°–ò–õ–ï–ù–ù–´–ô –°–ë–û–† –ó–ê–í–ï–†–®–ï–ù!")
            print(f"‚è±Ô∏è –í—Ä–µ–º—è: {elapsed/60:.1f} –º–∏–Ω—É—Ç")
            print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(results)} –±–∏–∑–Ω–µ—Å–æ–≤")
            print(f"üìû –° —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏: {phone_count} ({phone_percentage:.1f}%)")
            
            export_info = scraper.export_automated_results()
            if export_info:
                print(f"\nüìÅ –§–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã:")
                print(f"  ‚Ä¢ CSV: {export_info['csv_file']}")
                print(f"  ‚Ä¢ Excel: {export_info['excel_file']}")
                print(f"  ‚Ä¢ –û—Ç—á–µ—Ç: {export_info['report_file']}")
                
                if export_info['success']:
                    print(f"\n‚úÖ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê!")
                    print(f"üöÄ –ë–∞–∑–∞ –≥–æ—Ç–æ–≤–∞ –¥–ª—è outreach –∫–∞–º–ø–∞–Ω–∏–∏!")
                else:
                    print(f"\n‚ö†Ô∏è –¶–µ–ª—å –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞")
                    print(f"üìà –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –æ—Ö–≤–∞—Ç")
        else:
            print("‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–±—Ä–∞–Ω—ã")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        if scraper.results:
            print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
            scraper.export_automated_results()
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        scraper.logger.error(f"Critical error: {e}", exc_info=True)

if __name__ == "__main__":
    main() 