#!/usr/bin/env python3
"""
GOOGLE-ONLY SCRAPER - –ü–∞—Ä—Å–µ—Ä —Ä–∞–±–æ—Ç–∞—é—â–∏–π –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —Å Google
–°–¢–†–ê–¢–ï–ì–ò–Ø: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–±–æ—Ä –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –∏–∑ Google —Å—Ç—Ä–∞–Ω–∏—Ü 2-5
–ü–†–ò–û–†–ò–¢–ï–¢: –¢–µ–ª–µ—Ñ–æ–Ω—ã –ø—Ä–µ–≤—ã—à–µ –≤—Å–µ–≥–æ!
"""

import os
import json
import time
import random
import logging
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
from urllib.parse import quote_plus, urljoin, urlparse
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict

class GoogleOnlyScraper:
    def __init__(self):
        self.session = requests.Session()
        self.results = []
        self.processed_urls = set()
        self.logger = self._setup_logging()
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: —Ü–µ–ª—å –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º
        self.MIN_PHONE_PERCENTAGE = 85  # 85% —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        self.phone_patterns = [
            # tel: —Å—Å—ã–ª–∫–∏ (–∫–∞–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–ª —Å—Ç–∞—Ä—à–∏–π)
            re.compile(r'tel:[\s]*\+?1?[\s]*(\d{3})[\s]*(\d{3})[\s]*(\d{4})', re.IGNORECASE),
            re.compile(r'tel:[\s]*\+?1?[\s]*\(?(\d{3})\)?[\s]*(\d{3})[\s]*(\d{4})', re.IGNORECASE),
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            re.compile(r'\(?(\d{3})\)?[-.\s]*(\d{3})[-.\s]*(\d{4})'),
            re.compile(r'(\d{3})[-.\s]*(\d{3})[-.\s]*(\d{4})'),
            re.compile(r'1[\s]*\(?(\d{3})\)?[-.\s]*(\d{3})[-.\s]*(\d{4})'),
            
            # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            re.compile(r'phone[\s]*:[\s]*\(?(\d{3})\)?[-.\s]*(\d{3})[-.\s]*(\d{4})', re.IGNORECASE),
            re.compile(r'call[\s]*:?[\s]*\(?(\d{3})\)?[-.\s]*(\d{3})[-.\s]*(\d{4})', re.IGNORECASE),
            re.compile(r'contact[\s]*:?[\s]*\(?(\d{3})\)?[-.\s]*(\d{3})[-.\s]*(\d{4})', re.IGNORECASE),
            
            # –°–∫—Ä—ã—Ç—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
            re.compile(r'data-phone[\s]*=[\s]*["\'][\s]*\(?(\d{3})\)?[-.\s]*(\d{3})[-.\s]*(\d{4})', re.IGNORECASE),
            re.compile(r'data-tel[\s]*=[\s]*["\'][\s]*\(?(\d{3})\)?[-.\s]*(\d{3})[-.\s]*(\d{4})', re.IGNORECASE),
        ]
        
        # –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –æ—Ö–≤–∞—Ç–∞
        self.search_queries = [
            'scrap metal buyers',
            'metal recycling center',
            'scrap yard',
            'junk yard',
            'auto salvage',
            'copper buyers',
            'aluminum recycling',
            'steel scrap',
            'metal dealers',
            'scrap metal pickup',
            'recycling center',
            'salvage yard'
        ]
        
        # –¶–µ–ª–µ–≤—ã–µ –≥–æ—Ä–æ–¥–∞ (—Å—Ä–µ–¥–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ —Å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º)
        self.target_cities = [
            'Akron OH', 'Toledo OH', 'Dayton OH', 'Youngstown OH',
            'Rochester NY', 'Syracuse NY', 'Buffalo NY', 'Albany NY',
            'Scranton PA', 'Allentown PA', 'Reading PA', 'Erie PA',
            'Flint MI', 'Lansing MI', 'Kalamazoo MI', 'Grand Rapids MI',
            'Rockford IL', 'Peoria IL', 'Decatur IL', 'Springfield IL',
            'Fort Wayne IN', 'Evansville IN', 'South Bend IN',
            'Green Bay WI', 'Appleton WI', 'Oshkosh WI',
            'Cedar Rapids IA', 'Davenport IA', 'Sioux City IA',
            'Springfield MO', 'Columbia MO', 'Joplin MO',
            'Little Rock AR', 'Fayetteville AR', 'Jonesboro AR'
        ]

    def _setup_logging(self):
        logger = logging.getLogger('GoogleOnlyScraper')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger

    def collect_from_google_exclusively(self, target_count=200):
        """–°–±–æ—Ä –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑ Google"""
        self.logger.info(f"üîç GOOGLE-ONLY —Å–±–æ—Ä –¥–ª—è {target_count} –±–∏–∑–Ω–µ—Å–æ–≤")
        self.logger.info(f"üéØ –¶–ï–õ–¨: {self.MIN_PHONE_PERCENTAGE}% —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è Google
        self._show_comprehensive_google_instructions()
        
        # –ñ–¥–µ–º, –ø–æ–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–æ–±–µ—Ä–µ—Ç –¥–∞–Ω–Ω—ã–µ
        self._wait_for_google_data()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ JSON —Ñ–∞–π–ª—ã
        all_businesses = self._process_all_google_files()
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        unique_businesses = self._deduplicate_businesses(all_businesses)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        self.results = unique_businesses[:target_count]
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        phone_percentage = self._calculate_phone_percentage()
        self.logger.info(f"‚úÖ –ò–¢–û–ì–û: {len(self.results)} –±–∏–∑–Ω–µ—Å–æ–≤, {phone_percentage:.1f}% —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏")
        
        return self.results

    def _show_comprehensive_google_instructions(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è Google –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        print("\n" + "="*80)
        print("üîç GOOGLE-ONLY –ü–ê–†–°–ò–ù–ì - –ü–û–î–†–û–ë–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò")
        print("="*80)
        print("üéØ –¶–ï–õ–¨: –°–æ–±—Ä–∞—Ç—å –º–∞–∫—Å–∏–º—É–º –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –∏–∑ Google —Å—Ç—Ä–∞–Ω–∏—Ü 2-5")
        print()
        print("üìã –ü–õ–ê–ù –î–ï–ô–°–¢–í–ò–ô:")
        print("1. –û—Ç–∫—Ä–æ–π—Ç–µ Google –≤ –±—Ä–∞—É–∑–µ—Ä–µ")
        print("2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≥–æ—Ä–æ–¥–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –ø–æ–∏—Å–∫ –ø–æ –∫–∞–∂–¥–æ–º—É –∑–∞–ø—Ä–æ—Å—É")
        print("3. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã 2-5 (–ù–ï —Å—Ç—Ä–∞–Ω–∏—Ü—É 1!)")
        print("4. –ù–∞ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∑–∞–ø—É—Å—Ç–∏—Ç–µ JavaScript")
        print("5. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ JSON —Ñ–∞–π–ª—ã")
        print()
        print("üèôÔ∏è –¶–ï–õ–ï–í–´–ï –ì–û–†–û–î–ê:")
        for i, city in enumerate(self.target_cities, 1):
            print(f"   {i:2d}. {city}")
        print()
        print("üîç –ü–û–ò–°–ö–û–í–´–ï –ó–ê–ü–†–û–°–´:")
        for i, query in enumerate(self.search_queries, 1):
            print(f"   {i:2d}. {query}")
        print()
        print("üí° –ü–†–ò–ú–ï–† –ü–û–õ–ù–û–ì–û –ó–ê–ü–†–û–°–ê:")
        print("   'scrap metal buyers Akron OH'")
        print("   'metal recycling center Toledo OH'")
        print()
        print("üìÑ –°–¢–†–ê–ù–ò–¶–´ –î–õ–Ø –ü–ê–†–°–ò–ù–ì–ê:")
        print("   ‚Ä¢ –°—Ç—Ä–∞–Ω–∏—Ü–∞ 2 (—Å–∞–º–∞—è –≤–∞–∂–Ω–∞—è)")
        print("   ‚Ä¢ –°—Ç—Ä–∞–Ω–∏—Ü–∞ 3 (–≤–∞–∂–Ω–∞—è)")
        print("   ‚Ä¢ –°—Ç—Ä–∞–Ω–∏—Ü–∞ 4 (–ø–æ–ª–µ–∑–Ω–∞—è)")
        print("   ‚Ä¢ –°—Ç—Ä–∞–Ω–∏—Ü–∞ 5 (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è)")
        print()
        print("üîß JAVASCRIPT –î–õ–Ø –ö–û–ù–°–û–õ–ò:")
        print("-" * 50)
        print(self._get_enhanced_js_script())
        print("-" * 50)
        print()
        print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –§–ê–ô–õ–û–í:")
        print("   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –∫–∞–∫: city_query_pageN.json")
        print("   ‚Ä¢ –ü—Ä–∏–º–µ—Ä: akron_scrap_metal_page2.json")
        print("   ‚Ä¢ –í—Å–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞")
        print()
        print("‚ö†Ô∏è –í–ê–ñ–ù–û:")
        print("   ‚Ä¢ –î–µ–ª–∞–π—Ç–µ –ø–∞—É–∑—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (30-60 —Å–µ–∫)")
        print("   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ VPN –µ—Å–ª–∏ –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤")
        print("   ‚Ä¢ –§–æ–∫—É—Å –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö 2-5, –ù–ï –Ω–∞ –ø–µ—Ä–≤–æ–π!")
        print("="*80)

    def _get_enhanced_js_script(self):
        """–ü–æ–ª—É—á–∏—Ç—å —É–ª—É—á—à–µ–Ω–Ω—ã–π JavaScript —Å–∫—Ä–∏–ø—Ç"""
        return """
javascript:!(function(){
    console.log('üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Google —Å—Å—ã–ª–æ–∫ –¥–ª—è scrap metal...');
    
    // –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    window.scrollTo(0, document.body.scrollHeight);
    setTimeout(() => window.scrollTo(0, 0), 1000);
    
    // –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–Ω–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    var win = window.open('', 'ScrapMetalGoogleResults', 'width=1200,height=900,scrollbars=yes');
    win.document.write('<html><head><title>Google Scrap Metal Results</title></head><body>');
    win.document.write('<h1>üîç Google Scrap Metal Results</h1>');
    win.document.write('<p><strong>–í—Ä–µ–º—è —Å–±–æ—Ä–∞:</strong> ' + new Date().toLocaleString() + '</p>');
    win.document.write('<p><strong>–°—Ç—Ä–∞–Ω–∏—Ü–∞:</strong> ' + window.location.href + '</p>');
    
    var results = [];
    var processed = new Set();
    var relevantCount = 0;
    
    // –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    var keywords = [
        'scrap', 'metal', 'recycling', 'salvage', 'junk', 'steel', 'copper', 
        'aluminum', 'brass', 'iron', 'auto', 'yard', 'buyer', 'dealer'
    ];
    
    // –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Google
    var searchResults = document.querySelectorAll('.MjjYud, .g, .tF2Cxc');
    
    searchResults.forEach(function(item, index) {
        var link = item.querySelector('a');
        var href = link ? (link.getAttribute('data-href') || link.getAttribute('href')) : null;
        var titleElement = item.querySelector('h3, .DKV0Md');
        var descElement = item.querySelector('.VwiC3b, .s3v9rd, .x54gtf');
        
        if (href && titleElement && href.indexOf('http') === 0) {
            var title = titleElement.innerText || titleElement.textContent || '';
            var description = descElement ? (descElement.innerText || descElement.textContent || '') : '';
            
            // –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            var fullText = (title + ' ' + description).toLowerCase();
            var isRelevant = keywords.some(keyword => fullText.includes(keyword));
            
            if (isRelevant && !processed.has(href)) {
                processed.add(href);
                relevantCount++;
                
                var result = {
                    url: href,
                    title: title.substring(0, 150),
                    description: description.substring(0, 300),
                    position: index + 1,
                    page: window.location.href,
                    collected_at: new Date().toISOString()
                };
                
                results.push(result);
                
                // –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤ –æ–∫–Ω–µ
                win.document.write('<div style="margin:10px 0; padding:10px; border:1px solid #ddd; background:#f9f9f9;">');
                win.document.write('<h3>' + relevantCount + '. ' + title + '</h3>');
                win.document.write('<p><strong>URL:</strong> <a href="' + href + '" target="_blank">' + href + '</a></p>');
                win.document.write('<p><strong>–û–ø–∏—Å–∞–Ω–∏–µ:</strong> ' + description.substring(0, 200) + '...</p>');
                win.document.write('<p><strong>–ü–æ–∑–∏—Ü–∏—è:</strong> ' + (index + 1) + '</p>');
                win.document.write('</div>');
            }
        }
    });
    
    // –ò—Ç–æ–≥–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    win.document.write('<hr><h2>üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞</h2>');
    win.document.write('<p><strong>–ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:</strong> ' + relevantCount + '</p>');
    win.document.write('<p><strong>–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:</strong> ' + results.length + ' –∑–∞–ø–∏—Å–µ–π</p>');
    
    // JSON –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
    win.document.write('<h2>üìÑ JSON –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞</h2>');
    win.document.write('<p><strong>–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç JSON –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤ —Ñ–∞–π–ª:</strong></p>');
    win.document.write('<textarea rows="20" cols="120" onclick="this.select(); document.execCommand(\'copy\'); alert(\'JSON —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –±—É—Ñ–µ—Ä –æ–±–º–µ–Ω–∞!\');">' + 
                      JSON.stringify(results, null, 2) + '</textarea>');
    
    win.document.write('<h2>üí° –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é</h2>');
    win.document.write('<ol>');
    win.document.write('<li>–í—ã–¥–µ–ª–∏—Ç–µ –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –≤–µ—Å—å JSON –≤—ã—à–µ</li>');
    win.document.write('<li>–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª —Å –∏–º–µ–Ω–µ–º: city_query_pageN.json</li>');
    win.document.write('<li>–í—Å—Ç–∞–≤—å—Ç–µ JSON –≤ —Ñ–∞–π–ª –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ</li>');
    win.document.write('<li>–ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–æ–¥–æ–≤ –∏ –∑–∞–ø—Ä–æ—Å–æ–≤</li>');
    win.document.write('</ol>');
    
    win.document.write('</body></html>');
    
    console.log('‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!');
    console.log('üìä –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:', relevantCount);
    console.log('üíæ –°–∫–æ–ø–∏—Ä—É–π—Ç–µ JSON –∏–∑ –æ—Ç–∫—Ä—ã–≤—à–µ–≥–æ—Å—è –æ–∫–Ω–∞');
    
    return results;
})();
        """

    def _wait_for_google_data(self):
        """–ñ–¥–∞—Ç—å –ø–æ–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–æ–±–µ—Ä–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Google"""
        print("\nüîÑ –û–ñ–ò–î–ê–ù–ò–ï –î–ê–ù–ù–´–• GOOGLE...")
        print("–°–æ–±–µ—Ä–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤—ã—à–µ, –∑–∞—Ç–µ–º –Ω–∞–∂–º–∏—Ç–µ Enter")
        input("–ù–∞–∂–º–∏—Ç–µ Enter –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –≥–æ—Ç–æ–≤—ã...")

    def _process_all_google_files(self):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ JSON —Ñ–∞–π–ª—ã –æ—Ç Google"""
        businesses = []
        
        # –ò—â–µ–º –≤—Å–µ JSON —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ
        json_files = [f for f in os.listdir('.') if f.endswith('.json') and 'google' in f.lower()]
        
        if not json_files:
            self.logger.warning("‚ùå JSON —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            self.logger.info("–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª...")
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            return self._create_test_data()
        
        self.logger.info(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(json_files)} JSON —Ñ–∞–π–ª–æ–≤")
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    google_data = json.load(f)
                
                self.logger.info(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ {json_file}: {len(google_data)} —Å—Å—ã–ª–æ–∫")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Å—ã–ª–∫—É
                file_businesses = self._process_google_links(google_data, json_file)
                businesses.extend(file_businesses)
                
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ {json_file}: {e}")
                continue
        
        return businesses

    def _process_google_links(self, google_data, source_file):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Å—ã–ª–∫–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ Google —Ñ–∞–π–ª–∞"""
        businesses = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            
            for link_data in google_data:
                if not isinstance(link_data, dict):
                    continue
                
                url = link_data.get('url', '')
                if not url or url in self.processed_urls:
                    continue
                
                self.processed_urls.add(url)
                future = executor.submit(self._extract_business_from_url, link_data, source_file)
                futures.append(future)
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for future in as_completed(futures):
                try:
                    business = future.result(timeout=30)
                    if business:
                        businesses.append(business)
                        
                        if business.get('phone'):
                            self.logger.info(f"‚úÖ {business['name']}: {business['phone']}")
                        else:
                            self.logger.info(f"‚ùå {business['name']}: —Ç–µ–ª–µ—Ñ–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    
                except Exception as e:
                    self.logger.debug(f"Future error: {e}")
                    continue
        
        phone_count = sum(1 for b in businesses if b.get('phone'))
        self.logger.info(f"üìä –ò–∑ {source_file}: {len(businesses)} –±–∏–∑–Ω–µ—Å–æ–≤, {phone_count} —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏")
        
        return businesses

    def _extract_business_from_url(self, link_data, source_file):
        """–ò–∑–≤–ª–µ—á—å –º–∞–∫—Å–∏–º—É–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –±–∏–∑–Ω–µ—Å–µ —Å —Å–∞–π—Ç–∞"""
        url = link_data.get('url', '')
        if not url:
            return None
        
        try:
            # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ —Å–∞–π—Ç—É
            response = self._make_safe_request(url)
            if not response:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            page_text = response.text
            
            # –ê–ì–†–ï–°–°–ò–í–ù–´–ô –ø–æ–∏—Å–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–∞
            phone = self._aggressive_phone_search(page_text, soup)
            
            # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç —Å—Ç–∞—Ä—à–∏–π)
            if not phone:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            business = {
                'name': self._extract_business_name(link_data, soup),
                'phone': phone,
                'website': url,
                'email': self._extract_email(page_text, soup),
                'address': self._extract_address(soup),
                'city': self._extract_city(soup),
                'state': self._extract_state(soup),
                'zip_code': self._extract_zip(soup),
                'business_hours': self._extract_hours(soup),
                'services': self._extract_services(page_text, soup),
                'materials': self._extract_materials(page_text, soup),
                'description': self._extract_description(soup),
                'google_title': link_data.get('title', ''),
                'google_description': link_data.get('description', ''),
                'google_position': link_data.get('position', 0),
                'source_file': source_file,
                'source': 'Google_Only',
                'scraped_at': datetime.now().isoformat(),
                'phone_found_method': getattr(self, '_last_phone_method', 'unknown')
            }
            
            return business
            
        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å {url}: {e}")
            return None

    def _aggressive_phone_search(self, page_text, soup):
        """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        
        # –ú–µ—Ç–æ–¥ 1: tel: —Å—Å—ã–ª–∫–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, –∫–∞–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–ª —Å—Ç–∞—Ä—à–∏–π)
        tel_links = soup.find_all('a', href=lambda x: x and x.startswith('tel:'))
        for link in tel_links:
            tel_value = link.get('href', '').replace('tel:', '').strip()
            phone = self._clean_phone(tel_value)
            if phone:
                self._last_phone_method = 'tel_link'
                return phone
        
        # –ú–µ—Ç–æ–¥ 2: data-phone –∏ data-tel –∞—Ç—Ä–∏–±—É—Ç—ã
        phone_elements = soup.find_all(attrs={'data-phone': True}) + soup.find_all(attrs={'data-tel': True})
        for element in phone_elements:
            phone_value = element.get('data-phone') or element.get('data-tel')
            if phone_value:
                phone = self._clean_phone(phone_value)
                if phone:
                    self._last_phone_method = 'data_attribute'
                    return phone
        
        # –ú–µ—Ç–æ–¥ 3: –°–∫—Ä—ã—Ç—ã–µ –ø–æ–ª—è –∏ meta —Ç–µ–≥–∏
        meta_phones = soup.find_all('meta', attrs={'name': re.compile(r'phone|tel', re.I)})
        for meta in meta_phones:
            phone_value = meta.get('content', '')
            if phone_value:
                phone = self._clean_phone(phone_value)
                if phone:
                    self._last_phone_method = 'meta_tag'
                    return phone
        
        # –ú–µ—Ç–æ–¥ 4: –ü–æ–∏—Å–∫ –≤ JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                phone = self._extract_phone_from_json_ld(data)
                if phone:
                    self._last_phone_method = 'json_ld'
                    return phone
            except:
                continue
        
        # –ú–µ—Ç–æ–¥ 5: –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –≤ —Ç–µ–∫—Å—Ç–µ
        for i, pattern in enumerate(self.phone_patterns):
            matches = pattern.findall(page_text)
            for match in matches:
                phone = self._format_phone_match(match)
                if phone and self._validate_phone(phone):
                    self._last_phone_method = f'pattern_{i+1}'
                    return phone
        
        # –ú–µ—Ç–æ–¥ 6: –ü–æ–∏—Å–∫ –≤ specific CSS –∫–ª–∞—Å—Å–∞—Ö
        phone_classes = ['.phone', '.tel', '.contact-phone', '.phone-number', '.telephone']
        for class_name in phone_classes:
            elements = soup.select(class_name)
            for element in elements:
                text = element.get_text()
                phone = self._extract_phone_from_text(text)
                if phone:
                    self._last_phone_method = 'css_class'
                    return phone
        
        return ""

    def _extract_phone_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–ª–µ—Ñ–æ–Ω –∏–∑ JSON-LD –¥–∞–Ω–Ω—ã—Ö"""
        if isinstance(data, dict):
            # –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–∞
            if 'telephone' in data:
                return self._clean_phone(data['telephone'])
            if 'phone' in data:
                return self._clean_phone(data['phone'])
            
            # –ü–æ–∏—Å–∫ –≤ contact info
            if 'contactPoint' in data:
                contact = data['contactPoint']
                if isinstance(contact, dict) and 'telephone' in contact:
                    return self._clean_phone(contact['telephone'])
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
            for key, value in data.items():
                if isinstance(value, (dict, list)):
                    phone = self._extract_phone_from_json_ld(value)
                    if phone:
                        return phone
        
        elif isinstance(data, list):
            for item in data:
                phone = self._extract_phone_from_json_ld(item)
                if phone:
                    return phone
        
        return ""

    def _extract_phone_from_text(self, text):
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–ª–µ—Ñ–æ–Ω –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        for pattern in self.phone_patterns:
            matches = pattern.findall(text)
            for match in matches:
                phone = self._format_phone_match(match)
                if phone and self._validate_phone(phone):
                    return phone
        return ""

    def _validate_phone(self, phone):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        if not phone:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç
        digits = re.sub(r'\D', '', phone)
        if len(digits) not in [10, 11]:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–µ –≤—Å–µ —Ü–∏—Ñ—Ä—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ
        if len(set(digits)) < 3:
            return False
        
        return True

    def _extract_business_name(self, link_data, soup):
        """–ò–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –±–∏–∑–Ω–µ—Å–∞"""
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: title –∏–∑ Google
        google_title = link_data.get('title', '')
        if google_title:
            return google_title[:100]
        
        # –ò–∑ title —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        title = soup.find('title')
        if title:
            return title.get_text().strip()[:100]
        
        # –ò–∑ H1
        h1 = soup.find('h1')
        if h1:
            return h1.get_text().strip()[:100]
        
        return "Unknown Business"

    def _extract_email(self, page_text, soup):
        """–ò–∑–≤–ª–µ—á—å email"""
        email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        emails = email_pattern.findall(page_text)
        
        for email in emails:
            if not any(skip in email.lower() for skip in ['example', 'test', 'google', 'facebook', 'twitter']):
                return email
        
        return ""

    def _extract_address(self, soup):
        """–ò–∑–≤–ª–µ—á—å –∞–¥—Ä–µ—Å"""
        selectors = [
            '.address', '.location', '.contact-address',
            '[itemprop="streetAddress"]', '[itemprop="address"]',
            '.street-address', '.addr'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                address = element.get_text().strip()
                if len(address) > 10:
                    return address[:200]
        
        return ""

    def _extract_city(self, soup):
        """–ò–∑–≤–ª–µ—á—å –≥–æ—Ä–æ–¥"""
        selectors = [
            '[itemprop="addressLocality"]',
            '.city', '.locality'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        
        return ""

    def _extract_state(self, soup):
        """–ò–∑–≤–ª–µ—á—å —à—Ç–∞—Ç"""
        selectors = [
            '[itemprop="addressRegion"]',
            '.state', '.region'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        
        return ""

    def _extract_zip(self, soup):
        """–ò–∑–≤–ª–µ—á—å ZIP –∫–æ–¥"""
        selectors = [
            '[itemprop="postalCode"]',
            '.zip', '.postal-code'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        
        return ""

    def _extract_hours(self, soup):
        """–ò–∑–≤–ª–µ—á—å —Ä–∞–±–æ—á–∏–µ —á–∞—Å—ã"""
        selectors = [
            '.hours', '.business-hours', '.opening-hours',
            '[itemprop="openingHours"]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()[:200]
        
        return ""

    def _extract_services(self, page_text, soup):
        """–ò–∑–≤–ª–µ—á—å —É—Å–ª—É–≥–∏"""
        services = []
        service_keywords = [
            'pickup', 'container', 'demolition', 'processing',
            'sorting', 'weighing', 'cash', 'certified scales',
            'commercial', 'residential', 'industrial'
        ]
        
        text_lower = page_text.lower()
        for keyword in service_keywords:
            if keyword in text_lower:
                services.append(keyword)
        
        return services

    def _extract_materials(self, page_text, soup):
        """–ò–∑–≤–ª–µ—á—å –ø—Ä–∏–Ω–∏–º–∞–µ–º—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã"""
        materials = []
        material_keywords = [
            'copper', 'aluminum', 'steel', 'brass', 'iron',
            'stainless', 'lead', 'zinc', 'nickel', 'wire',
            'cable', 'battery', 'radiator', 'catalytic'
        ]
        
        text_lower = page_text.lower()
        for keyword in material_keywords:
            if keyword in text_lower:
                materials.append(keyword)
        
        return materials

    def _extract_description(self, soup):
        """–ò–∑–≤–ª–µ—á—å –æ–ø–∏—Å–∞–Ω–∏–µ"""
        # –ò–∑ meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            return meta_desc.get('content', '')[:300]
        
        # –ò–∑ –ø–µ—Ä–≤–æ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
        first_p = soup.find('p')
        if first_p:
            return first_p.get_text().strip()[:300]
        
        return ""

    def _create_test_data(self):
        """–°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
        self.logger.info("üìù –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        test_businesses = [
            {
                'name': 'Akron Scrap Metal',
                'phone': '(330) 555-0123',
                'website': 'http://akronscrap.com',
                'email': 'info@akronscrap.com',
                'address': '123 Steel St, Akron, OH 44301',
                'city': 'Akron',
                'state': 'OH',
                'source': 'Google_Test',
                'phone_found_method': 'tel_link',
                'scraped_at': datetime.now().isoformat()
            },
            {
                'name': 'Toledo Metal Recycling',
                'phone': '(419) 555-0456',
                'website': 'http://toledometalrecycling.com',
                'email': 'contact@toledometalrecycling.com',
                'address': '456 Metal Ave, Toledo, OH 43601',
                'city': 'Toledo',
                'state': 'OH',
                'source': 'Google_Test',
                'phone_found_method': 'data_attribute',
                'scraped_at': datetime.now().isoformat()
            }
        ]
        
        return test_businesses

    def _deduplicate_businesses(self, businesses):
        """–£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã"""
        seen = set()
        unique = []
        
        for business in businesses:
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
            key = (
                business.get('name', '').lower().strip(),
                business.get('phone', '').replace(' ', '').replace('-', '').replace('(', '').replace(')', ''),
                business.get('website', '').lower().strip()
            )
            
            if key not in seen:
                seen.add(key)
                unique.append(business)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏, –ø–æ—Ç–æ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        unique.sort(key=lambda x: (
            bool(x.get('phone')),
            bool(x.get('email')),
            bool(x.get('address')),
            len(x.get('name', ''))
        ), reverse=True)
        
        return unique

    def _calculate_phone_percentage(self):
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø—Ä–æ—Ü–µ–Ω—Ç —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏"""
        if not self.results:
            return 0
        return (sum(1 for b in self.results if b.get('phone')) / len(self.results)) * 100

    def _format_phone_match(self, match):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω"""
        if isinstance(match, tuple) and len(match) >= 3:
            return f"({match[0]}) {match[1]}-{match[2]}"
        elif isinstance(match, str):
            return self._clean_phone(match)
        return ""

    def _clean_phone(self, phone):
        """–û—á–∏—Å—Ç–∏—Ç—å –∏ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω"""
        if not phone:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä
        digits = re.sub(r'\D', '', phone)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º
        if len(digits) == 10:
            return f"({digits[:3]}) {digits[3:6]}-{digits[6:]}"
        elif len(digits) == 11 and digits[0] == '1':
            return f"({digits[1:4]}) {digits[4:7]}-{digits[7:]}"
        
        return ""

    def _make_safe_request(self, url, timeout=15):
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π HTTP –∑–∞–ø—Ä–æ—Å"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            response = self.session.get(url, headers=headers, timeout=timeout)
            if response.status_code == 200:
                return response
            
        except Exception as e:
            self.logger.debug(f"Request failed for {url}: {e}")
        
        return None

    def export_google_results(self, output_dir="output"):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ Google –¥–∞–Ω–Ω—ã–µ"""
        if not self.results:
            return None
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total = len(self.results)
        with_phones = sum(1 for b in self.results if b.get('phone'))
        phone_percentage = (with_phones / total) * 100 if total > 0 else 0
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º –ø–æ–∏—Å–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        methods = defaultdict(int)
        for business in self.results:
            if business.get('phone'):
                method = business.get('phone_found_method', 'unknown')
                methods[method] += 1
        
        # CSV —ç–∫—Å–ø–æ—Ä—Ç
        df = pd.DataFrame(self.results)
        csv_file = os.path.join(output_dir, f"google_only_results_{timestamp}.csv")
        df.to_csv(csv_file, index=False)
        
        # Excel —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ª–∏—Å—Ç–∞–º–∏
        excel_file = os.path.join(output_dir, f"google_only_results_{timestamp}.xlsx")
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            df.to_excel(writer, sheet_name='All Results', index=False)
            
            # –¢–æ–ª—å–∫–æ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏
            df_phones = df[df['phone'].notna() & (df['phone'] != '')]
            if not df_phones.empty:
                df_phones.to_excel(writer, sheet_name='With Phones', index=False)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ—Ç–æ–¥–æ–≤
            methods_df = pd.DataFrame(list(methods.items()), columns=['Method', 'Count'])
            methods_df.to_excel(writer, sheet_name='Phone Methods', index=False)
        
        # –û—Ç—á–µ—Ç
        report_file = os.path.join(output_dir, f"google_only_report_{timestamp}.txt")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("üîç GOOGLE-ONLY SCRAPER REPORT\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–ú–µ—Ç–æ–¥: –ò—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ Google –ø–∞—Ä—Å–∏–Ω–≥\n\n")
            
            f.write("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")
            f.write(f"–í—Å–µ–≥–æ –±–∏–∑–Ω–µ—Å–æ–≤: {total}\n")
            f.write(f"–° —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏: {with_phones} ({phone_percentage:.1f}%)\n")
            f.write(f"–¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞: {'‚úÖ –î–ê' if phone_percentage >= self.MIN_PHONE_PERCENTAGE else '‚ùå –ù–ï–¢'}\n\n")
            
            f.write("üîß –ú–ï–¢–û–î–´ –ü–û–ò–°–ö–ê –¢–ï–õ–ï–§–û–ù–û–í:\n")
            for method, count in sorted(methods.items(), key=lambda x: x[1], reverse=True):
                f.write(f"{method}: {count} —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤\n")
            f.write("\n")
            
            f.write("üéØ –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê:\n")
            f.write(f"‚Ä¢ tel: —Å—Å—ã–ª–∫–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç): {methods.get('tel_link', 0)}\n")
            f.write(f"‚Ä¢ data –∞—Ç—Ä–∏–±—É—Ç—ã: {methods.get('data_attribute', 0)}\n")
            f.write(f"‚Ä¢ JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {methods.get('json_ld', 0)}\n")
            f.write(f"‚Ä¢ –ü–∞—Ç—Ç–µ—Ä–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ: {sum(methods.get(f'pattern_{i}', 0) for i in range(1, 11))}\n")
            f.write("\n")
            
            if phone_percentage >= self.MIN_PHONE_PERCENTAGE:
                f.write("üéâ –£–°–ü–ï–•! –ì–æ—Ç–æ–≤–æ –¥–ª—è outreach –∫–∞–º–ø–∞–Ω–∏–∏!\n")
            else:
                f.write("‚ö†Ô∏è –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Google. –°–æ–±–µ—Ä–∏—Ç–µ –±–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü.\n")
        
        self.logger.info(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω:")
        self.logger.info(f"  ‚Ä¢ CSV: {csv_file}")
        self.logger.info(f"  ‚Ä¢ Excel: {excel_file}")
        self.logger.info(f"  ‚Ä¢ –û—Ç—á–µ—Ç: {report_file}")
        self.logger.info(f"üìä –ò–¢–û–ì–û: {phone_percentage:.1f}% —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏")
        
        return {
            'csv_file': csv_file,
            'excel_file': excel_file,
            'report_file': report_file,
            'total_businesses': total,
            'businesses_with_phones': with_phones,
            'phone_percentage': phone_percentage,
            'success': phone_percentage >= self.MIN_PHONE_PERCENTAGE
        }

def main():
    print("üîç GOOGLE-ONLY SCRAPER")
    print("=" * 50)
    print("üéØ –ü–∞—Ä—Å–∏–Ω–≥ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑ Google")
    print("üìû –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ñ–æ–∫—É—Å –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω—ã")
    print("üîç –°—Ç—Ä–∞–Ω–∏—Ü—ã 2-5 –¥–ª—è –Ω–∏–∑–∫–æ–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π")
    
    scraper = GoogleOnlyScraper()
    
    try:
        target = int(input("\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏–∑–Ω–µ—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 200): ") or "200")
        
        print(f"\nüöÄ –ó–∞–ø—É—Å–∫ Google-only —Å–±–æ—Ä–∞ –¥–ª—è {target} –±–∏–∑–Ω–µ—Å–æ–≤...")
        
        start_time = time.time()
        results = scraper.collect_from_google_exclusively(target)
        elapsed = time.time() - start_time
        
        if results:
            phone_count = sum(1 for b in results if b.get('phone'))
            phone_percentage = (phone_count / len(results)) * 100
            
            print(f"\n‚úÖ –°–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.1f} —Å–µ–∫—É–Ω–¥!")
            print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(results)} –±–∏–∑–Ω–µ—Å–æ–≤")
            print(f"üìû –° —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏: {phone_count} ({phone_percentage:.1f}%)")
            
            export_info = scraper.export_google_results()
            if export_info:
                print(f"\nüìÅ –§–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã:")
                print(f"  ‚Ä¢ {export_info['csv_file']}")
                print(f"  ‚Ä¢ {export_info['excel_file']}")
                print(f"  ‚Ä¢ {export_info['report_file']}")
                
                if export_info['success']:
                    print(f"\nüéâ –£–°–ü–ï–•! –¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞!")
                    print(f"üöÄ –ì–æ—Ç–æ–≤–æ –¥–ª—è outreach –∫–∞–º–ø–∞–Ω–∏–∏!")
                else:
                    print(f"\n‚ö†Ô∏è –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Google")
                    print(f"üìà –°–æ–±–µ—Ä–∏—Ç–µ –±–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏")
        else:
            print("‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–±—Ä–∞–Ω—ã")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main() 