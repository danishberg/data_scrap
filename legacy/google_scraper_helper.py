#!/usr/bin/env python3
"""
Google Scraper Helper - –ü–æ–ª—É–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—ã–¥–∞—á–∏ Google
–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç JavaScript-—Å–∫—Ä–∏–ø—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
"""

import os
import json
import time
import random
import logging
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
from urllib.parse import quote_plus, urljoin
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class GoogleScraperHelper:
    def __init__(self):
        self.session = requests.Session()
        self.logger = self._setup_logging()
        
        # –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JavaScript –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
        self.google_extraction_js = """
        // –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
        function extractGoogleResults() {
            console.log('üîç –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å Google —Å—Ç—Ä–∞–Ω–∏—Ü—ã...');
            
            // –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤–Ω–∏–∑ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            window.scrollTo(0, document.body.scrollHeight);
            
            var results = [];
            var processed = new Set();
            
            // –ò—â–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            var resultElements = document.querySelectorAll('.MjjYud, .g, .rc');
            
            resultElements.forEach(function(item, index) {
                try {
                    // –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Å—ã–ª–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    var linkElement = item.querySelector('a[href]');
                    var titleElement = item.querySelector('h3');
                    var descElement = item.querySelector('.VwiC3b, .s3v9rd, .st');
                    
                    if (linkElement && titleElement) {
                        var href = linkElement.getAttribute('href') || linkElement.getAttribute('data-href');
                        var title = titleElement.innerText || titleElement.textContent;
                        var description = descElement ? (descElement.innerText || descElement.textContent) : '';
                        
                        // –û—á–∏—â–∞–µ–º —Å—Å—ã–ª–∫—É –æ—Ç Google —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
                        if (href && href.startsWith('/url?q=')) {
                            href = decodeURIComponent(href.split('/url?q=')[1].split('&')[0]);
                        }
                        
                        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Å—Å—ã–ª–∫–∏
                        if (href && href.startsWith('http') && !processed.has(href)) {
                            processed.add(href);
                            
                            results.push({
                                position: index + 1,
                                url: href,
                                title: title.trim(),
                                description: description.trim().substring(0, 300),
                                domain: new URL(href).hostname,
                                relevance_score: calculateRelevanceScore(title, description)
                            });
                        }
                    }
                } catch (e) {
                    console.log('–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞:', e);
                }
            });
            
            // –§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            function calculateRelevanceScore(title, description) {
                var text = (title + ' ' + description).toLowerCase();
                var metalKeywords = ['scrap', 'metal', 'recycling', 'salvage', 'junk', 'steel', 'copper', 'aluminum'];
                var score = 0;
                
                metalKeywords.forEach(function(keyword) {
                    if (text.includes(keyword)) score += 1;
                });
                
                return score;
            }
            
            // –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            results.sort(function(a, b) {
                return b.relevance_score - a.relevance_score;
            });
            
            console.log('‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:', results.length);
            return results;
        }
        
        // –ó–∞–ø—É—Å–∫–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        return extractGoogleResults();
        """
        
        # –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–∫—Ä–∞–ø-–±–∏–∑–Ω–µ—Å–æ–≤
        self.search_queries = [
            # –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            'scrap metal recycling "{city}"',
            'metal scrap yard "{city}"',
            'junk yard "{city}"',
            'auto salvage "{city}"',
            'copper recycling "{city}"',
            'aluminum recycling "{city}"',
            'steel recycling "{city}"',
            
            # –î–ª–∏–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–±–æ–ª—å—à–µ —à–∞–Ω—Å–æ–≤ –Ω–∞–π—Ç–∏ –Ω–∏–∑–∫–æ–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —Å–∞–π—Ç—ã)
            'where to sell scrap metal "{city}"',
            'metal recycling center near "{city}"',
            'cash for scrap metal "{city}"',
            'scrap metal prices "{city}"',
            'metal buyers "{city}"',
            'industrial metal recycling "{city}"',
            'construction metal recycling "{city}"',
            'automotive metal recycling "{city}"',
            
            # –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            'local scrap metal dealers "{city}"',
            'small scrap yards "{city}"',
            'family owned metal recycling "{city}"',
            'independent scrap metal "{city}"'
        ]

    def _setup_logging(self):
        logger = logging.getLogger('GoogleScraperHelper')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger

    def generate_google_urls(self, cities, max_pages=5):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º URL'—ã –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –±—Ä–∞—É–∑–µ—Ä–µ"""
        self.logger.info(f"üîó –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º Google URL'—ã –¥–ª—è {len(cities)} –≥–æ—Ä–æ–¥–æ–≤")
        
        urls_data = []
        
        for city in cities:
            city_urls = []
            
            for query_template in self.search_queries:
                query = query_template.format(city=city)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º URL'—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                for page in range(0, max_pages):
                    start = page * 10
                    search_url = f"https://www.google.com/search?q={quote_plus(query)}&start={start}"
                    
                    city_urls.append({
                        'city': city,
                        'query': query,
                        'page': page + 1,
                        'url': search_url,
                        'expected_depth': 'deep' if page >= 2 else 'surface'
                    })
            
            urls_data.extend(city_urls)
        
        return urls_data

    def automated_google_scraping(self, cities, target_per_city=20):
        """–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ Google (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ!)"""
        self.logger.info(f"ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Google –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è {len(cities)} –≥–æ—Ä–æ–¥–æ–≤")
        self.logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ VPN –∏ –±—É–¥—å—Ç–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã —Å —á–∞—Å—Ç–æ—Ç–æ–π –∑–∞–ø—Ä–æ—Å–æ–≤!")
        
        all_results = []
        driver = self._setup_selenium_driver()
        
        try:
            for city in cities:
                city_results = []
                
                self.logger.info(f"üèôÔ∏è –ü–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –≥–æ—Ä–æ–¥–∞: {city}")
                
                # –í—ã–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª—É—á—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≥–æ—Ä–æ–¥–∞
                selected_queries = self.search_queries[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                
                for query_template in selected_queries:
                    if len(city_results) >= target_per_city:
                        break
                    
                    query = query_template.format(city=city)
                    
                    try:
                        # –ü–∞—Ä—Å–∏–º –≥–ª—É–±–æ–∫–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (2-4)
                        page_results = self._scrape_google_pages(driver, query, start_page=2, end_page=4)
                        city_results.extend(page_results)
                        
                        # –ë–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                        time.sleep(random.uniform(15, 30))
                        
                    except Exception as e:
                        self.logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞ '{query}': {e}")
                        continue
                
                all_results.extend(city_results[:target_per_city])
                self.logger.info(f"‚úÖ {city}: —Å–æ–±—Ä–∞–Ω–æ {len(city_results)} —Å—Å—ã–ª–æ–∫")
                
                # –ë–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –≥–æ—Ä–æ–¥–∞–º–∏
                time.sleep(random.uniform(60, 120))
        
        finally:
            driver.quit()
        
        return all_results

    def _setup_selenium_driver(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ–±-–¥—Ä–∞–π–≤–µ—Ä–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é"""
        options = Options()
        
        # –î–µ–ª–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏–º –Ω–∞ –æ–±—ã—á–Ω—ã–π
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # –†–∞–Ω–¥–æ–º–Ω—ã–π User-Agent
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        options.add_argument(f"user-agent={random.choice(user_agents)}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–Ω–¥–æ–º–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –æ–∫–Ω–∞
        window_sizes = ['1920,1080', '1366,768', '1536,864', '1280,720']
        selected_size = random.choice(window_sizes)
        options.add_argument(f"--window-size={selected_size}")
        
        driver = webdriver.Chrome(options=options)
        
        # –°–∫—Ä—ã–≤–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        return driver

    def _scrape_google_pages(self, driver, query, start_page=2, end_page=4):
        """–ü–∞—Ä—Å–∏–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Google –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        results = []
        
        for page in range(start_page, end_page + 1):
            try:
                start = (page - 1) * 10
                search_url = f"https://www.google.com/search?q={quote_plus(query)}&start={start}"
                
                self.logger.info(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {query}")
                driver.get(search_url)
                
                # –†–∞–Ω–¥–æ–º–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
                time.sleep(random.uniform(5, 10))
                
                # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –Ω–∞—à JavaScript –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
                page_results = driver.execute_script(self.google_extraction_js)
                
                if page_results:
                    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    for result in page_results:
                        result['search_query'] = query
                        result['google_page'] = page
                        result['scraped_at'] = datetime.now().isoformat()
                    
                    results.extend(page_results)
                    self.logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(page_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                time.sleep(random.uniform(8, 15))
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {e}")
                continue
        
        return results

    def process_google_links(self, google_results, max_concurrent=5):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–±—Ä–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤"""
        self.logger.info(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(google_results)} —Å—Å—ã–ª–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤")
        
        businesses = []
        
        for i, result in enumerate(google_results):
            try:
                self.logger.info(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {i+1}/{len(google_results)}: {result.get('domain', 'unknown')}")
                
                # –ü–∞—Ä—Å–∏–º —Å–∞–π—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
                business_data = self._extract_business_from_google_result(result)
                
                if business_data:
                    businesses.append(business_data)
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(random.uniform(2, 5))
                
            except Exception as e:
                self.logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {result.get('url', 'unknown')}: {e}")
                continue
        
        return businesses

    def _extract_business_from_google_result(self, google_result):
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –±–∏–∑–Ω–µ—Å–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ Google"""
        url = google_result.get('url', '')
        
        if not url:
            return None
        
        try:
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            response = self._make_safe_request(url, timeout=15)
            if not response or response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
            phone = self._extract_phone_aggressive(soup, response.text)
            email = self._extract_email_aggressive(soup, response.text)
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if not phone and not email:
                return None
            
            business = {
                'name': google_result.get('title', 'Unknown Business'),
                'description': google_result.get('description', ''),
                'website': url,
                'domain': google_result.get('domain', ''),
                'phone': phone,
                'email': email,
                'address': self._extract_address_from_page(soup),
                'google_position': google_result.get('position', 0),
                'google_page': google_result.get('google_page', 1),
                'google_query': google_result.get('search_query', ''),
                'relevance_score': google_result.get('relevance_score', 0),
                'source': 'Google_Deep_Search',
                'scraped_at': datetime.now().isoformat()
            }
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –≥–æ—Ä–æ–¥/—à—Ç–∞—Ç –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            location_info = self._extract_location_info(soup, google_result)
            business.update(location_info)
            
            return business
            
        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å {url}: {e}")
            return None

    def _extract_phone_aggressive(self, soup, page_text):
        """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        phone_patterns = [
            r'tel:\+?1?[-.\s]?\(?(\d{3})\)?[-.\s]?(\d{3})[-.\s]?(\d{4})',
            r'phone[:\s]*\+?1?[-.\s]?\(?(\d{3})\)?[-.\s]?(\d{3})[-.\s]?(\d{4})',
            r'call[:\s]*\+?1?[-.\s]?\(?(\d{3})\)?[-.\s]?(\d{3})[-.\s]?(\d{4})',
            r'\+?1?[-.\s]?\(?(\d{3})\)?[-.\s]?(\d{3})[-.\s]?(\d{4})',
            r'(\d{3})[-.\s](\d{3})[-.\s](\d{4})',
            r'\((\d{3})\)\s*(\d{3})[-.\s](\d{4})',
        ]
        
        # –ò—â–µ–º –≤ HTML –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        phone_elements = soup.select('a[href^="tel:"], [data-phone], .phone, #phone')
        for element in phone_elements:
            phone_text = element.get('href', '') or element.get('data-phone', '') or element.get_text()
            if phone_text:
                phone = self._clean_phone(phone_text.replace('tel:', ''))
                if phone:
                    return phone
        
        # –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        for pattern in phone_patterns:
            matches = re.findall(pattern, page_text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple) and len(match) == 3:
                    phone = f"({match[0]}) {match[1]}-{match[2]}"
                    return phone
        
        return ""

    def _extract_email_aggressive(self, soup, page_text):
        """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ email"""
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        
        # –ò—â–µ–º –≤ HTML
        email_elements = soup.select('a[href^="mailto:"], [data-email]')
        for element in email_elements:
            email_text = element.get('href', '').replace('mailto:', '') or element.get('data-email', '')
            if email_text and '@' in email_text:
                return email_text
        
        # –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ
        emails = re.findall(email_pattern, page_text)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º
        skip_domains = ['example.com', 'test.com', 'google.com', 'facebook.com']
        for email in emails:
            if not any(domain in email.lower() for domain in skip_domains):
                return email
        
        return ""

    def _extract_address_from_page(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞"""
        address_selectors = [
            '[itemtype*="PostalAddress"]',
            '.address', '.location', '.contact-address',
            '.street-address', '.business-address'
        ]
        
        for selector in address_selectors:
            element = soup.select_one(selector)
            if element:
                address = element.get_text().strip()
                if address and any(word in address.lower() for word in ['street', 'ave', 'road', 'drive', 'blvd']):
                    return address[:200]
        
        return ""

    def _extract_location_info(self, soup, google_result):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–∏"""
        location = {}
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        query = google_result.get('search_query', '')
        if '"' in query:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ—Ä–æ–¥ –∏–∑ –∫–∞–≤—ã—á–µ–∫
            city_match = re.search(r'"([^"]+)"', query)
            if city_match:
                city_full = city_match.group(1)
                if ' ' in city_full:
                    parts = city_full.split()
                    location['city'] = ' '.join(parts[:-1])
                    location['state'] = parts[-1]
                else:
                    location['city'] = city_full
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        if not location.get('city'):
            # –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∞–¥—Ä–µ—Å–æ–≤
            page_text = soup.get_text()
            state_pattern = r'\b([A-Z]{2})\s+\d{5}'
            state_matches = re.findall(state_pattern, page_text)
            if state_matches:
                location['state'] = state_matches[0]
        
        return location

    def _clean_phone(self, phone):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        if not phone:
            return ""
        
        digits = re.sub(r'\D', '', phone)
        
        if len(digits) == 10:
            return f"({digits[:3]}) {digits[3:6]}-{digits[6:]}"
        elif len(digits) == 11 and digits[0] == '1':
            return f"({digits[1:4]}) {digits[4:7]}-{digits[7:]}"
        
        return ""

    def _make_safe_request(self, url, timeout=15, max_retries=3):
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π HTTP –∑–∞–ø—Ä–æ—Å"""
        for attempt in range(max_retries):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive'
                }
                
                time.sleep(random.uniform(1, 3))
                response = self.session.get(url, headers=headers, timeout=timeout)
                
                if response.status_code == 200:
                    return response
                
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(2, 5))
        
        return None

    def export_google_extraction_script(self, output_dir="output"):
        """–≠–∫—Å–ø–æ—Ä—Ç JavaScript –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –±—Ä–∞—É–∑–µ—Ä–µ"""
        os.makedirs(output_dir, exist_ok=True)
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é —Å–∫—Ä–∏–ø—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        enhanced_script = """
// –£–õ–£–ß–®–ï–ù–ù–´–ô –°–ö–†–ò–ü–¢ –î–õ–Ø –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø GOOGLE –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
// –ó–∞–ø—É—Å–∫–∞–π—Ç–µ –≤ –∫–æ–Ω—Å–æ–ª–∏ –±—Ä–∞—É–∑–µ—Ä–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Google

javascript:!(function(){
    console.log('üîç –ó–∞–ø—É—Å–∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è Google —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...');
    
    // –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –≤–Ω–∏–∑ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    window.scrollTo(0, document.body.scrollHeight);
    
    var win = window.open('', 'GoogleResults', 'width=800,height=600,scrollbars=yes');
    win.document.write('<html><head><title>Google Results</title></head><body>');
    win.document.write('<h2>–°–æ–±—Ä–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ (' + new Date().toLocaleString() + ')</h2>');
    win.document.write('<p>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ —Å Google:</p>');
    
    var results = [];
    var processed = new Set();
    
    // –ò—â–µ–º –≤—Å–µ –±–ª–æ–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    var resultElements = document.querySelectorAll('.MjjYud, .g, .rc');
    
    resultElements.forEach(function(item, index) {
        try {
            var linkElement = item.querySelector('a[href]');
            var titleElement = item.querySelector('h3');
            var descElement = item.querySelector('.VwiC3b, .s3v9rd, .st, .IsZvec');
            
            if (linkElement && titleElement) {
                var href = linkElement.getAttribute('href') || linkElement.getAttribute('data-href');
                var title = titleElement.innerText || titleElement.textContent;
                var description = descElement ? (descElement.innerText || descElement.textContent) : '';
                
                // –û—á–∏—â–∞–µ–º Google redirect
                if (href && href.startsWith('/url?q=')) {
                    href = decodeURIComponent(href.split('/url?q=')[1].split('&')[0]);
                }
                
                if (href && href.startsWith('http') && !processed.has(href)) {
                    processed.add(href);
                    
                    // –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    var text = (title + ' ' + description).toLowerCase();
                    var metalKeywords = ['scrap', 'metal', 'recycling', 'salvage', 'junk', 'steel', 'copper', 'aluminum'];
                    var relevanceScore = 0;
                    metalKeywords.forEach(function(keyword) {
                        if (text.includes(keyword)) relevanceScore++;
                    });
                    
                    results.push({
                        position: index + 1,
                        url: href,
                        title: title.trim(),
                        description: description.trim().substring(0, 200),
                        domain: new URL(href).hostname,
                        relevance: relevanceScore
                    });
                    
                    // –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–∫–Ω–æ
                    win.document.write('<div style="margin: 10px 0; padding: 10px; border: 1px solid #ddd;">');
                    win.document.write('<strong>–ü–æ–∑–∏—Ü–∏—è ' + (index + 1) + ':</strong> ' + title + '<br>');
                    win.document.write('<a href="' + href + '" target="_blank">' + href + '</a><br>');
                    win.document.write('<small>' + description.substring(0, 150) + '...</small><br>');
                    win.document.write('<em>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: ' + relevanceScore + '/8</em>');
                    win.document.write('</div>');
                }
            }
        } catch (e) {
            console.log('–û—à–∏–±–∫–∞:', e);
        }
    });
    
    // –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    results.sort(function(a, b) { return b.relevance - a.relevance; });
    
    // –î–æ–±–∞–≤–ª—è–µ–º JSON –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
    win.document.write('<hr><h3>JSON –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞:</h3>');
    win.document.write('<textarea rows="10" cols="80">' + JSON.stringify(results, null, 2) + '</textarea>');
    win.document.write('<hr><p>–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: ' + results.length + '</p>');
    win.document.write('</body></html>');
    
    console.log('‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:', results.length);
    console.log('üìä –î–∞–Ω–Ω—ã–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω—ã –≤ –Ω–æ–≤–æ–º –æ–∫–Ω–µ');
    
    return results;
})();
        """
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–ø—Ç
        script_file = os.path.join(output_dir, f"google_extraction_script_{datetime.now().strftime('%Y%m%d_%H%M%S')}.js")
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(enhanced_script)
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é
        instruction_file = os.path.join(output_dir, f"google_scraping_instructions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        with open(instruction_file, 'w', encoding='utf-8') as f:
            f.write("üîç –ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ GOOGLE SCRAPER\n")
            f.write("=" * 60 + "\n\n")
            f.write("1. –ü–û–î–ì–û–¢–û–í–ö–ê:\n")
            f.write("   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ VPN –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏\n")
            f.write("   ‚Ä¢ –û—Ç–∫—Ä–æ–π—Ç–µ –±—Ä–∞—É–∑–µ—Ä –≤ —Ä–µ–∂–∏–º–µ –∏–Ω–∫–æ–≥–Ω–∏—Ç–æ\n")
            f.write("   ‚Ä¢ –û—á–∏—Å—Ç–∏—Ç–µ cookies Google\n\n")
            
            f.write("2. –ü–û–ò–°–ö –í GOOGLE:\n")
            f.write("   ‚Ä¢ –í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å: scrap metal recycling \"–≤–∞—à_–≥–æ—Ä–æ–¥\"\n")
            f.write("   ‚Ä¢ –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã 2-5 (–∏–º–µ–Ω–Ω–æ —Ç–∞–º –∫–æ–º–ø–∞–Ω–∏–∏ —Å –Ω–∏–∑–∫–∏–º–∏ –ø–æ–∑–∏—Ü–∏—è–º–∏)\n")
            f.write("   ‚Ä¢ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ\n\n")
            
            f.write("3. –ó–ê–ü–£–°–ö –°–ö–†–ò–ü–¢–ê:\n")
            f.write("   ‚Ä¢ –û—Ç–∫—Ä–æ–π—Ç–µ Developer Tools (F12)\n")
            f.write("   ‚Ä¢ –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É Console\n")
            f.write("   ‚Ä¢ –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏ –≤—Å—Ç–∞–≤—å—Ç–µ –≤–µ—Å—å JavaScript –∫–æ–¥\n")
            f.write("   ‚Ä¢ –ù–∞–∂–º–∏—Ç–µ Enter\n\n")
            
            f.write("4. –†–ï–ó–£–õ–¨–¢–ê–¢:\n")
            f.write("   ‚Ä¢ –û—Ç–∫—Ä–æ–µ—Ç—Å—è –Ω–æ–≤–æ–µ –æ–∫–Ω–æ —Å —Å–æ–±—Ä–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏\n")
            f.write("   ‚Ä¢ –°–∫–æ–ø–∏—Ä—É–π—Ç–µ JSON –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\n")
            f.write("   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª\n\n")
            
            f.write("5. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\n")
            f.write("   ‚Ä¢ –î–µ–ª–∞–π—Ç–µ –ø–∞—É–∑—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (5-10 –º–∏–Ω—É—Ç)\n")
            f.write("   ‚Ä¢ –§–æ–∫—É—Å–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö 2-5 Google\n")
            f.write("   ‚Ä¢ –ò—â–∏—Ç–µ –≤ —Å—Ä–µ–¥–Ω–∏—Ö –≥–æ—Ä–æ–¥–∞—Ö (–º–µ–Ω—å—à–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏)\n")
            f.write("   ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n\n")
            
            f.write("6. –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•:\n")
            f.write("   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–π JSON —Å –Ω–∞—à–∏–º –ø–∞—Ä—Å–µ—Ä–æ–º\n")
            f.write("   ‚Ä¢ –ü–∞—Ä—Å–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ—á–µ—Ç –∫–æ–Ω—Ç–∞–∫—Ç—ã —Å —Å–∞–π—Ç–æ–≤\n")
            f.write("   ‚Ä¢ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç–¥–∞–µ—Ç—Å—è –∫–æ–º–ø–∞–Ω–∏—è–º —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏\n")
        
        self.logger.info(f"‚úÖ Google extraction script exported:")
        self.logger.info(f"  ‚Ä¢ Script: {script_file}")
        self.logger.info(f"  ‚Ä¢ Instructions: {instruction_file}")
        
        return {
            'script_file': script_file,
            'instruction_file': instruction_file
        }

def main():
    print("üîç GOOGLE SCRAPER HELPER - –ü–æ–ª—É–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫")
    print("=" * 65)
    print("–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–ª—É–±–∏–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Google")
    
    helper = GoogleScraperHelper()
    
    print("\n–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:")
    print("1. –°–æ–∑–¥–∞—Ç—å JavaScript –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
    print("2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ!)")
    print("3. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ Google")
    
    choice = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1-3): ").strip()
    
    if choice == "1":
        # –≠–∫—Å–ø–æ—Ä—Ç JavaScript —Å–∫—Ä–∏–ø—Ç–∞
        result = helper.export_google_extraction_script()
        print(f"\n‚úÖ JavaScript —Å–∫—Ä–∏–ø—Ç —Å–æ–∑–¥–∞–Ω:")
        print(f"  ‚Ä¢ {result['script_file']}")
        print(f"  ‚Ä¢ {result['instruction_file']}")
        print("\nüìñ –°–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ —Ñ–∞–π–ª–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
        
    elif choice == "2":
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥
        print("\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ Google –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –±–ª–æ–∫–∏—Ä–æ–≤–∫–µ!")
        confirm = input("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (y/N): ").strip().lower()
        
        if confirm == 'y':
            cities = input("–í–≤–µ–¥–∏—Ç–µ –≥–æ—Ä–æ–¥–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é: ").split(',')
            cities = [city.strip() for city in cities if city.strip()]
            
            if cities:
                results = helper.automated_google_scraping(cities, target_per_city=10)
                print(f"\n‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Google")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏
                businesses = helper.process_google_links(results)
                print(f"üìû –ù–∞–π–¥–µ–Ω–æ {len(businesses)} –±–∏–∑–Ω–µ—Å–æ–≤ —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏")
        
    elif choice == "3":
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        json_file = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å Google –¥–∞–Ω–Ω—ã–º–∏: ").strip()
        
        if os.path.exists(json_file):
            with open(json_file, 'r') as f:
                google_data = json.load(f)
            
            businesses = helper.process_google_links(google_data)
            print(f"\nüìû –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(businesses)} –±–∏–∑–Ω–µ—Å–æ–≤ —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏")
            
            # –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"google_processed_businesses_{timestamp}.json"
            
            with open(output_file, 'w') as f:
                json.dump(businesses, f, indent=2, default=str)
            
            print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")
        else:
            print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    else:
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")

if __name__ == "__main__":
    main() 