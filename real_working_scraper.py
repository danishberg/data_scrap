#!/usr/bin/env python3
"""
Real Working Scraper - Actually scrapes real data from the internet
Uses legitimate APIs and less-protected sources
"""

import os
import sys
import json
import time
import random
import logging
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
from urllib.parse import quote_plus, urljoin
import re

class RealWorkingScraper:
    def __init__(self):
        self.session = requests.Session()
        self.results = []
        self.logger = self._setup_logging()
        
        # Rotate user agents and headers
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]

        # Add metal types to search for (your requirement)
        self.metal_types = {
            'copper': ['copper', '–º–µ–¥—å', 'cu', 'bare copper', 'copper wire', 'copper tubing'],
            'aluminum': ['aluminum', 'aluminium', '–∞–ª—é–º–∏–Ω–∏–π', 'al', 'aluminum cans', 'aluminum siding'],
            'steel': ['steel', '—Å—Ç–∞–ª—å', 'iron', '–∂–µ–ª–µ–∑–æ', 'scrap steel', 'structural steel'],
            'brass': ['brass', '–ª–∞—Ç—É–Ω—å', 'yellow brass', 'red brass', 'brass fittings'],
            'stainless_steel': ['stainless steel', '–Ω–µ—Ä–∂–∞–≤–µ–π–∫–∞', 'stainless', 'ss'],
            'lead': ['lead', '—Å–≤–∏–Ω–µ—Ü', 'pb', 'lead batteries', 'lead pipes'],
            'zinc': ['zinc', '—Ü–∏–Ω–∫', 'zn', 'galvanized'],
            'nickel': ['nickel', '–Ω–∏–∫–µ–ª—å', 'ni', 'nickel alloy'],
            'tin': ['tin', '–æ–ª–æ–≤–æ', 'sn'],
            'carbide': ['carbide', '–∫–∞—Ä–±–∏–¥', 'tungsten carbide', 'cutting tools'],
            'precious_metals': ['gold', 'silver', 'platinum', 'palladium', 'rhodium'],
            'electronic': ['electronics', 'e-waste', 'circuit boards', 'computer scrap', 'cell phones'],
            'automotive': ['catalytic converters', 'car batteries', 'radiators', 'engines'],
            'wire': ['wire', '–ø—Ä–æ–≤–æ–¥–∞', 'cable', 'insulated wire', 'romex wire'],
            'cast_iron': ['cast iron', '—á—É–≥—É–Ω', 'cast', 'machine parts'],
            'titanium': ['titanium', '—Ç–∏—Ç–∞–Ω', 'ti', 'aerospace grade']
        }
        
        # Services to look for
        self.services = [
            'pickup service', 'container rental', 'demolition', 'roll-off containers',
            'processing', 'sorting', 'weighing', 'cash payment', 'check payment',
            'industrial cleanup', 'auto dismantling', 'certified scales',
            'commercial accounts', 'residential pickup', 'same day pickup'
        ]

    def _setup_logging(self):
        logger = logging.getLogger('RealWorkingScraper')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger

    def scrape_real_sources(self, target_count=100):
        """Scrape from real internet sources"""
        self.logger.info(f"üåê Starting REAL data collection for {target_count} businesses")
        
        # Real sources that work
        sources = [
            self._scrape_overpass_api,  # OpenStreetMap data
            self._scrape_business_listings,
            self._scrape_chamber_commerce,
            self._scrape_recycling_orgs,
            self._scrape_government_sites
        ]
        
        for source_func in sources:
            try:
                self.logger.info(f"üì° Trying {source_func.__name__}...")
                results = source_func(target_count // len(sources))
                
                if results:
                    self.results.extend(results)
                    self.logger.info(f"‚úÖ {source_func.__name__}: {len(results)} real businesses found")
                    
                    if len(self.results) >= target_count:
                        break
                
                # Rate limiting between sources
                time.sleep(random.uniform(3, 8))
                
            except Exception as e:
                self.logger.error(f"‚ùå {source_func.__name__} failed: {e}")
                continue
        
        self.results = self._remove_duplicates(self.results)
        
        # PHASE 2: Enhance businesses with detailed metal/pricing data
        if self.results:
            self.logger.info("üî¨ PHASE 2: Enhancing with detailed metal types and pricing...")
            self.results = self._enhance_businesses_with_details()
        
        self.logger.info(f"üéØ Total REAL businesses collected: {len(self.results)}")
        return self.results

    def _enhance_businesses_with_details(self):
        """Enhance collected businesses with metal types, pricing, and services"""
        enhanced_businesses = []
        
        for i, business in enumerate(self.results[:25]):  # Limit to avoid timeouts
            try:
                self.logger.info(f"üîç Enhancing {i+1}/{min(25, len(self.results))}: {business['name']}")
                
                enhanced = business.copy()
                
                # Extract metal types from business name and data
                enhanced['metal_types'] = self._extract_metal_types(business)
                enhanced['services'] = self._extract_services(business)
                
                # Scrape website if available
                if business.get('website'):
                    website_details = self._scrape_business_website(business['website'], business['name'])
                    enhanced.update(website_details)
                
                # Enhance contact information
                enhanced['contact_quality'] = self._assess_contact_quality(enhanced)
                enhanced['business_type'] = self._classify_business_type(enhanced)
                enhanced['data_completeness'] = self._calculate_data_completeness(enhanced)
                
                enhanced_businesses.append(enhanced)
                
                # Rate limiting
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                self.logger.warning(f"‚ùå Enhancement failed for {business.get('name', 'Unknown')}: {e}")
                enhanced_businesses.append(business)  # Keep original if enhancement fails
                continue
        
        self.logger.info(f"‚úÖ Enhanced {len(enhanced_businesses)} businesses with detailed information")
        return enhanced_businesses

    def _extract_metal_types(self, business):
        """Extract metal types from business data"""
        found_metals = []
        
        # Combine all text sources
        text_sources = [
            business.get('name', ''),
            business.get('description', ''),
            business.get('shop_type', ''),
            business.get('recycling_materials', '')
        ]
        
        full_text = ' '.join(str(s) for s in text_sources).lower()
        
        # Check for each metal type
        for metal_category, keywords in self.metal_types.items():
            for keyword in keywords:
                if keyword.lower() in full_text:
                    found_metals.append(metal_category)
                    break  # Found this category, move to next
        
        return list(set(found_metals))  # Remove duplicates

    def _extract_services(self, business):
        """Extract services from business data"""
        found_services = []
        
        text_sources = [
            business.get('name', ''),
            business.get('description', ''),
            business.get('shop_type', '')
        ]
        
        full_text = ' '.join(str(s) for s in text_sources).lower()
        
        for service in self.services:
            if service.lower() in full_text:
                found_services.append(service)
        
        return found_services

    def _scrape_business_website(self, website_url, business_name):
        """Scrape business website for detailed metal types and pricing"""
        details = {}
        
        try:
            self.logger.info(f"üåê Scraping website: {website_url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
            
            response = self.session.get(website_url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.text, 'html.parser')
                text_content = soup.get_text().lower()
                
                # Extract detailed information
                details['metals_from_website'] = self._extract_metals_from_text(text_content)
                details['services_from_website'] = self._extract_services_from_text(text_content)
                details['pricing_from_website'] = self._extract_pricing_from_text(text_content)
                details['phone_from_website'] = self._extract_phone_from_text(text_content)
                details['email_from_website'] = self._extract_email_from_text(text_content)
                details['hours_from_website'] = self._extract_hours_from_text(text_content)
                
                self.logger.info(f"‚úÖ Website data extracted for {business_name}")
            else:
                self.logger.warning(f"‚ùå HTTP {response.status_code} for {website_url}")
                
        except Exception as e:
            self.logger.warning(f"‚ùå Website scraping failed for {website_url}: {e}")
        
        return details

    def _extract_metals_from_text(self, text):
        """Extract metal types from website text"""
        found_metals = []
        
        for metal_category, keywords in self.metal_types.items():
            for keyword in keywords:
                if keyword.lower() in text:
                    found_metals.append(metal_category)
                    break
        
        return list(set(found_metals))

    def _extract_services_from_text(self, text):
        """Extract services from website text"""
        found_services = []
        
        for service in self.services:
            if service.lower() in text:
                found_services.append(service)
        
        return found_services

    def _extract_pricing_from_text(self, text):
        """Extract pricing information from text"""
        import re
        pricing_info = []
        
        # Look for price patterns
        price_patterns = [
            r'\$\d+\.?\d*\s*per\s*pound',
            r'\$\d+\.?\d*\s*/\s*lb',
            r'\$\d+\.?\d*\s*per\s*ton',
            r'copper.*?\$\d+\.?\d*',
            r'aluminum.*?\$\d+\.?\d*',
            r'steel.*?\$\d+\.?\d*',
            r'brass.*?\$\d+\.?\d*',
            r'current\s*price.*?\$\d+\.?\d*'
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            pricing_info.extend(matches)
        
        return pricing_info[:5]  # Limit results

    def _extract_phone_from_text(self, text):
        """Extract phone numbers from website text"""
        import re
        phone_patterns = [
            r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',
            r'1[-.\s]?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}'
        ]
        
        for pattern in phone_patterns:
            matches = re.findall(pattern, text)
            if matches:
                return matches[0]
        
        return ""

    def _extract_email_from_text(self, text):
        """Extract email addresses from website text"""
        import re
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        matches = re.findall(email_pattern, text)
        return matches[0] if matches else ""

    def _extract_hours_from_text(self, text):
        """Extract business hours from website text"""
        import re
        hour_patterns = [
            r'mon.*?fri.*?\d{1,2}:\d{2}.*?\d{1,2}:\d{2}',
            r'monday.*?friday.*?\d{1,2}:\d{2}.*?\d{1,2}:\d{2}',
            r'hours.*?\d{1,2}:\d{2}.*?\d{1,2}:\d{2}',
            r'open.*?\d{1,2}:\d{2}.*?\d{1,2}:\d{2}'
        ]
        
        for pattern in hour_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
            if matches:
                return matches[0][:100]  # Limit length
        
        return ""

    def _assess_contact_quality(self, business):
        """Assess quality of contact information"""
        score = 0
        total_fields = 5
        
        if business.get('phone') or business.get('phone_from_website'):
            score += 1
        if business.get('website'):
            score += 1
        if business.get('email') or business.get('email_from_website'):
            score += 1
        if business.get('address'):
            score += 1
        if business.get('latitude') and business.get('longitude'):
            score += 1
        
        return f"{score}/{total_fields}"

    def _classify_business_type(self, business):
        """Classify type of metal business"""
        name = business.get('name', '').lower()
        shop_type = business.get('shop_type', '').lower()
        
        if 'scrap' in name or 'scrap' in shop_type:
            return 'Scrap Metal Dealer'
        elif 'auto' in name or 'salvage' in name:
            return 'Auto Salvage'
        elif 'electronic' in name or 'computer' in name:
            return 'Electronic Recycling'
        elif 'recycling' in name or 'recycling' in shop_type:
            return 'General Recycling'
        else:
            return 'Metal Processing'

    def _calculate_data_completeness(self, business):
        """Calculate data completeness percentage"""
        important_fields = [
            'name', 'address', 'city', 'state', 'phone', 'website',
            'latitude', 'longitude', 'metal_types', 'services'
        ]
        
        filled_fields = 0
        for field in important_fields:
            value = business.get(field)
            if value and (not isinstance(value, list) or len(value) > 0):
                filled_fields += 1
        
        return int((filled_fields / len(important_fields)) * 100)

    def _scrape_overpass_api(self, count):
        """Use Overpass API to get real business data from OpenStreetMap"""
        results = []
        
        try:
            base_url = "https://overpass-api.de/api/interpreter"
            
            # Overpass query for scrap yards and recycling centers
            query = """
            [out:json][timeout:25];
            (
              node["shop"="scrap_yard"](bbox);
              node["amenity"="recycling"](bbox);
              node["industrial"="scrap_yard"](bbox);
              way["shop"="scrap_yard"](bbox);
              way["amenity"="recycling"](bbox);
              way["industrial"="scrap_yard"](bbox);
            );
            out center meta;
            """
            
            # Search in major metropolitan areas
            bboxes = [
                "41.49,-87.92,42.02,-87.52",  # Chicago
                "29.52,-95.67,30.11,-95.07",  # Houston  
                "33.93,-84.67,34.25,-84.13",  # Atlanta
                "39.72,-75.28,40.14,-74.95",  # Philadelphia
                "42.23,-83.29,42.45,-82.91"   # Detroit
            ]
            
            for bbox in bboxes[:3]:  # Limit to avoid timeouts
                try:
                    bbox_query = query.replace("(bbox)", f"({bbox})")
                    
                    response = self._make_safe_request(base_url, data=bbox_query, method='POST')
                    if response and response.status_code == 200:
                        data = response.json()
                        businesses = self._parse_overpass_results(data)
                        results.extend(businesses)
                        self.logger.info(f"Found {len(businesses)} businesses in bbox {bbox}")
                    
                    time.sleep(random.uniform(2, 5))
                    
                except Exception as e:
                    self.logger.warning(f"Overpass error for bbox {bbox}: {e}")
                    continue
                    
        except Exception as e:
            self.logger.error(f"Overpass API error: {e}")
        
        return results

    def _scrape_business_listings(self, count):
        """Scrape from business listing sites that allow scraping"""
        results = []
        
        try:
            # Use the Internet Archive Wayback Machine for historical business data
            wayback_url = "https://web.archive.org/web/20231201000000*/yellowpages.com/search*scrap*metal"
            
            response = self._make_safe_request(wayback_url)
            if response and response.status_code == 200:
                # Parse archived business listings
                businesses = self._parse_archived_listings(response.text)
                results.extend(businesses[:count])
                
        except Exception as e:
            self.logger.error(f"Business listings error: {e}")
        
        return results

    def _scrape_chamber_commerce(self, count):
        """Scrape from Chamber of Commerce directories"""
        results = []
        
        try:
            # Many local chambers have open business directories
            chamber_urls = [
                "https://www.detroitchamber.com/member-directory/",
                "https://www.houstonchamber.com/member-directory/",
                "https://www.chicagolandchamber.org/member-directory/"
            ]
            
            for chamber_url in chamber_urls:
                try:
                    response = self._make_safe_request(chamber_url)
                    if response and response.status_code == 200:
                        businesses = self._parse_chamber_directory(response.text)
                        results.extend(businesses[:count//len(chamber_urls)])
                    
                    time.sleep(random.uniform(3, 7))
                    
                except Exception as e:
                    self.logger.warning(f"Chamber scraping error: {e}")
                    continue
                    
        except Exception as e:
            self.logger.error(f"Chamber of Commerce error: {e}")
        
        return results

    def _scrape_recycling_orgs(self, count):
        """Scrape from recycling organization websites"""
        results = []
        
        try:
            # Institute of Scrap Recycling Industries member directory
            isri_url = "https://www.isri.org/members/member-directory"
            
            response = self._make_safe_request(isri_url)
            if response and response.status_code == 200:
                businesses = self._parse_isri_directory(response.text)
                results.extend(businesses[:count])
                
        except Exception as e:
            self.logger.error(f"ISRI directory error: {e}")
        
        return results

    def _scrape_government_sites(self, count):
        """Scrape from government environmental/recycling databases"""
        results = []
        
        try:
            # EPA RCRAInfo public database
            epa_url = "https://enviro.epa.gov/enviro/efservice/br_facility/state_code/TX/JSON"
            
            response = self._make_safe_request(epa_url)
            if response and response.status_code == 200:
                data = response.json()
                businesses = self._parse_epa_facilities(data)
                results.extend(businesses[:count])
                
        except Exception as e:
            self.logger.error(f"EPA database error: {e}")
        
        return results

    def _make_safe_request(self, url, params=None, data=None, method='GET', max_retries=3):
        """Make HTTP request with proper error handling and retries"""
        for attempt in range(max_retries):
            try:
                headers = {
                    'User-Agent': random.choice(self.user_agents),
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                    'Cache-Control': 'max-age=0'
                }
                
                time.sleep(random.uniform(1, 3))
                
                if method == 'POST':
                    response = self.session.post(url, headers=headers, data=data, timeout=30)
                else:
                    response = self.session.get(url, headers=headers, params=params, timeout=30)
                
                if response.status_code == 200:
                    return response
                elif response.status_code == 429:
                    wait_time = (attempt + 1) * 10
                    self.logger.warning(f"Rate limited, waiting {wait_time}s")
                    time.sleep(wait_time)
                else:
                    self.logger.warning(f"HTTP {response.status_code} for {url}")
                    
            except Exception as e:
                self.logger.warning(f"Request error (attempt {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(2, 5))
        
        return None

    def _parse_overpass_results(self, data):
        """Parse OpenStreetMap Overpass API results"""
        businesses = []
        
        try:
            if 'elements' in data:
                for element in data['elements']:
                    tags = element.get('tags', {})
                    
                    # Extract business information
                    name = tags.get('name', tags.get('operator', 'Unknown Business'))
                    
                    # Get coordinates
                    lat = element.get('lat')
                    lon = element.get('lon')
                    
                    # For ways, use center coordinates
                    if not lat and 'center' in element:
                        lat = element['center']['lat']
                        lon = element['center']['lon']
                    
                    business = {
                        'name': name,
                        'address': self._build_address_from_tags(tags),
                        'city': tags.get('addr:city', ''),
                        'state': tags.get('addr:state', ''),
                        'zip_code': tags.get('addr:postcode', ''),
                        'phone': tags.get('phone', ''),
                        'website': tags.get('website', ''),
                        'latitude': lat,
                        'longitude': lon,
                        'shop_type': tags.get('shop', tags.get('amenity', tags.get('industrial', ''))),
                        'opening_hours': tags.get('opening_hours', ''),
                        'source': 'OpenStreetMap',
                        'osm_id': element.get('id'),
                        'scraped_at': datetime.now().isoformat()
                    }
                    
                    if name != 'Unknown Business':
                        businesses.append(business)
                        
        except Exception as e:
            self.logger.error(f"Error parsing Overpass results: {e}")
        
        return businesses

    def _build_address_from_tags(self, tags):
        """Build address from OSM tags"""
        address_parts = []
        
        if 'addr:housenumber' in tags:
            address_parts.append(tags['addr:housenumber'])
        if 'addr:street' in tags:
            address_parts.append(tags['addr:street'])
            
        return ' '.join(address_parts) if address_parts else ''

    def _parse_archived_listings(self, html):
        """Parse archived business listings"""
        businesses = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Look for archived business links
            links = soup.find_all('a', href=True)
            for link in links:
                if any(term in link.get_text().lower() for term in ['scrap', 'metal', 'recycl']):
                    business = {
                        'name': link.get_text(strip=True),
                        'source': 'Archived Listings',
                        'scraped_at': datetime.now().isoformat()
                    }
                    businesses.append(business)
                    
        except Exception as e:
            self.logger.error(f"Error parsing archived listings: {e}")
        
        return businesses

    def _parse_chamber_directory(self, html):
        """Parse Chamber of Commerce directory"""
        businesses = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Look for member listings
            member_elements = soup.find_all(['div', 'li', 'tr'], 
                                          class_=re.compile(r'member|business|directory'))
            
            for element in member_elements:
                text = element.get_text().lower()
                if any(term in text for term in ['metal', 'scrap', 'recycl', 'salvage']):
                    name_elem = element.find(['h2', 'h3', 'h4', 'a'])
                    if name_elem:
                        business = {
                            'name': name_elem.get_text(strip=True),
                            'source': 'Chamber of Commerce',
                            'scraped_at': datetime.now().isoformat()
                        }
                        businesses.append(business)
                        
        except Exception as e:
            self.logger.error(f"Error parsing chamber directory: {e}")
        
        return businesses

    def _parse_isri_directory(self, html):
        """Parse ISRI member directory"""
        businesses = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Look for member companies
            member_elements = soup.find_all(['div', 'li'], 
                                          class_=re.compile(r'member|company'))
            
            for element in member_elements:
                name_elem = element.find(['h3', 'h4', 'a'])
                address_elem = element.find(class_=re.compile(r'address|location'))
                
                if name_elem:
                    business = {
                        'name': name_elem.get_text(strip=True),
                        'address': address_elem.get_text(strip=True) if address_elem else '',
                        'source': 'ISRI Directory',
                        'scraped_at': datetime.now().isoformat()
                    }
                    businesses.append(business)
                    
        except Exception as e:
            self.logger.error(f"Error parsing ISRI directory: {e}")
        
        return businesses

    def _parse_epa_facilities(self, data):
        """Parse EPA facility data"""
        businesses = []
        
        try:
            if isinstance(data, list):
                for facility in data[:10]:  # Limit results
                    if 'metal' in facility.get('facility_name', '').lower():
                        business = {
                            'name': facility.get('facility_name', ''),
                            'address': facility.get('location_address', ''),
                            'city': facility.get('location_city', ''),
                            'state': facility.get('location_state', ''),
                            'zip_code': facility.get('location_zip_code', ''),
                            'epa_id': facility.get('handler_id', ''),
                            'source': 'EPA Database',
                            'scraped_at': datetime.now().isoformat()
                        }
                        businesses.append(business)
                        
        except Exception as e:
            self.logger.error(f"Error parsing EPA data: {e}")
        
        return businesses

    def _remove_duplicates(self, data):
        """Remove duplicate entries"""
        seen = set()
        unique_data = []
        
        for item in data:
            key = item['name'].lower().strip()
            if key and key not in seen and len(key) > 3:
                seen.add(key)
                unique_data.append(item)
        
        return unique_data

    def export_results(self, output_dir="output"):
        """Export enhanced results with metal types and pricing analysis"""
        if not self.results:
            self.logger.warning("No enhanced data to export")
            return None
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Convert results to DataFrame
        df = pd.DataFrame(self.results)
        
        # Export to CSV
        csv_file = os.path.join(output_dir, f"enhanced_scrap_centers_{timestamp}.csv")
        df.to_csv(csv_file, index=False)
        
        # Export to Excel with multiple analysis sheets
        excel_file = os.path.join(output_dir, f"enhanced_scrap_centers_{timestamp}.xlsx")
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            # Main data sheet
            df.to_excel(writer, sheet_name='All Businesses', index=False)
            
            # Metal types analysis sheet
            if any('metal_types' in str(b) for b in self.results):
                metal_analysis = self._create_metal_analysis()
                if not metal_analysis.empty:
                    metal_analysis.to_excel(writer, sheet_name='Metal Types Analysis', index=False)
            
            # High quality businesses (>60% complete data)
            high_quality_mask = df.get('data_completeness', pd.Series([0]*len(df))) > 60
            high_quality = df[high_quality_mask] if high_quality_mask.any() else pd.DataFrame()
            if not high_quality.empty:
                high_quality.to_excel(writer, sheet_name='High Quality Businesses', index=False)
            
            # Business types summary
            if 'business_type' in df.columns:
                type_summary = df['business_type'].value_counts().reset_index()
                type_summary.columns = ['Business Type', 'Count']
                type_summary.to_excel(writer, sheet_name='Business Types', index=False)
        
        # Export to JSON
        json_file = os.path.join(output_dir, f"enhanced_scrap_centers_{timestamp}.json")
        with open(json_file, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        # Create enhanced summary report
        self._create_enhanced_summary_report(output_dir, timestamp)
        
        self.logger.info(f"‚úÖ Enhanced data exported:")
        self.logger.info(f"  ‚Ä¢ CSV: {csv_file}")
        self.logger.info(f"  ‚Ä¢ Excel: {excel_file}")
        self.logger.info(f"  ‚Ä¢ JSON: {json_file}")
        
        return {
            'csv': csv_file,
            'excel': excel_file,
            'json': json_file,
            'count': len(self.results)
        }

    def _create_metal_analysis(self):
        """Create metal types analysis for Excel export"""
        metal_counts = {}
        
        for business in self.results:
            metals = business.get('metal_types', [])
            if isinstance(metals, list):
                for metal in metals:
                    metal_counts[metal] = metal_counts.get(metal, 0) + 1
        
        if not metal_counts:
            return pd.DataFrame()
        
        analysis_data = []
        total_businesses = len(self.results)
        
        for metal, count in sorted(metal_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_businesses) * 100 if total_businesses > 0 else 0
            analysis_data.append({
                'Metal Type': metal.replace('_', ' ').title(),
                'Businesses Count': count,
                'Percentage': f"{percentage:.1f}%"
            })
        
        return pd.DataFrame(analysis_data)

    def _create_enhanced_summary_report(self, output_dir, timestamp):
        """Create comprehensive summary report with metal types and business analysis"""
        report_file = os.path.join(output_dir, f"enhanced_analysis_report_{timestamp}.txt")
        
        total_businesses = len(self.results)
        with_metals = sum(1 for b in self.results if b.get('metal_types'))
        with_pricing = sum(1 for b in self.results if b.get('pricing_from_website'))
        with_websites = sum(1 for b in self.results if b.get('website'))
        with_phones = sum(1 for b in self.results if b.get('phone') or b.get('phone_from_website'))
        
        avg_completeness = sum(b.get('data_completeness', 0) for b in self.results) / total_businesses if total_businesses > 0 else 0
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("üîç ENHANCED SCRAP METAL CENTERS ANALYSIS REPORT\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Collection Method: Real Internet Data + Enhanced Website Analysis\n\n")
            
            f.write("üìä COMPREHENSIVE DATA SUMMARY\n")
            f.write("-" * 35 + "\n")
            f.write(f"Total Businesses Collected: {total_businesses}\n")
            f.write(f"Average Data Completeness: {avg_completeness:.1f}%\n")
            f.write(f"Businesses with Metal Types: {with_metals} ({with_metals/total_businesses*100:.1f}%)\n")
            f.write(f"Businesses with Pricing Info: {with_pricing} ({with_pricing/total_businesses*100:.1f}%)\n")
            f.write(f"Businesses with Websites: {with_websites} ({with_websites/total_businesses*100:.1f}%)\n")
            f.write(f"Businesses with Phone Numbers: {with_phones} ({with_phones/total_businesses*100:.1f}%)\n\n")
            
            # Metal types breakdown
            if with_metals > 0:
                f.write("üîß METAL TYPES ACCEPTED (Top 10)\n")
                f.write("-" * 35 + "\n")
                metal_analysis = self._create_metal_analysis()
                for _, row in metal_analysis.head(10).iterrows():
                    f.write(f"{row['Metal Type']}: {row['Businesses Count']} businesses ({row['Percentage']})\n")
                f.write("\n")
            
            # Top quality businesses
            f.write("‚≠ê HIGHEST QUALITY BUSINESSES\n")
            f.write("-" * 30 + "\n")
            quality_businesses = sorted(
                [b for b in self.results if b.get('data_completeness', 0) > 50],
                key=lambda x: x.get('data_completeness', 0),
                reverse=True
            )[:10]
            
            for i, business in enumerate(quality_businesses, 1):
                f.write(f"{i}. {business['name']}\n")
                f.write(f"   üìä Data Completeness: {business.get('data_completeness', 0)}%\n")
                f.write(f"   üìç Location: {business.get('city', 'N/A')}, {business.get('state', 'N/A')}\n")
                if business.get('phone') or business.get('phone_from_website'):
                    phone = business.get('phone') or business.get('phone_from_website', '')
                    f.write(f"   üìû Phone: {phone}\n")
                if business.get('website'):
                    f.write(f"   üåê Website: {business['website']}\n")
                if business.get('metal_types'):
                    metals = ', '.join(business['metal_types'][:4])  # Show top 4 metals
                    f.write(f"   üîß Accepts: {metals}\n")
                if business.get('business_type'):
                    f.write(f"   üè≠ Type: {business['business_type']}\n")
                f.write("\n")
            
            # Geographic distribution
            f.write("üìç GEOGRAPHIC DISTRIBUTION\n")
            f.write("-" * 25 + "\n")
            states = {}
            for business in self.results:
                state = business.get('state', 'Unknown')
                if state:
                    states[state] = states.get(state, 0) + 1
            
            for state, count in sorted(states.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_businesses) * 100
                f.write(f"{state}: {count} businesses ({percentage:.1f}%)\n")
            f.write("\n")
            
            f.write("üí° KEY BUSINESS INSIGHTS\n")
            f.write("-" * 25 + "\n")
            f.write("‚Ä¢ Metal recycling businesses are concentrated in industrial areas\n")
            f.write("‚Ä¢ Most established businesses accept multiple metal types\n")
            f.write("‚Ä¢ Businesses with websites generally have higher data completeness\n")
            f.write("‚Ä¢ Contact information quality varies significantly by region\n")
            f.write("‚Ä¢ Pricing information is typically available on business websites\n\n")
            
            f.write("üéØ ACTIONABLE RECOMMENDATIONS\n")
            f.write("-" * 30 + "\n")
            f.write("1. Prioritize businesses with 70%+ data completeness for outreach\n")
            f.write("2. Focus on multi-metal businesses for diverse partnership opportunities\n")
            f.write("3. Contact businesses with websites for detailed pricing negotiations\n")
            f.write("4. Use geographic clusters for efficient logistics planning\n")
            f.write("5. Verify all contact information before business meetings\n")
            f.write("6. Consider specialized approach for electronic/precious metal dealers\n")
            f.write("7. Build relationships with high-volume scrap metal dealers first\n")
        
        self.logger.info(f"‚úÖ Enhanced analysis report created: {report_file}")

def main():
    print("üîç ENHANCED Internet Data Scraper for Scrap Metal Centers")
    print("=" * 65)
    print("Collects REAL data from internet sources + Enhanced Analysis:")
    print("‚úì Metal types accepted (copper, aluminum, steel, etc.)")
    print("‚úì Pricing information where available") 
    print("‚úì Services offered (pickup, containers, processing)")
    print("‚úì Complete contact details and business hours")
    print("‚úì Data quality scoring and business classification")
    
    scraper = RealWorkingScraper()
    
    try:
        target_count = int(input("\nEnter target number of businesses to collect (default 50): ") or "50")
        
        print(f"\nüöÄ Starting ENHANCED collection for {target_count} businesses...")
        print("Process:")
        print("1. üì° Searching OpenStreetMap for real business locations")
        print("2. üî¨ Enhancing data with metal types and pricing info")
        print("3. üåê Scraping business websites for detailed information")
        print("4. üìä Creating comprehensive analysis reports")
        print("\nThis process may take several minutes...")
        
        results = scraper.scrape_real_sources(target_count)
        
        if results:
            print(f"\n‚úÖ Successfully collected {len(results)} enhanced businesses!")
            
            export_info = scraper.export_results()
            if export_info:
                print(f"\nüìÅ Enhanced data files created:")
                print(f"  ‚Ä¢ CSV: {export_info['csv']}")
                print(f"  ‚Ä¢ Excel: {export_info['excel']}")
                print(f"  ‚Ä¢ JSON: {export_info['json']}")
                print(f"\nüéØ Total enhanced records: {export_info['count']}")
                print("\nüîç EXCEL FILE CONTAINS DETAILED ANALYSIS:")
                print("  ‚Üí 'Metal Types Analysis' sheet - breakdown by metal types")
                print("  ‚Üí 'High Quality Businesses' sheet - best businesses for outreach")
                print("  ‚Üí 'Business Types' sheet - classification summary")
                print("  ‚Üí Enhanced analysis report with actionable insights")
            else:
                print("‚ùå Export failed")
        else:
            print("‚ùå No enhanced data collected. Check internet connection.")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Process interrupted by user")
    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    main() 