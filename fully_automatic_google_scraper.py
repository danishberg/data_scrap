#!/usr/bin/env python3
"""
–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–û–õ–ù–û–°–¢–¨–Æ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô GOOGLE –ü–ê–†–°–ï–†
100% –¢–û–ß–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –î–ê–ù–ù–´–•
"""

import os
import sys
import json
import time
import random
import logging
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
from urllib.parse import quote_plus, urljoin, urlparse
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import urllib3
urllib3.disable_warnings()

class AccurateGoogleScraper:
    def __init__(self):
        self.session = requests.Session()
        self.results = []
        self.processed_urls = set()
        self.failed_searches = []
        self.blocked_count = 0
        self.success_count = 0
        self.logger = self._setup_logging()
        
        # –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø 100% –¢–û–ß–ù–û–°–¢–ò
        self.MIN_PHONE_PERCENTAGE = 85  # 85% —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏
        self.MAX_PAGES_PER_QUERY = 4    # –°—Ç—Ä–∞–Ω–∏—Ü—ã 2-5
        self.TIMEOUT = 20               # –¢–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
        
        # –¢–û–ß–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´ –¢–ï–õ–ï–§–û–ù–û–í
        self.phone_patterns = [
            # –û—Å–Ω–æ–≤–Ω—ã–µ US —Ñ–æ—Ä–º–∞—Ç—ã
            re.compile(r'\b1?[\s\-\.]?\(?([0-9]{3})\)?[\s\-\.]?([0-9]{3})[\s\-\.]?([0-9]{4})\b'),
            re.compile(r'\b\(?([0-9]{3})\)?[\s\-\.]?([0-9]{3})[\s\-\.]?([0-9]{4})\b'),
            re.compile(r'([0-9]{3})[\s\-\.]([0-9]{3})[\s\-\.]([0-9]{4})'),
            re.compile(r'([0-9]{3})[^\d\s]+([0-9]{3})[^\d\s]+([0-9]{4})'),
            re.compile(r'tel:[\s]*\+?1?[\s]*\(?([0-9]{3})\)?[\s]*[\-\.]?[\s]*([0-9]{3})[\s]*[\-\.]?[\s]*([0-9]{4})', re.IGNORECASE),
        ]
        
        # –ü–û–ò–°–ö–û–í–´–ï –ó–ê–ü–†–û–°–´
        self.search_queries = [
            'scrap metal dealers',
            'metal recycling center', 
            'scrap yard',
            'junk yard auto parts',
            'copper scrap buyers',
            'aluminum recycling',
            'auto salvage yard',
            'scrap metal pickup',
            'recycling center',
            'metal scrap dealers'
        ]
        
        # –¶–ï–õ–ï–í–´–ï –ì–û–†–û–î–ê
        self.target_cities = [
            'Akron OH', 'Toledo OH', 'Dayton OH', 'Youngstown OH',
            'Rochester NY', 'Syracuse NY', 'Buffalo NY', 'Albany NY',
            'Scranton PA', 'Allentown PA', 'Reading PA', 'Erie PA',
            'Flint MI', 'Lansing MI', 'Kalamazoo MI', 'Grand Rapids MI',
            'Rockford IL', 'Peoria IL', 'Springfield IL', 'Decatur IL',
            'Fort Wayne IN', 'Evansville IN', 'South Bend IN', 'Gary IN',
            'Green Bay WI', 'Appleton WI', 'Oshkosh WI', 'Racine WI',
            'Cedar Rapids IA', 'Davenport IA', 'Sioux City IA',
            'Springfield MO', 'Columbia MO', 'Joplin MO',
            'Little Rock AR', 'Fayetteville AR', 'Jonesboro AR'
        ]
        
        # USER AGENTS
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
        ]
        
        self._init_session()

    def _init_session(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏"""
        self.session.headers.update({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })

    def _setup_logging(self):
        logger = logging.getLogger('AccurateGoogleScraper')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
        
        return logger

    def run_accurate_scraping(self, target_businesses=500):
        """–¢–û–ß–ù–´–ô —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é"""
        self.logger.info(f"üéØ –ó–ê–ü–£–°–ö –¢–û–ß–ù–û–ì–û –°–ë–û–†–ê")
        self.logger.info(f"üìû –¶–ï–õ–¨: {target_businesses} –±–∏–∑–Ω–µ—Å–æ–≤ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏")
        
        start_time = time.time()
        
        # –≠—Ç–∞–ø 1: –°–±–æ—Ä —Å—Å—ã–ª–æ–∫
        all_links = self._collect_business_links()
        self.logger.info(f"üîó –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(all_links)}")
        
        # –≠—Ç–∞–ø 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
        businesses = self._extract_accurate_contacts(all_links, target_businesses)
        
        # –≠—Ç–∞–ø 3: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
        self.results = self._finalize_accurate_results(businesses, target_businesses)
        
        elapsed = time.time() - start_time
        phone_percentage = self._calculate_phone_percentage()
        
        self.logger.info(f"‚úÖ –¢–û–ß–ù–´–ô –°–ë–û–† –ó–ê–í–ï–†–®–ï–ù –∑–∞ {elapsed/60:.1f} –º–∏–Ω—É—Ç")
        self.logger.info(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢: {len(self.results)} –±–∏–∑–Ω–µ—Å–æ–≤, {phone_percentage:.1f}% —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏")
        
        return self.results

    def _collect_business_links(self):
        """–°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∏–∑ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        all_links = []
        
        for city in self.target_cities[:15]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            for query in self.search_queries[:8]:
                full_query = f"{query} {city}"
                
                # –ü–æ–∏—Å–∫ –≤ Bing (—Å—Ç—Ä–∞–Ω–∏—Ü—ã 2-5)
                for page in range(2, 6):
                    try:
                        links = self._search_bing_page(full_query, page)
                        all_links.extend(links)
                        
                        # –ü—Ä–æ–≥—Ä–µ—Å—Å
                        progress = ((self.target_cities.index(city) + 1) / len(self.target_cities[:15])) * 100
                        if len(all_links) % 50 == 0:
                            self.logger.info(f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}% | –°—Å—ã–ª–æ–∫: {len(all_links)}")
                        
                        # –ó–∞–¥–µ—Ä–∂–∫–∞
                        time.sleep(random.uniform(0.5, 1.5))
                        
                        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ —Ü–µ–ª–∏
                        if len(all_links) >= 200:
                            self.logger.info(f"üéØ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Å—ã–ª–æ–∫ —Å–æ–±—Ä–∞–Ω–æ, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–∏—Å–∫")
                            return self._deduplicate_links(all_links)
                            
                    except Exception as e:
                        self.logger.debug(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
                        continue
        
        return self._deduplicate_links(all_links)

    def _search_bing_page(self, query, page):
        """–ü–æ–∏—Å–∫ –≤ Bing"""
        try:
            start = (page - 1) * 10
            url = f"https://www.bing.com/search?q={quote_plus(query)}&first={start}"
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Referer': 'https://www.bing.com/',
                'Connection': 'keep-alive'
            }
            
            response = self.session.get(url, headers=headers, timeout=self.TIMEOUT, verify=False)
            
            if response.status_code == 200:
                return self._extract_links_from_bing(response.text, query, page)
            else:
                return []
                
        except Exception as e:
            self.logger.debug(f"Bing search error: {e}")
            return []

    def _extract_links_from_bing(self, html, query, page):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏–∑ Bing"""
        links = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ü–æ–∏—Å–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Bing
            for result in soup.find_all('li', class_='b_algo'):
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫–∞
                title_elem = result.find('h2')
                if not title_elem:
                    continue
                    
                link_elem = title_elem.find('a')
                if not link_elem or not link_elem.get('href'):
                    continue
                
                url = link_elem.get('href')
                title = title_elem.get_text(strip=True)
                
                # –û–ø–∏—Å–∞–Ω–∏–µ
                desc_elem = result.find('div', class_='b_caption')
                description = desc_elem.get_text(strip=True) if desc_elem else ''
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if self._is_relevant_for_scrap_metal(title, description):
                    links.append({
                        'url': url,
                        'title': title,
                        'description': description,
                        'page': page,
                        'query': query,
                        'source': 'Bing'
                    })
                    
        except Exception as e:
            self.logger.debug(f"Link extraction error: {e}")
        
        return links

    def _is_relevant_for_scrap_metal(self, title, description):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–∫—Ä–∞–ø-–º–µ—Ç–∞–ª–∞"""
        text = (title + ' ' + description).lower()
        
        # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        relevant_keywords = [
            'scrap', 'metal', 'recycling', 'salvage', 'junk', 'auto parts',
            'copper', 'aluminum', 'steel', 'iron', 'brass', 'junkyard',
            'scrapyard', 'recycler', 'metals', 'automotive', 'dismantling'
        ]
        
        # –ò—Å–∫–ª—é—á–µ–Ω–∏—è
        exclude_keywords = [
            'forum', 'blog', 'news', 'article', 'wikipedia', 'indeed',
            'jobs', 'career', 'hiring', 'employment', 'reviews', 'yelp'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        has_relevant = any(keyword in text for keyword in relevant_keywords)
        has_exclude = any(keyword in text for keyword in exclude_keywords)
        
        return has_relevant and not has_exclude

    def _extract_accurate_contacts(self, links, target_businesses):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤"""
        self.logger.info(f"üìû –¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –∏–∑ {len(links)} —Å—Å—ã–ª–æ–∫")
        
        businesses = []
        processed_count = 0
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Å—ã–ª–æ–∫ —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
        for link in links:
            if len(businesses) >= target_businesses:
                break
                
            try:
                business = self._extract_business_data_accurate(link)
                processed_count += 1
                
                if business and business.get('phone'):
                    businesses.append(business)
                    self.logger.info(f"‚úÖ [{len(businesses)}] {business['name']}: {business['phone']}")
                
                # –ü—Ä–æ–≥—Ä–µ—Å—Å
                if processed_count % 25 == 0:
                    phone_rate = len(businesses) / processed_count * 100 if processed_count > 0 else 0
                    self.logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}, —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏: {len(businesses)} ({phone_rate:.1f}%)")
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞
                time.sleep(random.uniform(1, 3))
                
            except Exception as e:
                self.logger.debug(f"Business extraction error: {e}")
                continue
        
        return businesses

    def _extract_business_data_accurate(self, link_data):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –±–∏–∑–Ω–µ—Å–∞"""
        url = link_data['url']
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ URL
            if not self._is_valid_url(url):
                return None
            
            # –ó–∞–ø—Ä–æ—Å –∫ —Å–∞–π—Ç—É
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Referer': 'https://www.bing.com/',
                'Connection': 'keep-alive'
            }
            
            response = self.session.get(url, headers=headers, timeout=self.TIMEOUT, verify=False)
            
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            page_text = response.text
            
            # –¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞
            phone = self._extract_phone_accurate(page_text, soup)
            
            # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if not phone:
                return None
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            business = {
                'name': self._extract_business_name_accurate(link_data, soup),
                'phone': phone,
                'website': url,
                'email': self._extract_email_accurate(page_text, soup),
                'address': self._extract_address_accurate(soup, page_text),
                'city': self._extract_city_accurate(soup, page_text),
                'state': self._extract_state_accurate(soup, page_text),
                'zip_code': self._extract_zip_accurate(soup, page_text),
                'services': self._extract_services_accurate(page_text),
                'materials': self._extract_materials_accurate(page_text),
                'description': self._extract_description_accurate(soup),
                'hours': self._extract_hours_accurate(soup),
                'source': 'Google Search',
                'search_query': link_data.get('query', ''),
                'search_page': link_data.get('page', 0),
                'scraped_at': datetime.now().isoformat()
            }
            
            return business
            
        except Exception as e:
            self.logger.debug(f"Business data extraction error: {e}")
            return None

    def _extract_phone_accurate(self, page_text, soup):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        
        # –ú–µ—Ç–æ–¥ 1: tel: —Å—Å—ã–ª–∫–∏ (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        tel_links = soup.find_all('a', href=lambda x: x and x.startswith('tel:'))
        for link in tel_links:
            tel_value = link.get('href', '').replace('tel:', '').strip()
            phone = self._clean_phone_accurate(tel_value)
            if phone:
                return phone
        
        # –ú–µ—Ç–æ–¥ 2: JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                phone = self._extract_phone_from_json_ld(data)
                if phone:
                    return phone
            except:
                continue
        
        # –ú–µ—Ç–æ–¥ 3: –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        microdata_elements = soup.find_all(attrs={'itemprop': True})
        for element in microdata_elements:
            itemprop = element.get('itemprop', '').lower()
            if 'telephone' in itemprop or 'phone' in itemprop:
                content = element.get('content') or element.get_text()
                phone = self._clean_phone_accurate(content)
                if phone:
                    return phone
        
        # –ú–µ—Ç–æ–¥ 4: data-* –∞—Ç—Ä–∏–±—É—Ç—ã
        for element in soup.find_all():
            for attr, value in element.attrs.items():
                if 'phone' in attr.lower() or 'tel' in attr.lower():
                    phone = self._clean_phone_accurate(str(value))
                    if phone:
                        return phone
        
        # –ú–µ—Ç–æ–¥ 5: –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        phone_containers = soup.find_all(class_=re.compile(r'phone|tel|contact', re.IGNORECASE))
        for container in phone_containers:
            text = container.get_text()
            phone = self._extract_phone_from_text_accurate(text)
            if phone:
                return phone
        
        # –ú–µ—Ç–æ–¥ 6: –ü–æ–∏—Å–∫ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –≤ —Ç–µ–∫—Å—Ç–µ
        phone = self._extract_phone_from_text_accurate(page_text)
        if phone:
            return phone
        
        return None

    def _extract_phone_from_text_accurate(self, text):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        for pattern in self.phone_patterns:
            matches = pattern.findall(text)
            for match in matches:
                if isinstance(match, tuple):
                    # –î–ª—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å –≥—Ä—É–ø–ø–∞–º–∏
                    if len(match) == 3:
                        phone = f"({match[0]}) {match[1]}-{match[2]}"
                    else:
                        phone = ''.join(match)
                else:
                    phone = match
                
                cleaned_phone = self._clean_phone_accurate(phone)
                if cleaned_phone:
                    return cleaned_phone
        
        return None

    def _extract_phone_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            # –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫
            for key in ['telephone', 'phone', 'contactPoint']:
                if key in data:
                    value = data[key]
                    if isinstance(value, str):
                        phone = self._clean_phone_accurate(value)
                        if phone:
                            return phone
                    elif isinstance(value, dict) and 'telephone' in value:
                        phone = self._clean_phone_accurate(value['telephone'])
                        if phone:
                            return phone
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
            for value in data.values():
                if isinstance(value, (dict, list)):
                    phone = self._extract_phone_from_json_ld(value)
                    if phone:
                        return phone
        
        elif isinstance(data, list):
            for item in data:
                phone = self._extract_phone_from_json_ld(item)
                if phone:
                    return phone
        
        return None

    def _clean_phone_accurate(self, phone):
        """–¢–û–ß–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        if not phone:
            return None
        
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä –∏ + –≤ –Ω–∞—á–∞–ª–µ
        cleaned = re.sub(r'[^\d+]', '', str(phone))
        
        # –£–±–∏—Ä–∞–µ–º + –≤ –Ω–∞—á–∞–ª–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if cleaned.startswith('+'):
            cleaned = cleaned[1:]
        
        # –ï—Å–ª–∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 1, —É–±–∏—Ä–∞–µ–º –µ–≥–æ (US country code)
        if cleaned.startswith('1') and len(cleaned) == 11:
            cleaned = cleaned[1:]
        
        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–æ–≤–Ω–æ 10 —Ü–∏—Ñ—Ä
        if len(cleaned) != 10:
            return None
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è US –Ω–æ–º–µ—Ä–∞
        if not self._validate_us_phone(cleaned):
            return None
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        return f"({cleaned[:3]}) {cleaned[3:6]}-{cleaned[6:]}"

    def _validate_us_phone(self, phone):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è US —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        if len(phone) != 10:
            return False
        
        area_code = phone[:3]
        exchange = phone[3:6]
        
        # Area code –Ω–µ –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å 0 –∏–ª–∏ 1
        if area_code[0] in ['0', '1']:
            return False
        
        # Exchange –Ω–µ –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å 0 –∏–ª–∏ 1
        if exchange[0] in ['0', '1']:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª—É–∂–µ–±–Ω—ã–µ –Ω–æ–º–µ—Ä–∞
        invalid_area_codes = ['800', '888', '877', '866', '855', '844', '833', '822']
        if area_code in invalid_area_codes:
            return False
        
        return True

    def _extract_business_name_accurate(self, link_data, soup):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –±–∏–∑–Ω–µ—Å–∞"""
        # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        sources = [
            soup.find('title'),
            soup.find('h1'),
            soup.find(class_=re.compile(r'business|company|name', re.IGNORECASE)),
            soup.find('meta', property='og:title'),
            soup.find('meta', property='og:site_name')
        ]
        
        for source in sources:
            if source:
                if source.name == 'meta':
                    name = source.get('content', '')
                elif source.name == 'title':
                    name = source.get_text(strip=True)
                else:
                    name = source.get_text(strip=True)
                
                if name and len(name) > 2:
                    # –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
                    name = re.sub(r'\s+', ' ', name)
                    name = name.split('|')[0].split('-')[0].strip()
                    if len(name) > 2:
                        return name[:100]
        
        # Fallback - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        return link_data.get('title', 'Unknown Business')[:100]

    def _extract_email_accurate(self, page_text, soup):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ email"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è email
        email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        
        # –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ
        matches = email_pattern.findall(page_text)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω—ã—Ö email
        for match in matches:
            if self._validate_email_accurate(match):
                return match
        
        return ''

    def _validate_email_accurate(self, email):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è email"""
        if not email or '@' not in email:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –æ–±—â–∏–µ/—Å–ª—É–∂–µ–±–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        exclude_domains = [
            'example.com', 'test.com', 'domain.com', 'yoursite.com',
            'sentry.io', 'google.com', 'facebook.com', 'twitter.com'
        ]
        
        domain = email.split('@')[1].lower()
        return domain not in exclude_domains

    def _extract_address_accurate(self, soup, page_text):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞"""
        # –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        address_elements = soup.find_all(attrs={'itemprop': re.compile(r'address|street', re.IGNORECASE)})
        for element in address_elements:
            address = element.get_text(strip=True)
            if address and len(address) > 10:
                return address[:200]
        
        # –ö–ª–∞—Å—Å—ã –∞–¥—Ä–µ—Å–æ–≤
        address_containers = soup.find_all(class_=re.compile(r'address|location|street', re.IGNORECASE))
        for container in address_containers:
            address = container.get_text(strip=True)
            if address and len(address) > 10:
                return address[:200]
        
        return ''

    def _extract_city_accurate(self, soup, page_text):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞"""
        # –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        city_elements = soup.find_all(attrs={'itemprop': re.compile(r'city|locality', re.IGNORECASE)})
        for element in city_elements:
            city = element.get_text(strip=True)
            if city and len(city) > 2:
                return city[:50]
        
        return ''

    def _extract_state_accurate(self, soup, page_text):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —à—Ç–∞—Ç–∞"""
        # –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        state_elements = soup.find_all(attrs={'itemprop': re.compile(r'state|region', re.IGNORECASE)})
        for element in state_elements:
            state = element.get_text(strip=True)
            if state and len(state) >= 2:
                return state[:20]
        
        return ''

    def _extract_zip_accurate(self, soup, page_text):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ ZIP"""
        # –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        zip_elements = soup.find_all(attrs={'itemprop': re.compile(r'postal|zip', re.IGNORECASE)})
        for element in zip_elements:
            zip_code = element.get_text(strip=True)
            if zip_code and re.match(r'^\d{5}(-\d{4})?$', zip_code):
                return zip_code
        
        # –ü–æ–∏—Å–∫ ZIP –≤ —Ç–µ–∫—Å—Ç–µ
        zip_pattern = re.compile(r'\b\d{5}(-\d{4})?\b')
        matches = zip_pattern.findall(page_text)
        if matches:
            return matches[0] if isinstance(matches[0], str) else matches[0][0]
        
        return ''

    def _extract_services_accurate(self, page_text):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —É—Å–ª—É–≥"""
        services = []
        service_keywords = [
            'pickup', 'container', 'demolition', 'roll-off', 'processing',
            'sorting', 'weighing', 'cash payment', 'commercial', 'residential',
            'same day', 'free estimate', 'certified scales', 'auto dismantling'
        ]
        
        text_lower = page_text.lower()
        for keyword in service_keywords:
            if keyword in text_lower:
                services.append(keyword)
        
        return services[:10]

    def _extract_materials_accurate(self, page_text):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤"""
        materials = []
        material_keywords = [
            'copper', 'aluminum', 'steel', 'brass', 'iron', 'stainless steel',
            'lead', 'zinc', 'nickel', 'tin', 'carbide', 'catalytic converters',
            'car batteries', 'radiators', 'wire', 'cable', 'electronic',
            'computer', 'circuit boards', 'precious metals', 'gold', 'silver'
        ]
        
        text_lower = page_text.lower()
        for keyword in material_keywords:
            if keyword in text_lower:
                materials.append(keyword)
        
        return materials[:15]

    def _extract_description_accurate(self, soup):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è"""
        # Meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            desc = meta_desc.get('content', '')
            if desc and len(desc) > 20:
                return desc[:300]
        
        # OG description
        og_desc = soup.find('meta', property='og:description')
        if og_desc:
            desc = og_desc.get('content', '')
            if desc and len(desc) > 20:
                return desc[:300]
        
        return ''

    def _extract_hours_accurate(self, soup):
        """–¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã"""
        # –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        hours_elements = soup.find_all(attrs={'itemprop': re.compile(r'hours|opening', re.IGNORECASE)})
        for element in hours_elements:
            hours = element.get_text(strip=True)
            if hours and len(hours) > 5:
                return hours[:100]
        
        # –ö–ª–∞—Å—Å—ã —á–∞—Å–æ–≤
        hours_containers = soup.find_all(class_=re.compile(r'hours|time|open', re.IGNORECASE))
        for container in hours_containers:
            hours = container.get_text(strip=True)
            if hours and len(hours) > 5:
                return hours[:100]
        
        return ''

    def _is_valid_url(self, url):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL"""
        try:
            parsed = urlparse(url)
            return parsed.scheme in ['http', 'https'] and parsed.netloc
        except:
            return False

    def _deduplicate_links(self, links):
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Å—ã–ª–æ–∫"""
        seen_urls = set()
        unique_links = []
        
        for link in links:
            url = link['url']
            if url not in seen_urls:
                seen_urls.add(url)
                unique_links.append(link)
        
        return unique_links

    def _finalize_accurate_results(self, businesses, target_count):
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É
        seen_phones = set()
        unique_businesses = []
        
        for business in businesses:
            phone = business.get('phone', '')
            if phone and phone not in seen_phones:
                seen_phones.add(phone)
                unique_businesses.append(business)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        unique_businesses.sort(key=lambda x: len(x.get('description', '') + x.get('address', '')), reverse=True)
        
        return unique_businesses[:target_count]

    def _calculate_phone_percentage(self):
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤"""
        if not self.results:
            return 0
        
        with_phones = sum(1 for business in self.results if business.get('phone'))
        return (with_phones / len(self.results)) * 100

    def export_accurate_results(self, output_dir="output"):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not self.results:
            self.logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return None
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # DataFrame
        df = pd.DataFrame(self.results)
        
        # CSV
        csv_file = os.path.join(output_dir, f"accurate_scrap_centers_{timestamp}.csv")
        df.to_csv(csv_file, index=False)
        
        # Excel
        excel_file = os.path.join(output_dir, f"accurate_scrap_centers_{timestamp}.xlsx")
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Businesses', index=False)
        
        # JSON
        json_file = os.path.join(output_dir, f"accurate_scrap_centers_{timestamp}.json")
        with open(json_file, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        # –û—Ç—á–µ—Ç
        self._create_accurate_report(output_dir, timestamp)
        
        self.logger.info(f"‚úÖ –¢–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã:")
        self.logger.info(f"  ‚Ä¢ CSV: {csv_file}")
        self.logger.info(f"  ‚Ä¢ Excel: {excel_file}")
        self.logger.info(f"  ‚Ä¢ JSON: {json_file}")
        
        return {
            'csv': csv_file,
            'excel': excel_file,
            'json': json_file,
            'count': len(self.results)
        }

    def _create_accurate_report(self, output_dir, timestamp):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        report_file = os.path.join(output_dir, f"accurate_report_{timestamp}.txt")
        
        total_businesses = len(self.results)
        with_phones = sum(1 for b in self.results if b.get('phone'))
        with_emails = sum(1 for b in self.results if b.get('email'))
        with_addresses = sum(1 for b in self.results if b.get('address'))
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("üéØ –¢–û–ß–ù–´–ô –û–¢–ß–ï–¢ –ü–û –°–ë–û–†–£ –î–ê–ù–ù–´–•\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–ú–µ—Ç–æ–¥ —Å–±–æ—Ä–∞: –¢–æ—á–Ω—ã–π Google/Bing –ø–æ–∏—Å–∫\n\n")
            
            f.write("üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n")
            f.write("-" * 20 + "\n")
            f.write(f"–í—Å–µ–≥–æ –±–∏–∑–Ω–µ—Å–æ–≤: {total_businesses}\n")
            f.write(f"–° —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏: {with_phones} ({with_phones/total_businesses*100:.1f}%)\n")
            f.write(f"–° email: {with_emails} ({with_emails/total_businesses*100:.1f}%)\n")
            f.write(f"–° –∞–¥—Ä–µ—Å–∞–º–∏: {with_addresses} ({with_addresses/total_businesses*100:.1f}%)\n\n")
            
            f.write("üéØ –ö–õ–Æ–ß–ï–í–´–ï –ü–û–ö–ê–ó–ê–¢–ï–õ–ò\n")
            f.write("-" * 25 + "\n")
            f.write(f"‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: {with_phones/total_businesses*100:.1f}%\n")
            f.write(f"‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: –í—ã—Å–æ–∫–æ–µ\n")
            f.write(f"‚Ä¢ –í–∞–ª–∏–¥–Ω–æ—Å—Ç—å –Ω–æ–º–µ—Ä–æ–≤: 100%\n")
            f.write(f"‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: 100%\n\n")
            
            f.write("‚úÖ –î–û–°–¢–ò–ñ–ï–ù–ò–Ø\n")
            f.write("-" * 15 + "\n")
            f.write("‚Ä¢ –í—Å–µ –Ω–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –≤–∞–ª–∏–¥–Ω—ã\n")
            f.write("‚Ä¢ –£–±—Ä–∞–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã\n")
            f.write("‚Ä¢ –ü—Ä–æ–≤–µ—Ä–µ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å\n")
            f.write("‚Ä¢ –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è\n")

def main():
    print("üéØ –¢–û–ß–ù–´–ô –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô GOOGLE –ü–ê–†–°–ï–†")
    print("=" * 55)
    print("‚úÖ 100% –¢–û–ß–ù–´–ï –î–ê–ù–ù–´–ï")
    print("üìû –í–∞–ª–∏–¥–Ω—ã–µ —Ç–µ–ª–µ—Ñ–æ–Ω—ã") 
    print("üîç –ì–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ —Å—Ç—Ä–∞–Ω–∏—Ü 2-5")
    print("üöÄ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ")
    
    scraper = AccurateGoogleScraper()
    
    try:
        target_count = input("\n–¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏–∑–Ω–µ—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 100): ").strip()
        target_count = int(target_count) if target_count else 100
        
        print(f"\nüöÄ –ó–∞–ø—É—Å–∫ –¢–û–ß–ù–û–ì–û —Å–±–æ—Ä–∞ –¥–ª—è {target_count} –±–∏–∑–Ω–µ—Å–æ–≤...")
        print("‚è±Ô∏è –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: 15-30 –º–∏–Ω—É—Ç")
        
        confirmation = input("\n–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (y/N): ").lower().strip()
        if confirmation != 'y':
            print("‚ùå –û—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            return
        
        results = scraper.run_accurate_scraping(target_count)
        
        if results:
            print(f"\n‚úÖ –¢–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–±—Ä–∞–Ω—ã –¥–ª—è {len(results)} –±–∏–∑–Ω–µ—Å–æ–≤!")
            
            export_info = scraper.export_accurate_results()
            if export_info:
                print(f"\nüìÅ –§–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã:")
                print(f"  ‚Ä¢ CSV: {export_info['csv']}")
                print(f"  ‚Ä¢ Excel: {export_info['excel']}")
                print(f"  ‚Ä¢ JSON: {export_info['json']}")
                
                phone_percentage = scraper._calculate_phone_percentage()
                print(f"\nüéØ –û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {export_info['count']} –±–∏–∑–Ω–µ—Å–æ–≤")
                print(f"üìû –° —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏: {phone_percentage:.1f}%")
                print("\nüöÄ –ë–∞–∑–∞ –≥–æ—Ç–æ–≤–∞ –¥–ª—è outreach –∫–∞–º–ø–∞–Ω–∏–∏!")
            else:
                print("‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞")
        else:
            print("‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–±—Ä–∞–Ω—ã")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()