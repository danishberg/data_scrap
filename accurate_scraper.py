#!/usr/bin/env python3
"""
–¢–û–ß–ù–´–ô –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ü–ê–†–°–ï–† –ú–ï–¢–ê–õ–õ–û–õ–û–ú–ê
100% –¢–û–ß–ù–´–ï –î–ê–ù–ù–´–ï | –ì–õ–û–ë–ê–õ–¨–ù–´–ô –û–•–í–ê–¢ | –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ü–û–õ–ù–û–¢–ê
"""

import os
import sys
import json
import time
import random
import logging
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
from urllib.parse import quote_plus, urlparse
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import urllib3
urllib3.disable_warnings()

class USMetalScraper:
    def __init__(self):
        self.session = requests.Session()
        self.results = []
        self.processed_urls = set()
        self.logger = self._setup_logging()
        
        # –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô –°–ö–û–†–û–°–¢–ò –ò –¢–û–ß–ù–û–°–¢–ò
        self.MIN_PHONE_PERCENTAGE = 85  # 85% —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏
        self.TIMEOUT = 6                # Fast timeout for speed
        self.MAX_WORKERS = 12           # Maximum parallel workers for speed
        
        # US PHONE PATTERNS - Optimized for US businesses
        self.phone_patterns = [
            # Standard US format: (555) 123-4567
            re.compile(r'\b\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})\b'),
            # US with country code: 1-555-123-4567
            re.compile(r'\b1[-.\s]?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})\b'),
            # tel: links format
            re.compile(r'tel:[\s]*\+?1?[-.\s]?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})', re.IGNORECASE),
            # Various US formatting variations
            re.compile(r'\b([0-9]{3})[-.\s]+([0-9]{3})[-.\s]+([0-9]{4})\b'),
            re.compile(r'\b([0-9]{3})\.([0-9]{3})\.([0-9]{4})\b'),
            re.compile(r'\b([0-9]{3})\s([0-9]{3})\s([0-9]{4})\b'),
        ]
        
        # –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ü–û–ò–°–ö–û–í–´–ï –ó–ê–ü–†–û–°–´ (–ì–õ–û–ë–ê–õ–¨–ù–´–ï)
        self.search_queries = [
            'scrap metal dealers',
            'metal recycling center',
            'scrap yard',
            'junk yard auto parts',
            'copper scrap buyers',
            'aluminum recycling',
            'auto salvage yard',
            'scrap metal pickup',
            'metal scrap dealers',
            'recycling centre',
            'scrap metal merchants',
            'waste metal collection',
            'metal recovery services',
            'industrial metal recycling',
            'non-ferrous metal dealers'
        ]
        
        # US TARGET LOCATIONS - Strategic focus on scrap metal markets
        self.target_locations = [
            # Major US metropolitan areas
            'New York NY', 'Los Angeles CA', 'Chicago IL', 'Houston TX', 'Phoenix AZ',
            'Philadelphia PA', 'San Antonio TX', 'San Diego CA', 'Dallas TX', 'San Jose CA',
            'Austin TX', 'Jacksonville FL', 'Fort Worth TX', 'Columbus OH', 'Charlotte NC',
            'San Francisco CA', 'Indianapolis IN', 'Seattle WA', 'Denver CO', 'Washington DC',
            'Boston MA', 'El Paso TX', 'Nashville TN', 'Detroit MI', 'Oklahoma City OK',
            'Portland OR', 'Las Vegas NV', 'Memphis TN', 'Louisville KY', 'Baltimore MD',
            'Milwaukee WI', 'Albuquerque NM', 'Tucson AZ', 'Fresno CA', 'Sacramento CA',
            'Mesa AZ', 'Kansas City MO', 'Atlanta GA', 'Long Beach CA', 'Colorado Springs CO',
            'Raleigh NC', 'Miami FL', 'Virginia Beach VA', 'Omaha NE', 'Oakland CA',
            'Minneapolis MN', 'Tulsa OK', 'Arlington TX', 'Tampa FL', 'New Orleans LA',
            
            # Mid-tier cities with high scrap metal potential
            'Cleveland OH', 'Pittsburgh PA', 'Cincinnati OH', 'Toledo OH', 'Akron OH',
            'Dayton OH', 'Youngstown OH', 'Canton OH', 'Buffalo NY', 'Rochester NY',
            'Syracuse NY', 'Albany NY', 'Utica NY', 'Binghamton NY', 'Elmira NY',
            'Scranton PA', 'Allentown PA', 'Reading PA', 'Erie PA', 'Bethlehem PA',
            'Harrisburg PA', 'Lancaster PA', 'York PA', 'Wilkes-Barre PA',
            'Flint MI', 'Lansing MI', 'Kalamazoo MI', 'Grand Rapids MI', 'Saginaw MI',
            'Bay City MI', 'Battle Creek MI', 'Jackson MI', 'Muskegon MI',
            'Rockford IL', 'Peoria IL', 'Decatur IL', 'Springfield IL', 'Champaign IL',
            'Bloomington IL', 'Quincy IL', 'Danville IL', 'Kankakee IL',
            'Fort Wayne IN', 'Evansville IN', 'South Bend IN', 'Gary IN', 'Muncie IN',
            'Terre Haute IN', 'Anderson IN', 'Kokomo IN', 'Lafayette IN',
            'Green Bay WI', 'Appleton WI', 'Oshkosh WI', 'Racine WI', 'Kenosha WI',
            'Eau Claire WI', 'Wausau WI', 'La Crosse WI', 'Janesville WI',
            'Cedar Rapids IA', 'Davenport IA', 'Sioux City IA', 'Waterloo IA',
            'Dubuque IA', 'Ames IA', 'Council Bluffs IA', 'Mason City IA',
            'Springfield MO', 'Columbia MO', 'Joplin MO', 'St. Joseph MO',
            'Cape Girardeau MO', 'Jefferson City MO', 'Sedalia MO', 'St. Louis MO',
            'Little Rock AR', 'Fayetteville AR', 'Jonesboro AR', 'Pine Bluff AR',
            'Fort Smith AR', 'Texarkana AR', 'Hot Springs AR', 'Conway AR',
            'Birmingham AL', 'Mobile AL', 'Montgomery AL', 'Huntsville AL',
            'Tuscaloosa AL', 'Dothan AL', 'Decatur AL', 'Florence AL',
            'Jackson MS', 'Gulfport MS', 'Hattiesburg MS', 'Meridian MS',
            'Biloxi MS', 'Tupelo MS', 'Greenville MS', 'Vicksburg MS',
            'Shreveport LA', 'Baton Rouge LA', 'Lafayette LA', 'Lake Charles LA',
            'Monroe LA', 'Alexandria LA', 'Houma LA', 'Bossier City LA',
            'Knoxville TN', 'Chattanooga TN', 'Clarksville TN', 'Murfreesboro TN',
            'Jackson TN', 'Johnson City TN', 'Kingsport TN', 'Franklin TN'
        ]
        
        # –ú–ê–¢–ï–†–ò–ê–õ–´ –î–õ–Ø –ü–û–ò–°–ö–ê
        self.materials_keywords = [
            'copper', 'aluminum', 'aluminium', 'steel', 'iron', 'brass', 'bronze',
            'stainless steel', 'lead', 'zinc', 'nickel', 'tin', 'titanium',
            'carbide', 'tungsten', 'precious metals', 'gold', 'silver', 'platinum',
            'catalytic converters', 'car batteries', 'radiators', 'electric motors',
            'transformers', 'wire', 'cable', 'circuit boards', 'electronic scrap',
            'computer scrap', 'mobile phones', 'cast iron', 'wrought iron',
            'structural steel', 'rebar', 'pipes', 'tubes', 'sheet metal',
            'coils', 'turnings', 'shredded metal', 'HMS', 'heavy melting scrap'
        ]
        
        # USER AGENTS
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/121.0.0.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15'
        ]
        
        self._init_session()

    def _init_session(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏"""
        self.session.headers.update({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'DNT': '1'
        })

    def _setup_logging(self):
        logger = logging.getLogger('USMetalScraper')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
        
        return logger

    def run_comprehensive_scraping(self, target_businesses=500):
        """–ö–û–ú–ü–õ–ï–ö–°–ù–´–ô —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ –°–®–ê —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–Ω–æ—Ç–æ–π"""
        self.logger.info(f"üá∫üá∏ –ó–ê–ü–£–°–ö US SCRAP METAL –°–ë–û–†–ê")
        self.logger.info(f"üìû –¶–ï–õ–¨: {target_businesses} –±–∏–∑–Ω–µ—Å–æ–≤ —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏")
        self.logger.info(f"üîç –û–•–í–ê–¢: {len(self.target_locations)} US –ª–æ–∫–∞—Ü–∏–π")
        self.logger.info(f"üìã –ú–ê–¢–ï–†–ò–ê–õ–´: {len(self.materials_keywords)} —Ç–∏–ø–æ–≤")
        
        start_time = time.time()
        
        # –≠—Ç–∞–ø 1: –ú–∞—Å—Å–æ–≤—ã–π —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫
        self.logger.info("üîó –≠—Ç–∞–ø 1: –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∏–∑ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º")
        all_links = self._collect_comprehensive_links()
        self.logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(all_links)}")
        
        # –≠—Ç–∞–ø 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.logger.info("üìä –≠—Ç–∞–ø 2: –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
        businesses = self._extract_comprehensive_data(all_links, target_businesses)
        
        # –≠—Ç–∞–ø 3: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ
        self.logger.info("üî¨ –≠—Ç–∞–ø 3: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
        self.results = self._finalize_comprehensive_results(businesses, target_businesses)
        
        elapsed = time.time() - start_time
        phone_percentage = self._calculate_contact_percentage()
        
        self.logger.info(f"‚úÖ US –°–ë–û–† –ó–ê–í–ï–†–®–ï–ù –∑–∞ {elapsed/60:.1f} –º–∏–Ω—É—Ç")
        self.logger.info(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢: {len(self.results)} –±–∏–∑–Ω–µ—Å–æ–≤")
        self.logger.info(f"üìû –° –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏: {phone_percentage:.1f}%")
        
        return self.results

    def _collect_comprehensive_links(self):
        """–ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫"""
        self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫...")
        
        all_links = []
        
        # –ú–µ–Ω—å—à–µ –ª–æ–∫–∞—Ü–∏–π, –Ω–æ –±—ã—Å—Ç—Ä–µ–µ
        target_locations = self.target_locations[:15]  # Reduced for speed
        target_queries = self.search_queries[:8]       # Reduced for speed
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        search_tasks = []
        for location in target_locations:
            for query in target_queries:
                for page in range(2, 4):  # Only pages 2-3 for speed
                    search_tasks.append((f"{query} {location}", page))
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫
        with ThreadPoolExecutor(max_workers=12) as executor:
            future_to_task = {
                executor.submit(self._fast_bing_search, query, page): (query, page)
                for query, page in search_tasks[:100]  # Limit for speed
            }
            
            for future in as_completed(future_to_task, timeout=600):  # 10 minutes max
                try:
                    links = future.result(timeout=10)
                    all_links.extend(links)
                    
                    # Progress update
                    if len(all_links) % 50 == 0:
                        self.logger.info(f"üìà –°–æ–±—Ä–∞–Ω–æ —Å—Å—ã–ª–æ–∫: {len(all_links)}")
                    
                    # Stop when we have enough
                    if len(all_links) >= 500:
                        self.logger.info(f"üéØ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Å—ã–ª–æ–∫: {len(all_links)}")
                        break
                        
                except Exception as e:
                    self.logger.debug(f"Search task failed: {e}")
                    continue
        
        unique_links = self._deduplicate_links(all_links)
        self.logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(unique_links)}")
        return unique_links
    
    def _fast_bing_search(self, query, page):
        """–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –≤ Bing –±–µ–∑ –∑–∞–¥–µ—Ä–∂–µ–∫"""
        links = []
        
        try:
            start = (page - 1) * 10
            url = f"https://www.bing.com/search?q={quote_plus(query)}&first={start}&count=10"
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9'
            }
            
            response = self.session.get(url, headers=headers, timeout=8, verify=False)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                for result in soup.find_all('li', class_='b_algo'):
                    try:
                        title_elem = result.find('h2')
                        if not title_elem:
                            continue
                            
                        link_elem = title_elem.find('a')
                        if not link_elem or not link_elem.get('href'):
                            continue
                        
                        url = link_elem.get('href')
                        title = title_elem.get_text(strip=True)
                        
                        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                        if self._quick_relevance_check(title):
                            links.append({
                                'url': url,
                                'title': title,
                                'page': page,
                                'query': query,
                                'source': 'Bing'
                            })
                            
                    except Exception as e:
                        continue
                        
        except Exception as e:
            self.logger.debug(f"Fast search failed: {e}")
        
        return links
    
    def _quick_relevance_check(self, title):
        """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É"""
        title_lower = title.lower()
        
        # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
        relevant = ['scrap', 'metal', 'recycling', 'salvage', 'junk', 'yard', 'steel', 'copper', 'aluminum']
        
        # –ò—Å–∫–ª—é—á–µ–Ω–∏—è
        exclude = ['software', 'app', 'game', 'news', 'blog', 'wikipedia', 'facebook', 'jobs']
        
        has_relevant = any(word in title_lower for word in relevant)
        has_exclude = any(word in title_lower for word in exclude)
        
        return has_relevant and not has_exclude

    def _extract_comprehensive_data(self, links, target_businesses):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–æ–Ω—Ç–∞–∫—Ç—ã"""
        self.logger.info(f"‚ö° –ë–´–°–¢–†–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {len(links)} —Å—Å—ã–ª–æ–∫")
        
        businesses = []
        processed_count = 0
        
        # Process more links faster
        links_to_process = min(len(links), target_businesses * 8)
        
        # Faster parallel processing
        with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
            futures = {
                executor.submit(self._fast_extract_business, link): link 
                for link in links[:links_to_process]
            }
            
            for future in as_completed(futures, timeout=900):  # 15 minutes max
                try:
                    business = future.result(timeout=8)  # Faster timeout
                    processed_count += 1
                    
                    if business:
                        businesses.append(business)
                        
                        # Quick logging
                        phone = business.get('phone', 'N/A')
                        email = business.get('email', 'N/A')
                        self.logger.info(f"‚úÖ [{len(businesses)}] {business['name'][:25]}... | üìû {phone}")
                    
                    # Progress every 100 for speed
                    if processed_count % 100 == 0:
                        contact_rate = len(businesses) / processed_count * 100 if processed_count > 0 else 0
                        self.logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}, –Ω–∞–π–¥–µ–Ω–æ: {len(businesses)} ({contact_rate:.1f}%)")
                    
                    # Stop when target reached
                    if len(businesses) >= target_businesses:
                        self.logger.info(f"üéØ –¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞: {len(businesses)} –±–∏–∑–Ω–µ—Å–æ–≤")
                        break
                        
                except Exception as e:
                    processed_count += 1
                    continue
        
        final_rate = len(businesses) / processed_count * 100 if processed_count > 0 else 0
        self.logger.info(f"üìä –ò–¢–û–ì–û: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count}, –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(businesses)} ({final_rate:.1f}%)")
        
        return businesses

    def _fast_extract_business(self, link_data):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        url = link_data['url']
        
        try:
            if not self._is_valid_url(url):
                return None
            
            # Fast request with short timeout
            headers = {'User-Agent': random.choice(self.user_agents)}
            response = self.session.get(url, headers=headers, timeout=6, verify=False)
            
            if response.status_code != 200:
                return None
            
            page_text = response.text
            
            # Fast regex-based extraction
            phone = self._fast_extract_phone(page_text)
            email = self._fast_extract_email(page_text)
            
            # Skip if no contact info
            if not (phone or email):
                return None
            
            # Minimal BeautifulSoup parsing only for essential fields
            soup = BeautifulSoup(response.text, 'html.parser')
            
            business = {
                'name': self._fast_extract_name(link_data, soup),
                'phone': phone,
                'email': email,
                'website': url,
                'address': self._fast_extract_address(page_text),
                'city': self._fast_extract_city(page_text),
                'state': self._fast_extract_state(page_text),
                'country': 'United States',
                'materials_accepted': self._fast_extract_materials(page_text),
                'source': link_data.get('source', ''),
                'scraped_at': datetime.now().isoformat()
            }
            
            return business
                
        except Exception as e:
            return None
    
    def _fast_extract_phone(self, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ —á–µ—Ä–µ–∑ regex"""
        # US phone patterns - faster regex
        patterns = [
            r'\b\(?([2-9][0-9]{2})\)?[-.\s]?([2-9][0-9]{2})[-.\s]?([0-9]{4})\b',
            r'\b1[-.\s]?\(?([2-9][0-9]{2})\)?[-.\s]?([2-9][0-9]{2})[-.\s]?([0-9]{4})\b',
            r'tel:[\s]*\+?1?[-.\s]?\(?([2-9][0-9]{2})\)?[-.\s]?([2-9][0-9]{2})[-.\s]?([0-9]{4})'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match) == 3:
                    area, exchange, number = match
                    # Quick validation
                    if area[0] not in ['0', '1'] and exchange[0] not in ['0', '1']:
                        return f"({area}) {exchange}-{number}"
        
        return None
    
    def _fast_extract_email(self, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ email —á–µ—Ä–µ–∑ regex"""
        pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        matches = re.findall(pattern, text)
        
        for match in matches:
            # Skip common non-business domains
            if not any(skip in match.lower() for skip in ['example.com', 'google.com', 'facebook.com']):
                return match
        
        return None
    
    def _fast_extract_name(self, link_data, soup):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        # Try title first
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text().strip()
            if title and len(title) > 5:
                # Clean title
                name = title.split('|')[0].split('-')[0].strip()
                return name[:100]
        
        # Fallback to search result title
        return link_data.get('title', 'Unknown Business')[:100]
    
    def _fast_extract_address(self, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ —á–µ—Ä–µ–∑ regex"""
        # US address patterns
        patterns = [
            r'\b\d+\s+[A-Za-z0-9\s]+(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Boulevard|Blvd|Lane|Ln|Way|Circle|Cir|Court|Ct)\b[^,\n]*',
            r'\b\d+\s+[A-Za-z0-9\s]+(?:St|Ave|Rd|Dr|Blvd|Ln|Way|Cir|Ct)\.?\s*[,\n]'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                return matches[0][:150]
        
        return None
    
    def _fast_extract_city(self, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ —á–µ—Ä–µ–∑ regex"""
        # Look for city patterns
        pattern = r'\b([A-Za-z\s]+),\s*([A-Z]{2})\s*\d{5}'
        matches = re.findall(pattern, text)
        
        for match in matches:
            city = match[0].strip()
            if len(city) > 2 and city[0].isupper():
                return city[:50]
        
        return None
    
    def _fast_extract_state(self, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —à—Ç–∞—Ç–∞ —á–µ—Ä–µ–∑ regex"""
        # US state abbreviations
        pattern = r'\b([A-Z]{2})\s*\d{5}(?:-\d{4})?\b'
        matches = re.findall(pattern, text)
        
        us_states = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']
        
        for match in matches:
            if match in us_states:
                return match
        
        return None
    
    def _fast_extract_materials(self, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ —á–µ—Ä–µ–∑ regex"""
        text_lower = text.lower()
        materials = []
        
        # Quick material check
        material_keywords = ['copper', 'aluminum', 'steel', 'iron', 'brass', 'scrap metal']
        
        for material in material_keywords:
            if material in text_lower:
                materials.append(material)
        
        return materials if materials else None

    def _extract_phone_comprehensive(self, page_text, soup):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ (–≥–ª–æ–±–∞–ª—å–Ω–æ–µ)"""
        
        # –ú–µ—Ç–æ–¥ 1: tel: —Å—Å—ã–ª–∫–∏ (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        tel_links = soup.find_all('a', href=lambda x: x and x.startswith('tel:'))
        for link in tel_links:
            tel_value = link.get('href', '').replace('tel:', '').strip()
            phone = self._clean_phone_global(tel_value)
            if phone:
                return phone
        
        # –ú–µ—Ç–æ–¥ 2: JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                phone = self._extract_phone_from_json_ld(data)
                if phone:
                    return self._clean_phone_global(phone)
            except:
                continue
        
        # –ú–µ—Ç–æ–¥ 3: –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        microdata_elements = soup.find_all(attrs={'itemprop': True})
        for element in microdata_elements:
            itemprop = element.get('itemprop', '').lower()
            if 'telephone' in itemprop or 'phone' in itemprop:
                content = element.get('content') or element.get_text()
                phone = self._clean_phone_global(content)
                if phone:
                    return phone
        
        # –ú–µ—Ç–æ–¥ 4: data-* –∞—Ç—Ä–∏–±—É—Ç—ã
        for element in soup.find_all():
            for attr, value in element.attrs.items():
                if 'phone' in attr.lower() or 'tel' in attr.lower():
                    phone = self._clean_phone_global(str(value))
                    if phone:
                        return phone
        
        # –ú–µ—Ç–æ–¥ 5: –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        phone_containers = soup.find_all(class_=re.compile(r'phone|tel|contact|call', re.IGNORECASE))
        for container in phone_containers:
            text = container.get_text()
            phone = self._extract_phone_from_text_global(text)
            if phone:
                return phone
        
        # –ú–µ—Ç–æ–¥ 6: –ü–æ–∏—Å–∫ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –≤ —Ç–µ–∫—Å—Ç–µ
        phone = self._extract_phone_from_text_us(page_text)
        if phone:
            return phone
        
        return None

    def _extract_phone_from_text_us(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (US —Ñ–æ–∫—É—Å)"""
        for pattern in self.phone_patterns:
            matches = pattern.findall(text)
            for match in matches:
                if isinstance(match, tuple):
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º tuple results
                    phone = ' '.join(str(m) for m in match if m)
                else:
                    phone = str(match)
                
                cleaned_phone = self._clean_phone_us(phone)
                if cleaned_phone:
                    return cleaned_phone
        
        return None

    def _clean_phone_us(self, phone):
        """–û—á–∏—Å—Ç–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è US —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤"""
        if not phone:
            return None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã
        digits_only = re.sub(r'\D', '', str(phone))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –¥–ª–∏–Ω—É US –Ω–æ–º–µ—Ä–∞
        if len(digits_only) == 10:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π US —Ñ–æ—Ä–º–∞—Ç
            area_code = digits_only[:3]
            exchange = digits_only[3:6]
            number = digits_only[6:]
        elif len(digits_only) == 11 and digits_only.startswith('1'):
            # US –Ω–æ–º–µ—Ä —Å –∫–æ–¥–æ–º —Å—Ç—Ä–∞–Ω—ã
            area_code = digits_only[1:4]
            exchange = digits_only[4:7]
            number = digits_only[7:]
        else:
            # –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è US –Ω–æ–º–µ—Ä–∞
            return None
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è US –Ω–æ–º–µ—Ä–∞
        if not self._validate_us_phone(area_code, exchange, number):
            return None
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º US —Ñ–æ—Ä–º–∞—Ç–µ
        return f"({area_code}) {exchange}-{number}"
    
    def _clean_phone_global(self, phone):
        """–ì–ª–æ–±–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤ - –∏—Å–ø–æ–ª—å–∑—É–µ–º US –≤–∞–ª–∏–¥–∞—Ü–∏—é"""
        # For US-focused scraper, use US validation
        return self._clean_phone_us(phone)
    
    def _extract_phone_from_text_global(self, text):
        """–ì–ª–æ–±–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º US –º–µ—Ç–æ–¥—ã"""
        # For US-focused scraper, use US extraction
        return self._extract_phone_from_text_us(text)

    def _validate_us_phone(self, area_code, exchange, number):
        """–°—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è US —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞"""
        # Area code –Ω–µ –º–æ–∂–µ—Ç –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å 0 –∏–ª–∏ 1
        if area_code[0] in ['0', '1']:
            return False
        
        # Exchange –Ω–µ –º–æ–∂–µ—Ç –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å 0 –∏–ª–∏ 1
        if exchange[0] in ['0', '1']:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ area codes
        invalid_areas = ['000', '111', '222', '333', '444', '555', '666', '777', '888', '999']
        if area_code in invalid_areas:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ toll-free –Ω–æ–º–µ—Ä–∞ (–Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –º–µ—Å—Ç–Ω–æ–≥–æ –±–∏–∑–Ω–µ—Å–∞)
        toll_free_areas = ['800', '833', '844', '855', '866', '877', '888']
        if area_code in toll_free_areas:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ service numbers
        if exchange == '555' and number.startswith('01'):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ü–∏—Ñ—Ä
        full_number = area_code + exchange + number
        if len(set(full_number)) < 4:  # –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–∏—Ñ—Ä
            return False
        
        return True

    def _extract_email_comprehensive(self, page_text, soup):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ email"""
        # –ú–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è email (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É)
        
        # 1. mailto: —Å—Å—ã–ª–∫–∏
        mailto_links = soup.find_all('a', href=lambda x: x and x.startswith('mailto:'))
        for link in mailto_links:
            email = link.get('href', '').replace('mailto:', '').strip()
            if self._validate_email_global(email):
                return email
        
        # 2. JSON-LD
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                email = self._extract_email_from_json_ld(data)
                if email:
                    return email
            except:
                continue
        
        # 3. –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ
        email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        matches = email_pattern.findall(page_text)
        
        for match in matches:
            if self._validate_email_global(match):
                return match
        
        return None

    def _extract_whatsapp(self, page_text, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ WhatsApp"""
        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ WhatsApp
        whatsapp_patterns = [
            re.compile(r'whatsapp\.com/send\?phone=([0-9]+)', re.IGNORECASE),
            re.compile(r'wa\.me/([0-9]+)', re.IGNORECASE),
            re.compile(r'api\.whatsapp\.com/send\?phone=([0-9]+)', re.IGNORECASE)
        ]
        
        for pattern in whatsapp_patterns:
            matches = pattern.findall(page_text)
            if matches:
                return matches[0]
        
        return None

    def _extract_social_media(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–æ—Ü—Å–µ—Ç–∏"""
        social_media = {}
        
        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–æ—Ü—Å–µ—Ç–∏
        social_patterns = {
            'facebook': r'facebook\.com/[^/\s]+',
            'twitter': r'twitter\.com/[^/\s]+',
            'instagram': r'instagram\.com/[^/\s]+',
            'linkedin': r'linkedin\.com/[^/\s]+',
            'youtube': r'youtube\.com/[^/\s]+',
            'tiktok': r'tiktok\.com/[^/\s]+',
        }
        
        page_text = str(soup)
        
        for platform, pattern in social_patterns.items():
            matches = re.findall(pattern, page_text, re.IGNORECASE)
            if matches:
                social_media[platform] = matches[0]
        
        return social_media if social_media else None

    def _extract_business_name_comprehensive(self, link_data, soup):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –±–∏–∑–Ω–µ—Å–∞"""
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è
        sources = [
            # JSON-LD
            self._extract_name_from_json_ld_comprehensive(soup),
            # Meta tags
            self._extract_name_from_meta_tags(soup),
            # Title tag
            self._extract_name_from_title(soup),
            # H1 tag
            self._extract_name_from_h1(soup),
            # Fallback - –∏–∑ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            link_data.get('title', 'Unknown Business')
        ]
        
        for source in sources:
            if source and len(source.strip()) > 2:
                # –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
                name = re.sub(r'\s+', ' ', source.strip())
                name = name.split('|')[0].split('-')[0].strip()
                return name[:150]
        
        return 'Unknown Business'

    def _extract_materials_comprehensive(self, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–Ω–∏–º–∞–µ–º—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤"""
        materials_found = []
        text_lower = page_text.lower()
        
        for material in self.materials_keywords:
            if material in text_lower:
                materials_found.append(material)
        
        return materials_found if materials_found else None

    def _extract_pricing_info_comprehensive(self, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ü–µ–Ω–∞—Ö"""
        pricing_patterns = [
            re.compile(r'\$\d+\.?\d*\s*per\s*(?:pound|lb|kilogram|kg|ton|tonne)', re.IGNORECASE),
            re.compile(r'\$\d+\.?\d*\s*/\s*(?:pound|lb|kilogram|kg|ton|tonne)', re.IGNORECASE),
            re.compile(r'(?:copper|aluminum|steel|brass|iron).*?\$\d+\.?\d*', re.IGNORECASE),
            re.compile(r'current\s*price.*?\$\d+\.?\d*', re.IGNORECASE),
            re.compile(r'scrap\s*price.*?\$\d+\.?\d*', re.IGNORECASE)
        ]
        
        pricing_info = []
        for pattern in pricing_patterns:
            matches = pattern.findall(page_text)
            pricing_info.extend(matches[:5])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        
        return pricing_info if pricing_info else None

    def _extract_working_hours_comprehensive(self, soup):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–±–æ—á–∏—Ö —á–∞—Å–æ–≤"""
        # –ü–æ–∏—Å–∫ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
        hours_selectors = [
            '[itemprop*="openingHours"]',
            '[itemprop*="hours"]',
            '.hours',
            '.opening-hours',
            '.business-hours',
            '.working-hours',
            '.schedule'
        ]
        
        for selector in hours_selectors:
            elements = soup.select(selector)
            for element in elements:
                hours_text = element.get_text(strip=True)
                if len(hours_text) > 10 and any(day in hours_text.lower() for day in ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']):
                    return hours_text[:200]
        
        return None

    def _extract_services_comprehensive(self, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —É—Å–ª—É–≥"""
        services_keywords = [
            'pickup', 'collection', 'container rental', 'roll-off', 'demolition',
            'dismantling', 'processing', 'sorting', 'weighing', 'cash payment',
            'check payment', 'commercial', 'residential', 'industrial',
            'same day', 'free estimate', 'certified scales', 'licensed',
            'insured', 'bonded', 'environmental', 'hazardous waste'
        ]
        
        services_found = []
        text_lower = page_text.lower()
        
        for service in services_keywords:
            if service in text_lower:
                services_found.append(service)
        
        return services_found if services_found else None

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    def _extract_country_comprehensive(self, soup, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã"""
        # –ü–æ–∏—Å–∫ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        country_indicators = {
            'United States': ['usa', 'us', 'united states', 'america'],
            'Canada': ['canada', 'ca'],
            'United Kingdom': ['uk', 'united kingdom', 'england', 'scotland', 'wales'],
            'Australia': ['australia', 'au'],
            'New Zealand': ['new zealand', 'nz'],
            'Ireland': ['ireland', 'ie']
        }
        
        text_lower = page_text.lower()
        
        for country, indicators in country_indicators.items():
            if any(indicator in text_lower for indicator in indicators):
                return country
        
        return 'Unknown'

    def _extract_certifications(self, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤"""
        cert_patterns = [
            re.compile(r'ISO\s*\d{4,5}', re.IGNORECASE),
            re.compile(r'certified\s+\w+', re.IGNORECASE),
            re.compile(r'licensed\s+\w+', re.IGNORECASE),
            re.compile(r'EPA\s+\w+', re.IGNORECASE),
            re.compile(r'R2\s+certified', re.IGNORECASE)
        ]
        
        certifications = []
        for pattern in cert_patterns:
            matches = pattern.findall(page_text)
            certifications.extend(matches[:3])
        
        return certifications if certifications else None

    def _extract_years_in_business(self, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–µ—Ç —Ä–∞–±–æ—Ç—ã"""
        years_patterns = [
            re.compile(r'(\d{1,2})\s*\+?\s*years?\s+(?:in\s+)?business', re.IGNORECASE),
            re.compile(r'established\s+(?:in\s+)?(\d{4})', re.IGNORECASE),
            re.compile(r'since\s+(\d{4})', re.IGNORECASE),
            re.compile(r'founded\s+(?:in\s+)?(\d{4})', re.IGNORECASE)
        ]
        
        for pattern in years_patterns:
            matches = pattern.findall(page_text)
            if matches:
                return matches[0]
        
        return None

    def _extract_languages(self, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤"""
        language_patterns = [
            re.compile(r'languages?\s*:?\s*([^.]+)', re.IGNORECASE),
            re.compile(r'speak\s+([^.]+)', re.IGNORECASE),
            re.compile(r'bilingual\s+([^.]+)', re.IGNORECASE)
        ]
        
        for pattern in language_patterns:
            matches = pattern.findall(page_text)
            if matches:
                return matches[0][:50]
        
        return None

    def _extract_payment_methods(self, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–æ–≤ –æ–ø–ª–∞—Ç—ã"""
        payment_keywords = [
            'cash', 'check', 'credit card', 'debit card', 'paypal',
            'wire transfer', 'bank transfer', 'financing', 'terms'
        ]
        
        payment_methods = []
        text_lower = page_text.lower()
        
        for method in payment_keywords:
            if method in text_lower:
                payment_methods.append(method)
        
        return payment_methods if payment_methods else None

    def _extract_additional_services(self, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ª—É–≥"""
        additional_services = []
        
        service_patterns = [
            re.compile(r'we\s+also\s+([^.]+)', re.IGNORECASE),
            re.compile(r'additionally\s+([^.]+)', re.IGNORECASE),
            re.compile(r'other\s+services\s*:?\s*([^.]+)', re.IGNORECASE)
        ]
        
        for pattern in service_patterns:
            matches = pattern.findall(page_text)
            additional_services.extend(matches)
        
        return additional_services[:3] if additional_services else None

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _extract_name_from_json_ld_comprehensive(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ JSON-LD"""
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                name = self._extract_name_from_json_ld(data)
                if name:
                    return name
            except:
                continue
        return None

    def _extract_name_from_meta_tags(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤"""
        meta_tags = [
            soup.find('meta', property='og:site_name'),
            soup.find('meta', property='og:title'),
            soup.find('meta', {'name': 'application-name'}),
            soup.find('meta', {'name': 'twitter:title'})
        ]
        
        for tag in meta_tags:
            if tag:
                content = tag.get('content', '')
                if content and len(content) > 2:
                    return content
        
        return None

    def _extract_name_from_title(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ title"""
        title_tag = soup.find('title')
        if title_tag:
            title_text = title_tag.get_text().strip()
            if title_text:
                return title_text
        return None

    def _extract_name_from_h1(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ H1"""
        h1_tag = soup.find('h1')
        if h1_tag:
            h1_text = h1_tag.get_text().strip()
            if h1_text:
                return h1_text
        return None

    def _extract_description_comprehensive(self, soup):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è"""
        # –ü–æ–∏—Å–∫ –≤ –º–µ—Ç–∞-—Ç–µ–≥–∞—Ö
        meta_descriptions = [
            soup.find('meta', {'name': 'description'}),
            soup.find('meta', property='og:description'),
            soup.find('meta', {'name': 'twitter:description'})
        ]
        
        for meta in meta_descriptions:
            if meta:
                content = meta.get('content', '')
                if content and len(content) > 20:
                    return content[:500]
        
        # –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ
        description_selectors = [
            '.description',
            '.about',
            '.overview',
            '.intro',
            '.summary'
        ]
        
        for selector in description_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text(strip=True)
                if len(text) > 50:
                    return text[:500]
        
        return None

    def _extract_address_comprehensive(self, soup, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞"""
        # JSON-LD
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                address = self._extract_address_from_json_ld(data)
                if address:
                    return address
            except:
                continue
        
        # –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        address_elements = soup.find_all(attrs={'itemprop': re.compile(r'address|street', re.IGNORECASE)})
        for element in address_elements:
            address = element.get('content') or element.get_text().strip()
            if len(address) > 10:
                return address[:200]
        
        return None

    def _extract_city_comprehensive(self, soup, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞"""
        # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥—Ä—É–≥–∏–º –º–µ—Ç–æ–¥–∞–º
        city_elements = soup.find_all(attrs={'itemprop': re.compile(r'city|locality', re.IGNORECASE)})
        for element in city_elements:
            city = element.get('content') or element.get_text().strip()
            if city and len(city) > 2:
                return city[:50]
        
        return None

    def _extract_state_comprehensive(self, soup, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —à—Ç–∞—Ç–∞"""
        state_elements = soup.find_all(attrs={'itemprop': re.compile(r'state|region', re.IGNORECASE)})
        for element in state_elements:
            state = element.get('content') or element.get_text().strip()
            if state and len(state) >= 2:
                return state[:20]
        
        return None

    def _extract_zip_comprehensive(self, soup, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ—á—Ç–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        # –ü–æ–∏—Å–∫ –≤ –º–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã—Ö
        zip_elements = soup.find_all(attrs={'itemprop': re.compile(r'postal|zip', re.IGNORECASE)})
        for element in zip_elements:
            zip_code = element.get('content') or element.get_text().strip()
            if zip_code and re.match(r'^\d{5}(-\d{4})?$', zip_code):
                return zip_code
        
        # –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ
        zip_patterns = [
            re.compile(r'\b\d{5}(-\d{4})?\b'),  # US ZIP
            re.compile(r'\b[A-Z]\d[A-Z]\s*\d[A-Z]\d\b'),  # Canada postal code
            re.compile(r'\b[A-Z]{1,2}\d{1,2}\s*\d[A-Z]{2}\b')  # UK postcode
        ]
        
        for pattern in zip_patterns:
            matches = pattern.findall(page_text)
            if matches:
                return matches[0] if isinstance(matches[0], str) else matches[0][0]
        
        return None

    def _extract_latitude(self, soup, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —à–∏—Ä–æ—Ç—ã"""
        # –ü–æ–∏—Å–∫ –≤ –º–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã—Ö –∏ JSON-LD
        lat_elements = soup.find_all(attrs={'itemprop': 'latitude'})
        for element in lat_elements:
            lat = element.get('content') or element.get_text().strip()
            try:
                return float(lat)
            except:
                continue
        
        return None

    def _extract_longitude(self, soup, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ª–≥–æ—Ç—ã"""
        lng_elements = soup.find_all(attrs={'itemprop': 'longitude'})
        for element in lng_elements:
            lng = element.get('content') or element.get_text().strip()
            try:
                return float(lng)
            except:
                continue
        
        return None

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è JSON-LD
    def _extract_phone_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            for key in ['telephone', 'phone', 'contactPoint']:
                if key in data:
                    value = data[key]
                    if isinstance(value, str):
                        return value
                    elif isinstance(value, dict) and 'telephone' in value:
                        return value['telephone']
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    result = self._extract_phone_from_json_ld(value)
                    if result:
                        return result
        
        elif isinstance(data, list):
            for item in data:
                result = self._extract_phone_from_json_ld(item)
                if result:
                    return result
        
        return None

    def _extract_email_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ email –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            for key in ['email', 'contactPoint']:
                if key in data:
                    value = data[key]
                    if isinstance(value, str) and '@' in value:
                        return value
                    elif isinstance(value, dict) and 'email' in value:
                        return value['email']
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    result = self._extract_email_from_json_ld(value)
                    if result:
                        return result
        
        elif isinstance(data, list):
            for item in data:
                result = self._extract_email_from_json_ld(item)
                if result:
                    return result
        
        return None

    def _extract_name_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            for key in ['name', 'legalName', 'alternateName']:
                if key in data and isinstance(data[key], str):
                    return data[key].strip()
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    result = self._extract_name_from_json_ld(value)
                    if result:
                        return result
        
        elif isinstance(data, list):
            for item in data:
                result = self._extract_name_from_json_ld(item)
                if result:
                    return result
        
        return None

    def _extract_address_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            if 'address' in data:
                addr = data['address']
                if isinstance(addr, str):
                    return addr
                elif isinstance(addr, dict):
                    parts = []
                    for key in ['streetAddress', 'addressLocality', 'addressRegion', 'postalCode']:
                        if key in addr and addr[key]:
                            parts.append(str(addr[key]))
                    if parts:
                        return ', '.join(parts)
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    result = self._extract_address_from_json_ld(value)
                    if result:
                        return result
        
        elif isinstance(data, list):
            for item in data:
                result = self._extract_address_from_json_ld(item)
                if result:
                    return result
        
        return None

    def _validate_email_global(self, email):
        """–ì–ª–æ–±–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è email"""
        if not email or '@' not in email:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        exclude_domains = [
            'example.com', 'test.com', 'domain.com',
            'google.com', 'facebook.com', 'twitter.com',
            'linkedin.com', 'youtube.com', 'instagram.com'
        ]
        
        email_lower = email.lower()
        for domain in exclude_domains:
            if domain in email_lower:
                return False
        
        # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
        if re.match(r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}$', email):
            return True
        
        return False

    def _calculate_data_completeness(self, business):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö"""
        important_fields = [
            'name', 'phone', 'email', 'website', 'address',
            'city', 'state', 'country', 'working_hours',
            'materials_accepted', 'services', 'description'
        ]
        
        filled_fields = 0
        for field in important_fields:
            value = business.get(field)
            if value and (not isinstance(value, list) or len(value) > 0):
                filled_fields += 1
        
        return int((filled_fields / len(important_fields)) * 100)

    def _is_valid_url(self, url):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL"""
        try:
            parsed = urlparse(url)
            return parsed.scheme in ['http', 'https'] and parsed.netloc
        except:
            return False

    def _deduplicate_links(self, links):
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Å—ã–ª–æ–∫"""
        seen_urls = set()
        unique_links = []
        
        for link in links:
            url = link['url']
            if url not in seen_urls:
                seen_urls.add(url)
                unique_links.append(link)
        
        return unique_links

    def _finalize_comprehensive_results(self, businesses, target_count):
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É –∏ email
        seen_contacts = set()
        unique_businesses = []
        
        for business in businesses:
            phone = business.get('phone', '')
            email = business.get('email', '')
            
            contact_key = f"{phone}|{email}"
            if contact_key not in seen_contacts:
                seen_contacts.add(contact_key)
                unique_businesses.append(business)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ–ª–Ω–æ—Ç–µ –¥–∞–Ω–Ω—ã—Ö
        unique_businesses.sort(key=lambda x: x.get('data_completeness', 0), reverse=True)
        
        return unique_businesses[:target_count]

    def _calculate_contact_percentage(self):
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤"""
        if not self.results:
            return 0
        
        with_contacts = sum(1 for business in self.results 
                           if business.get('phone') or business.get('email') or business.get('whatsapp'))
        return (with_contacts / len(self.results)) * 100

    def export_comprehensive_results(self, output_dir="output"):
        """–≠–∫—Å–ø–æ—Ä—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not self.results:
            self.logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return None
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # DataFrame
        df = pd.DataFrame(self.results)
        
        # CSV
        csv_file = os.path.join(output_dir, f"comprehensive_metal_businesses_{timestamp}.csv")
        df.to_csv(csv_file, index=False)
        
        # Excel —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ª–∏—Å—Ç–∞–º–∏
        excel_file = os.path.join(output_dir, f"comprehensive_metal_businesses_{timestamp}.xlsx")
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            # –ì–ª–∞–≤–Ω—ã–π –ª–∏—Å—Ç
            df.to_excel(writer, sheet_name='All Businesses', index=False)
            
            # –õ–∏—Å—Ç —Å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            high_quality = df[df['data_completeness'] >= 70]
            if not high_quality.empty:
                high_quality.to_excel(writer, sheet_name='High Quality Data', index=False)
            
            # –õ–∏—Å—Ç —Å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            contacts_df = df[['name', 'phone', 'email', 'whatsapp', 'website', 'address', 'city', 'state', 'country']]
            contacts_df.to_excel(writer, sheet_name='Contact Information', index=False)
            
            # –õ–∏—Å—Ç —Å –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –∏ —Ü–µ–Ω–∞–º–∏
            materials_df = df[['name', 'materials_accepted', 'pricing_info', 'services', 'certifications']]
            materials_df.to_excel(writer, sheet_name='Materials & Pricing', index=False)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats_data = self._create_comprehensive_statistics()
            stats_df = pd.DataFrame(stats_data)
            stats_df.to_excel(writer, sheet_name='Statistics', index=False)
        
        # JSON
        json_file = os.path.join(output_dir, f"comprehensive_metal_businesses_{timestamp}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, default=str, ensure_ascii=False)
        
        # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç
        report_file = self._create_comprehensive_report(output_dir, timestamp)
        
        self.logger.info(f"‚úÖ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ï –¥–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã:")
        self.logger.info(f"  ‚Ä¢ CSV: {csv_file}")
        self.logger.info(f"  ‚Ä¢ Excel: {excel_file}")
        self.logger.info(f"  ‚Ä¢ JSON: {json_file}")
        self.logger.info(f"  ‚Ä¢ –û—Ç—á–µ—Ç: {report_file}")
        
        return {
            'csv': csv_file,
            'excel': excel_file,
            'json': json_file,
            'report': report_file,
            'count': len(self.results)
        }

    def _create_comprehensive_statistics(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if not self.results:
            return []
        
        total = len(self.results)
        
        stats = [
            {'Metric': 'Total Businesses', 'Count': total, 'Percentage': '100.0%'},
            {'Metric': 'With Phone Numbers', 'Count': sum(1 for b in self.results if b.get('phone')), 'Percentage': f"{sum(1 for b in self.results if b.get('phone'))/total*100:.1f}%"},
            {'Metric': 'With Email Addresses', 'Count': sum(1 for b in self.results if b.get('email')), 'Percentage': f"{sum(1 for b in self.results if b.get('email'))/total*100:.1f}%"},
            {'Metric': 'With WhatsApp', 'Count': sum(1 for b in self.results if b.get('whatsapp')), 'Percentage': f"{sum(1 for b in self.results if b.get('whatsapp'))/total*100:.1f}%"},
            {'Metric': 'With Complete Address', 'Count': sum(1 for b in self.results if b.get('address')), 'Percentage': f"{sum(1 for b in self.results if b.get('address'))/total*100:.1f}%"},
            {'Metric': 'With Working Hours', 'Count': sum(1 for b in self.results if b.get('working_hours')), 'Percentage': f"{sum(1 for b in self.results if b.get('working_hours'))/total*100:.1f}%"},
            {'Metric': 'With Materials Info', 'Count': sum(1 for b in self.results if b.get('materials_accepted')), 'Percentage': f"{sum(1 for b in self.results if b.get('materials_accepted'))/total*100:.1f}%"},
            {'Metric': 'With Pricing Info', 'Count': sum(1 for b in self.results if b.get('pricing_info')), 'Percentage': f"{sum(1 for b in self.results if b.get('pricing_info'))/total*100:.1f}%"},
            {'Metric': 'With Services Info', 'Count': sum(1 for b in self.results if b.get('services')), 'Percentage': f"{sum(1 for b in self.results if b.get('services'))/total*100:.1f}%"},
            {'Metric': 'With Social Media', 'Count': sum(1 for b in self.results if b.get('social_media')), 'Percentage': f"{sum(1 for b in self.results if b.get('social_media'))/total*100:.1f}%"},
            {'Metric': 'High Quality Data (>70%)', 'Count': sum(1 for b in self.results if b.get('data_completeness', 0) > 70), 'Percentage': f"{sum(1 for b in self.results if b.get('data_completeness', 0) > 70)/total*100:.1f}%"},
        ]
        
        return stats

    def _create_comprehensive_report(self, output_dir, timestamp):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        report_file = os.path.join(output_dir, f"comprehensive_report_{timestamp}.txt")
        
        total_businesses = len(self.results)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats = self._create_comprehensive_statistics()
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º
        countries = {}
        for business in self.results:
            country = business.get('country', 'Unknown')
            countries[country] = countries.get(country, 0) + 1
        
        # –¢–æ–ø –º–∞—Ç–µ—Ä–∏–∞–ª—ã
        all_materials = []
        for business in self.results:
            materials = business.get('materials_accepted', [])
            if materials:
                all_materials.extend(materials)
        
        material_counts = {}
        for material in all_materials:
            material_counts[material] = material_counts.get(material, 0) + 1
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("üá∫üá∏ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –û–¢–ß–ï–¢ –ü–û US SCRAP METAL –°–ë–û–†–£\n")
            f.write("=" * 70 + "\n\n")
            f.write(f"–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–ú–µ—Ç–æ–¥ —Å–±–æ—Ä–∞: US –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–∏—Å–∫\n")
            f.write(f"–û—Ö–≤–∞—Ç: {len(self.target_locations)} –ª–æ–∫–∞—Ü–∏–π –ø–æ –°–®–ê\n\n")
            
            f.write("üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n")
            f.write("-" * 30 + "\n")
            for stat in stats:
                f.write(f"{stat['Metric']}: {stat['Count']} ({stat['Percentage']})\n")
            f.write("\n")
            
            f.write("üá∫üá∏ –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –®–¢–ê–¢–ê–ú\n")
            f.write("-" * 35 + "\n")
            for state, count in sorted(countries.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_businesses) * 100
                f.write(f"{state}: {count} –±–∏–∑–Ω–µ—Å–æ–≤ ({percentage:.1f}%)\n")
            f.write("\n")
            
            f.write("üîß –ü–û–ü–£–õ–Ø–†–ù–´–ï –ú–ê–¢–ï–†–ò–ê–õ–´\n")
            f.write("-" * 25 + "\n")
            top_materials = sorted(material_counts.items(), key=lambda x: x[1], reverse=True)[:15]
            for material, count in top_materials:
                f.write(f"{material}: {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π\n")
            f.write("\n")
            
            f.write("üéØ –ö–õ–Æ–ß–ï–í–´–ï –î–û–°–¢–ò–ñ–ï–ù–ò–Ø\n")
            f.write("-" * 25 + "\n")
            avg_completeness = sum(b.get('data_completeness', 0) for b in self.results) / total_businesses
            f.write(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –ø–æ–ª–Ω–æ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö: {avg_completeness:.1f}%\n")
            f.write(f"‚Ä¢ US –æ—Ö–≤–∞—Ç: {len(countries)} —à—Ç–∞—Ç–æ–≤\n")
            f.write(f"‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: –°—Ç—Ä–æ–≥–∞—è US –ø—Ä–æ–≤–µ—Ä–∫–∞\n")
            f.write(f"‚Ä¢ –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ—Å—Ç—å: {len(stats)} –º–µ—Ç—Ä–∏–∫ —Å–æ–±—Ä–∞–Ω–æ\n")
            f.write(f"‚Ä¢ –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å: –í—ã—Å–æ–∫–∞—è\n")
            f.write(f"‚Ä¢ –ú–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: –î–µ—Ç–∞–ª—å–Ω–∞—è\n\n")
            
            f.write("üí° –ë–ò–ó–ù–ï–°-–ê–ù–ê–õ–ò–¢–ò–ö–ê\n")
            f.write("-" * 20 + "\n")
            f.write("‚Ä¢ Copper –∏ aluminum - –Ω–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã\n")
            f.write("‚Ä¢ –°—Ä–µ–¥–Ω–∏–µ US –≥–æ—Ä–æ–¥–∞ –∏–º–µ—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤\n")
            f.write("‚Ä¢ Rust Belt —Ä–µ–≥–∏–æ–Ω—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤—ã—Å–æ–∫—É—é –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é –±–∏–∑–Ω–µ—Å–æ–≤\n")
            f.write("‚Ä¢ –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –∞–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤\n")
            f.write("‚Ä¢ –°—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏—Å–∫–ª—é—á–∞–µ—Ç –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ–º–µ—Ä–∞\n\n")
            
            f.write("üöÄ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø OUTREACH\n")
            f.write("-" * 30 + "\n")
            f.write("1. –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–∏–∑–Ω–µ—Å—ã —Å –≤—ã—Å–æ–∫–æ–π –ø–æ–ª–Ω–æ—Ç–æ–π –¥–∞–Ω–Ω—ã—Ö (>70%)\n")
            f.write("2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã —Å–≤—è–∑–∏ (—Ç–µ–ª–µ—Ñ–æ–Ω, email, WhatsApp)\n")
            f.write("3. –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥—Ö–æ–¥ –ø–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏\n")
            f.write("4. –§–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö (copper, aluminum)\n")
            f.write("5. –£—á–∏—Ç—ã–≤–∞—Ç—å —Ä–∞–±–æ—á–∏–µ —á–∞—Å—ã –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∫–æ–Ω—Ç–∞–∫—Ç–∞\n")
            f.write("6. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ö–≤–∞—Ç–∞\n")
        
        return report_file

def main():
    print("‚ö° –°–£–ü–ï–†-–ë–´–°–¢–†–´–ô US SCRAP METAL –ü–ê–†–°–ï–†")
    print("=" * 65)
    print("üöÄ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –°–ö–û–†–û–°–¢–¨")
    print("üá∫üá∏ –§–û–ö–£–° –ù–ê –°–®–ê")
    print("üìû –ü–†–ò–û–†–ò–¢–ï–¢ –ö–û–ù–¢–ê–ö–¢–û–í")
    print("‚ö° –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê")
    print("üéØ –¢–û–ß–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï")
    print("üí® –ë–´–°–¢–†–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    
    scraper = USMetalScraper()
    
    try:
        target_count = input("\n–¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏–∑–Ω–µ—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 200): ").strip()
        target_count = int(target_count) if target_count else 200
        
        print(f"\nüöÄ –ó–∞–ø—É—Å–∫ –°–£–ü–ï–†-–ë–´–°–¢–†–û–ì–û —Å–±–æ—Ä–∞ –¥–ª—è {target_count} –±–∏–∑–Ω–µ—Å–æ–≤...")
        print("üá∫üá∏ –û—Ö–≤–∞—Ç: –°–®–ê (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)")
        print("üìã –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –¢–µ–ª–µ—Ñ–æ–Ω—ã, Email, –ê–¥—Ä–µ—Å–∞")
        print("‚ö° –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ + Regex")
        print("üéØ –°–∫–æ—Ä–æ—Å—Ç—å: –î–æ 10x –±—ã—Å—Ç—Ä–µ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ")
        print(f"‚è±Ô∏è –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: {max(1, target_count // 20)}-{max(2, target_count // 10)} –º–∏–Ω—É—Ç")
        print(f"üìä –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {target_count} –±–∏–∑–Ω–µ—Å–æ–≤ —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏")
        
        confirmation = input("\n–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (y/N): ").lower().strip()
        if confirmation != 'y':
            print("‚ùå –û—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            return
        
        results = scraper.run_comprehensive_scraping(target_count)
        
        if results:
            print(f"\n‚úÖ –°—É–ø–µ—Ä-–±—ã—Å—Ç—Ä—ã–π —Å–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω! {len(results)} –±–∏–∑–Ω–µ—Å–æ–≤ –Ω–∞–π–¥–µ–Ω–æ!")
            
            export_info = scraper.export_comprehensive_results()
            if export_info:
                print(f"\nüìÅ –§–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã:")
                print(f"  ‚Ä¢ CSV: {export_info['csv']}")
                print(f"  ‚Ä¢ Excel: {export_info['excel']}")
                print(f"  ‚Ä¢ JSON: {export_info['json']}")
                print(f"  ‚Ä¢ –û—Ç—á–µ—Ç: {export_info['report']}")
                
                contact_percentage = scraper._calculate_contact_percentage()
                
                print(f"\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
                print(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {export_info['count']} –±–∏–∑–Ω–µ—Å–æ–≤")
                print(f"üìû –° –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏: {contact_percentage:.1f}%")
                print(f"üá∫üá∏ US –æ—Ö–≤–∞—Ç: –î–æ—Å—Ç–∏–≥–Ω—É—Ç")
                print(f"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è")
                print("\nüöÄ US SCRAP METAL –ë–ê–ó–ê –ì–û–¢–û–í–ê –î–õ–Ø OUTREACH!")
                print("üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CSV/Excel –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤")
            else:
                print("‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞")
        else:
            print("‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–±—Ä–∞–Ω—ã")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        if scraper.results:
            print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
            scraper.export_comprehensive_results()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        scraper.logger.error(f"Error: {e}", exc_info=True)

if __name__ == "__main__":
    main() 