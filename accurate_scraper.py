#!/usr/bin/env python3
"""
–¢–û–ß–ù–´–ô –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ü–ê–†–°–ï–† –ú–ï–¢–ê–õ–õ–û–õ–û–ú–ê
100% –¢–û–ß–ù–´–ï –î–ê–ù–ù–´–ï | –ì–õ–û–ë–ê–õ–¨–ù–´–ô –û–•–í–ê–¢ | –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ü–û–õ–ù–û–¢–ê
"""

import os
import sys
import json
import time
import random
import logging
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
from urllib.parse import quote_plus, urlparse
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import urllib3
urllib3.disable_warnings()

class USMetalScraper:
    def __init__(self):
        self.session = requests.Session()
        self.results = []
        self.processed_urls = set()
        self.logger = self._setup_logging()
        
        # –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô –¢–û–ß–ù–û–°–¢–ò –ò –ü–û–õ–ù–û–¢–´
        self.MIN_PHONE_PERCENTAGE = 50  # –°–Ω–∏–∂–µ–Ω–æ –¥–æ 50% –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        self.TIMEOUT = 10               # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        self.MAX_WORKERS = 16           # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø–æ—Ç–æ–∫–∏
        self.LINK_BATCH_SIZE = 50       # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ –±–∞—Ç—á–∏ –¥–ª—è —Å—Å—ã–ª–æ–∫
        self.MAX_LINKS_PER_SEARCH = 50  # –ë–æ–ª—å—à–µ —Å—Å—ã–ª–æ–∫ —Å –∫–∞–∂–¥–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.TARGET_SUCCESS_RATE = 0.15 # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —Ü–µ–ª–µ–≤–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞ (15%)
        
        # –£–õ–£–ß–®–ï–ù–ù–´–ï US PHONE PATTERNS - –ë–æ–ª–µ–µ –≥–∏–±–∫–∏–µ –∏ –ø–æ–ª–Ω—ã–µ
        self.phone_patterns = [
            # Standard US format: (555) 123-4567
            re.compile(r'\b\(?([2-9][0-8][0-9])\)?[-.\s]?([2-9][0-9]{2})[-.\s]?([0-9]{4})\b'),
            # US with country code: 1-555-123-4567
            re.compile(r'\b1[-.\s]?\(?([2-9][0-8][0-9])\)?[-.\s]?([2-9][0-9]{2})[-.\s]?([0-9]{4})\b'),
            # Toll-free numbers: 800-123-4567
            re.compile(r'\b\(?([8][0-9]{2})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})\b'),
            # Various US formatting variations
            re.compile(r'\b([2-9][0-8][0-9])[-.\s]+([2-9][0-9]{2})[-.\s]+([0-9]{4})\b'),
            re.compile(r'\b([2-9][0-8][0-9])\.([2-9][0-9]{2})\.([0-9]{4})\b'),
            re.compile(r'\b([2-9][0-8][0-9])\s([2-9][0-9]{2})\s([0-9]{4})\b'),
            # Tel: links format - –±–æ–ª–µ–µ –≥–∏–±–∫–∏–π
            re.compile(r'tel:[\s]*\+?1?[-.\s]?\(?([2-9][0-8][0-9])\)?[-.\s]?([2-9][0-9]{2})[-.\s]?([0-9]{4})', re.IGNORECASE),
            # International format with +1
            re.compile(r'\+1[-.\s]?\(?([2-9][0-8][0-9])\)?[-.\s]?([2-9][0-9]{2})[-.\s]?([0-9]{4})\b'),
            # Loose format for business numbers
            re.compile(r'\b([2-9][0-8][0-9])[^\d]*([2-9][0-9]{2})[^\d]*([0-9]{4})\b'),
            # More flexible patterns for website display
            re.compile(r'(?:phone|tel|call)[\s:]*\(?([2-9][0-8][0-9])\)?[-.\s]?([2-9][0-9]{2})[-.\s]?([0-9]{4})', re.IGNORECASE),
        ]
        
        # –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ü–û–ò–°–ö–û–í–´–ï –ó–ê–ü–†–û–°–´ (–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ô –û–•–í–ê–¢)
        self.search_queries = [
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
            'scrap metal dealers',
            'metal recycling center',
            'scrap yard',
            'junk yard auto parts',
            'copper scrap buyers',
            'aluminum recycling',
            'auto salvage yard',
            'scrap metal pickup',
            'metal scrap dealers',
            'recycling centre',
            'scrap metal merchants',
            'waste metal collection',
            'metal recovery services',
            'industrial metal recycling',
            'non-ferrous metal dealers',
            
            # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
            'copper wire buyers',
            'aluminum can recycling',
            'steel scrap buyers',
            'iron scrap dealers',
            'brass scrap buyers',
            'stainless steel recycling',
            'catalytic converter buyers',
            'car battery recycling',
            'radiator scrap buyers',
            'electric motor scrap',
            
            # –¢–∏–ø—ã –±–∏–∑–Ω–µ—Å–æ–≤
            'metal processing facility',
            'scrap metal facility',
            'metal salvage company',
            'industrial metal buyers',
            'commercial metal recycling',
            'metal waste management',
            'scrap metal collection',
            'metal demolition services',
            'construction metal recycling',
            'automotive metal recycling',
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
            'cash for scrap metal',
            'sell scrap metal near me',
            'metal buyers near me',
            'scrap metal prices',
            'metal recycling services',
            'scrap metal removal',
            'metal demolition company',
            'industrial scrap buyers',
            'commercial scrap metal',
            'heavy metal recycling'
        ]
        
        # –†–ê–°–®–ò–†–ï–ù–ù–´–ï US TARGET LOCATIONS - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –æ—Ö–≤–∞—Ç –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        self.target_locations = [
            # Major US metropolitan areas (Tier 1)
            'New York NY', 'Los Angeles CA', 'Chicago IL', 'Houston TX', 'Phoenix AZ',
            'Philadelphia PA', 'San Antonio TX', 'San Diego CA', 'Dallas TX', 'San Jose CA',
            'Austin TX', 'Jacksonville FL', 'Fort Worth TX', 'Columbus OH', 'Charlotte NC',
            'San Francisco CA', 'Indianapolis IN', 'Seattle WA', 'Denver CO', 'Washington DC',
            'Boston MA', 'El Paso TX', 'Nashville TN', 'Detroit MI', 'Oklahoma City OK',
            'Portland OR', 'Las Vegas NV', 'Memphis TN', 'Louisville KY', 'Baltimore MD',
            'Milwaukee WI', 'Albuquerque NM', 'Tucson AZ', 'Fresno CA', 'Sacramento CA',
            'Mesa AZ', 'Kansas City MO', 'Atlanta GA', 'Long Beach CA', 'Colorado Springs CO',
            
            # High-potential scrap metal markets (Tier 2)
            'Cleveland OH', 'Pittsburgh PA', 'Cincinnati OH', 'Toledo OH', 'Akron OH',
            'Dayton OH', 'Youngstown OH', 'Canton OH', 'Buffalo NY', 'Rochester NY',
            'Syracuse NY', 'Albany NY', 'Utica NY', 'Binghamton NY', 'Elmira NY',
            'Scranton PA', 'Allentown PA', 'Reading PA', 'Erie PA', 'Bethlehem PA',
            'Harrisburg PA', 'Lancaster PA', 'York PA', 'Wilkes-Barre PA',
            'Flint MI', 'Lansing MI', 'Kalamazoo MI', 'Grand Rapids MI', 'Saginaw MI',
            'Birmingham AL', 'Mobile AL', 'Montgomery AL', 'Huntsville AL',
            'Little Rock AR', 'Fayetteville AR', 'Jonesboro AR', 'Pine Bluff AR',
            
            # Additional strategic locations (Tier 3)
            'Tampa FL', 'Miami FL', 'Orlando FL', 'St. Petersburg FL', 'Hialeah FL',
            'Tallahassee FL', 'Fort Lauderdale FL', 'Pembroke Pines FL', 'Hollywood FL',
            'Gainesville FL', 'Coral Springs FL', 'Clearwater FL', 'Lakeland FL',
            'Virginia Beach VA', 'Norfolk VA', 'Chesapeake VA', 'Richmond VA', 'Newport News VA',
            'Alexandria VA', 'Portsmouth VA', 'Suffolk VA', 'Hampton VA', 'Roanoke VA',
            'Omaha NE', 'Lincoln NE', 'Bellevue NE', 'Grand Island NE', 'Kearney NE',
            'Fremont NE', 'Hastings NE', 'North Platte NE', 'Norfolk NE', 'Columbus NE',
            
            # Midwest expansion
            'Minneapolis MN', 'St. Paul MN', 'Rochester MN', 'Duluth MN', 'Bloomington MN',
            'Brooklyn Park MN', 'Plymouth MN', 'St. Cloud MN', 'Eagan MN', 'Woodbury MN',
            'Maple Grove MN', 'Eden Prairie MN', 'Coon Rapids MN', 'Burnsville MN',
            'Green Bay WI', 'Appleton WI', 'Oshkosh WI', 'Racine WI', 'Kenosha WI',
            'Eau Claire WI', 'Wausau WI', 'La Crosse WI', 'Janesville WI', 'West Allis WI',
            
            # Southwest expansion
            'Tucson AZ', 'Mesa AZ', 'Chandler AZ', 'Glendale AZ', 'Scottsdale AZ',
            'Gilbert AZ', 'Tempe AZ', 'Peoria AZ', 'Surprise AZ', 'Yuma AZ',
            'Flagstaff AZ', 'Lake Havasu City AZ', 'Casa Grande AZ', 'Oro Valley AZ',
            'Albuquerque NM', 'Las Cruces NM', 'Rio Rancho NM', 'Santa Fe NM',
            'Roswell NM', 'Farmington NM', 'Clovis NM', 'Hobbs NM', 'Alamogordo NM',
            
            # Texas expansion
            'Houston TX', 'San Antonio TX', 'Dallas TX', 'Austin TX', 'Fort Worth TX',
            'El Paso TX', 'Arlington TX', 'Corpus Christi TX', 'Plano TX', 'Lubbock TX',
            'Laredo TX', 'Garland TX', 'Irving TX', 'Amarillo TX', 'Grand Prairie TX',
            'Brownsville TX', 'McKinney TX', 'Frisco TX', 'Pasadena TX', 'Killeen TX',
            
            # California expansion
            'Los Angeles CA', 'San Diego CA', 'San Jose CA', 'San Francisco CA',
            'Fresno CA', 'Sacramento CA', 'Long Beach CA', 'Oakland CA', 'Bakersfield CA',
            'Anaheim CA', 'Santa Ana CA', 'Riverside CA', 'Stockton CA', 'Chula Vista CA',
            'Irvine CA', 'Fremont CA', 'San Bernardino CA', 'Modesto CA', 'Fontana CA',
            
            # East Coast expansion
            'Newark NJ', 'Jersey City NJ', 'Paterson NJ', 'Elizabeth NJ', 'Edison NJ',
            'Woodbridge NJ', 'Lakewood NJ', 'Toms River NJ', 'Hamilton NJ', 'Trenton NJ',
            'Camden NJ', 'Brick NJ', 'Howell NJ', 'Gloucester NJ', 'Union City NJ',
            'Providence RI', 'Warwick RI', 'Cranston RI', 'Pawtucket RI', 'East Providence RI',
            'Woonsocket RI', 'Newport RI', 'Central Falls RI', 'Westerly RI', 'Cumberland RI'
        ]
        
        # –ú–ê–¢–ï–†–ò–ê–õ–´ –î–õ–Ø –ü–û–ò–°–ö–ê
        self.materials_keywords = [
            'copper', 'aluminum', 'aluminium', 'steel', 'iron', 'brass', 'bronze',
            'stainless steel', 'lead', 'zinc', 'nickel', 'tin', 'titanium',
            'carbide', 'tungsten', 'precious metals', 'gold', 'silver', 'platinum',
            'catalytic converters', 'car batteries', 'radiators', 'electric motors',
            'transformers', 'wire', 'cable', 'circuit boards', 'electronic scrap',
            'computer scrap', 'mobile phones', 'cast iron', 'wrought iron',
            'structural steel', 'rebar', 'pipes', 'tubes', 'sheet metal',
            'coils', 'turnings', 'shredded metal', 'HMS', 'heavy melting scrap'
        ]
        
        # USER AGENTS
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/121.0.0.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15'
        ]
        
        self._init_session()

    def _init_session(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏"""
        self.session.headers.update({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'DNT': '1'
        })

    def _setup_logging(self):
        logger = logging.getLogger('USMetalScraper')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
        
        return logger

    def run_comprehensive_scraping(self, target_businesses=500):
        """–ö–û–ú–ü–õ–ï–ö–°–ù–´–ô —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ –°–®–ê —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–Ω–æ—Ç–æ–π"""
        self.logger.info(f"üá∫üá∏ –ó–ê–ü–£–°–ö US SCRAP METAL –°–ë–û–†–ê")
        self.logger.info(f"üìû –¶–ï–õ–¨: {target_businesses} –±–∏–∑–Ω–µ—Å–æ–≤ —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏")
        self.logger.info(f"üîç –û–•–í–ê–¢: {len(self.target_locations)} US –ª–æ–∫–∞—Ü–∏–π")
        self.logger.info(f"üìã –ú–ê–¢–ï–†–ò–ê–õ–´: {len(self.materials_keywords)} —Ç–∏–ø–æ–≤")
        
        start_time = time.time()
        
        # –≠—Ç–∞–ø 1: –ú–∞—Å—Å–æ–≤—ã–π —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫
        self.logger.info("üîó –≠—Ç–∞–ø 1: –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∏–∑ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º")
        all_links = self._collect_comprehensive_links()
        self.logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(all_links)}")
        
        # –≠—Ç–∞–ø 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.logger.info("üìä –≠—Ç–∞–ø 2: –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
        businesses = self._extract_comprehensive_data(all_links, target_businesses)
        
        # –≠—Ç–∞–ø 3: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ
        self.logger.info("üî¨ –≠—Ç–∞–ø 3: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
        self.results = self._finalize_comprehensive_results(businesses, target_businesses)
        
        elapsed = time.time() - start_time
        phone_percentage = self._calculate_contact_percentage()
        
        self.logger.info(f"‚úÖ US –°–ë–û–† –ó–ê–í–ï–†–®–ï–ù –∑–∞ {elapsed/60:.1f} –º–∏–Ω—É—Ç")
        self.logger.info(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢: {len(self.results)} –±–∏–∑–Ω–µ—Å–æ–≤")
        self.logger.info(f"üìû –° –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏: {phone_percentage:.1f}%")
        
        return self.results

    def _collect_comprehensive_links(self):
        """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ì–û —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫...")
        
        all_links = []
        
        # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ—Ö–≤–∞—Ç –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        target_locations = self.target_locations[:50]  # 50 —Ç–æ–ø-–ª–æ–∫–∞—Ü–∏–π
        target_queries = self.search_queries[:20]      # 20 –ª—É—á—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        search_tasks = []
        for location in target_locations:
            for query in target_queries:
                # –°—Ç—Ä–∞–Ω–∏—Ü—ã 2-5 –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –æ—Ö–≤–∞—Ç–∞
                for page in range(2, 6):
                    search_tasks.append((f"{query} {location}", page))
        
        self.logger.info(f"üìã –°–æ–∑–¥–∞–Ω–æ {len(search_tasks)} –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–¥–∞—á –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –æ—Ö–≤–∞—Ç–∞")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º–∏ –±–∞—Ç—á–∞–º–∏
        with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            for i in range(0, len(search_tasks), self.LINK_BATCH_SIZE):
                batch = search_tasks[i:i + self.LINK_BATCH_SIZE]
                batch_links = []
                
                # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ–º –±–∞—Ç—á –ø–æ–∏—Å–∫–æ–≤
                futures = {
                    executor.submit(self._fast_bing_search, query, page): (query, page)
                    for query, page in batch
                }
                
                for future in as_completed(futures, timeout=150):  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç
                    try:
                        links = future.result(timeout=10)
                        if links:
                            batch_links.extend(links)
                            
                            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø–æ–∏—Å–∫
                            if len(batch_links) >= 50:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 50
                                batch_links = batch_links[:50]
                                
                    except Exception as e:
                        self.logger.debug(f"Batch search failed: {e}")
                        continue
                
                # –î–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á –∫ –æ–±—â–µ–º—É —Å–ø–∏—Å–∫—É
                all_links.extend(batch_links)
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                progress = (i + self.LINK_BATCH_SIZE) / len(search_tasks) * 100
                self.logger.info(f"üìä –ë–∞—Ç—á {i//self.LINK_BATCH_SIZE + 1}: +{len(batch_links)} —Å—Å—ã–ª–æ–∫ | –í—Å–µ–≥–æ: {len(all_links)} | –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%")
                
                # –°–æ–±–∏—Ä–∞–µ–º –¥–æ 2000 —Å—Å—ã–ª–æ–∫ –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                if len(all_links) >= 2000:
                    self.logger.info(f"üéØ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç: {len(all_links)}")
                    break
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        unique_links = self._deduplicate_links(all_links)
        self.logger.info(f"‚úÖ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ô —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {len(unique_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
        return unique_links
    
    def _fast_bing_search(self, query, page):
        """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Bing –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        links = []
        
        try:
            start = (page - 1) * 10
            url = f"https://www.bing.com/search?q={quote_plus(query)}&first={start}&count=10"
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Cache-Control': 'no-cache',
                'Referer': 'https://www.bing.com/'
            }
            
            # –ù–∞–¥–µ–∂–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
            for attempt in range(2):
                try:
                    response = self.session.get(url, headers=headers, timeout=8, verify=False)
                    if response.status_code == 200:
                        break
                except:
                    if attempt == 0:
                        time.sleep(1)
                        continue
                    else:
                        return links
            
            if response.status_code == 200:
                # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏
                results = soup.find_all('li', class_='b_algo')
                
                for result in results:
                    try:
                        # –¢–æ—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                        h2 = result.find('h2')
                        if not h2:
                            continue
                            
                        link_elem = h2.find('a', href=True)
                        if not link_elem:
                            continue
                        
                        url = link_elem['href']
                        title = h2.get_text(strip=True)
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                        description = ""
                        desc_elem = result.find('p') or result.find('div', class_='b_caption')
                        if desc_elem:
                            description = desc_elem.get_text(strip=True)[:200]
                        
                        # –°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                        if self._is_highly_relevant(title, url, description):
                            links.append({
                                'url': url,
                                'title': title,
                                'description': description,
                                'page': page,
                                'query': query,
                                'source': 'Bing'
                            })
                            
                        # –°–æ–±–∏—Ä–∞–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                        if len(links) >= 12:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç
                            break
                            
                    except Exception as e:
                        continue
                        
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            time.sleep(random.uniform(0.8, 1.5))
                        
        except Exception as e:
            self.logger.debug(f"Search failed for '{query}' page {page}: {e}")
        
        return links
    
    def _is_highly_relevant(self, title, url, description):
        """–°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        title_lower = title.lower()
        url_lower = url.lower()
        desc_lower = description.lower()
        
        # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ —Å –≤—ã—Å–æ–∫–æ–π —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å—é
        highly_relevant = [
            'scrap', 'metal', 'recycling', 'salvage', 'junk', 'yard', 
            'steel', 'copper', 'aluminum', 'iron', 'brass', 'buyer',
            'dealer', 'processing', 'facility', 'center', 'company'
        ]
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–æ—á–Ω–æ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–∞–π—Ç—ã
        exclude_domains = [
            'wikipedia.org', 'facebook.com', 'youtube.com', 'linkedin.com', 
            'indeed.com', 'glassdoor.com', 'amazon.com', 'ebay.com',
            'craigslist.org', 'reddit.com', 'twitter.com', 'instagram.com',
            'pinterest.com', 'tiktok.com', 'zillow.com', 'realtor.com'
        ]
        
        exclude_words = [
            'software', 'app', 'game', 'news', 'blog', 'jobs', 'career', 
            'hiring', 'employment', 'resume', 'salary', 'review', 'rating',
            'price guide', 'calculator', 'directory', 'listing', 'classifieds'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –≤–æ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–∞—Ö
        combined_text = f"{title_lower} {url_lower} {desc_lower}"
        
        # –î–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ö–æ—Ç—è –±—ã 2 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–ª–æ–≤–∞
        relevant_count = sum(1 for word in highly_relevant if word in combined_text)
        has_sufficient_relevance = relevant_count >= 2
        
        # –ù–µ –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏—Å–∫–ª—é—á–∞—é—â–∏–µ –¥–æ–º–µ–Ω—ã –∏–ª–∏ —Å–ª–æ–≤–∞
        has_exclude_domain = any(domain in url_lower for domain in exclude_domains)
        has_exclude_word = any(word in combined_text for word in exclude_words)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–∏–∑–Ω–µ—Å-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        business_indicators = [
            'llc', 'inc', 'corp', 'company', 'co.', 'ltd', 'phone', 'contact',
            'address', 'location', 'hours', 'service', 'about us', 'home'
        ]
        has_business_indicators = any(indicator in combined_text for indicator in business_indicators)
        
        return (has_sufficient_relevance and not has_exclude_domain and 
                not has_exclude_word and has_business_indicators)

    def _extract_comprehensive_data(self, links, target_businesses):
        """–¢–æ—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏"""
        self.logger.info(f"üéØ –¢–û–ß–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {len(links)} —Å—Å—ã–ª–æ–∫")
        self.logger.info(f"üèÜ –¶–ï–õ–¨: –ù–∞–π—Ç–∏ –¢–û–ß–ù–û {target_businesses} –±–∏–∑–Ω–µ—Å–æ–≤")
        
        businesses = []
        processed_count = 0
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        links_to_process = len(links)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
        
        self.logger.info(f"üìä –ì–æ—Ç–æ–≤—ã –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å {links_to_process} —Å—Å—ã–ª–æ–∫ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏")
        
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        initial_batch_size = 50
        current_batch_size = initial_batch_size
        
        i = 0
        while i < links_to_process and len(businesses) < target_businesses:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –µ—Å–ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å –º–µ–¥–ª–µ–Ω–Ω—ã–π
            if i > 200 and len(businesses) < target_businesses * 0.3:
                current_batch_size = 60
            elif i > 400 and len(businesses) < target_businesses * 0.5:
                current_batch_size = 70
            
            batch = links[i:i + current_batch_size]
            batch_businesses = []
            
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
            with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
                futures = {
                    executor.submit(self._super_fast_extract, link): link 
                    for link in batch
                }
                
                for future in as_completed(futures, timeout=120):  # –ë–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–µ–π
                    try:
                        business = future.result(timeout=8)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç
                        processed_count += 1
                        
                        if business:
                            batch_businesses.append(business)
                            
                            # –ë—ã—Å—Ç—Ä–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –±–∏–∑–Ω–µ—Å–æ–≤
                            phone = business.get('phone', 'N/A')
                            name = business['name'][:30] + '...' if len(business['name']) > 30 else business['name']
                            self.logger.info(f"‚úÖ [{len(businesses) + len(batch_businesses)}] {name} | üìû {phone}")
                            
                    except Exception as e:
                        processed_count += 1
                        continue
            
            # –î–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á –∫ –æ–±—â–µ–º—É —Å–ø–∏—Å–∫—É
            businesses.extend(batch_businesses)
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            progress = (i + current_batch_size) / links_to_process * 100
            current_rate = len(businesses) / processed_count * 100 if processed_count > 0 else 0
            remaining_needed = target_businesses - len(businesses)
            
            self.logger.info(f"üìä –ë–∞—Ç—á {i//initial_batch_size + 1}: +{len(batch_businesses)} –±–∏–∑–Ω–µ—Å–æ–≤ | "
                           f"–í—Å–µ–≥–æ: {len(businesses)}/{target_businesses} | –û—Å—Ç–∞–ª–æ—Å—å: {remaining_needed} | "
                           f"–£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {current_rate:.1f}% | –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%")
            
            # –¢–û–ß–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –î–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ —Ü–µ–ª–∏?
            if len(businesses) >= target_businesses:
                self.logger.info(f"üéØ –¢–û–ß–ù–ê–Ø –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê: {len(businesses)} –±–∏–∑–Ω–µ—Å–æ–≤!")
                break
            
            # –ï—Å–ª–∏ –Ω–∞–º –Ω—É–∂–Ω–æ –≤—Å–µ–≥–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–∏–∑–Ω–µ—Å–æ–≤, —É–º–µ–Ω—å—à–∞–µ–º –±–∞—Ç—á –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            if remaining_needed <= 10 and remaining_needed > 0:
                current_batch_size = min(20, current_batch_size)
                self.logger.info(f"üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø—É—Ä—Ç: –Ω—É–∂–Ω–æ –µ—â–µ {remaining_needed} –±–∏–∑–Ω–µ—Å–æ–≤")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            time.sleep(0.2)
            
            i += current_batch_size
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        final_rate = len(businesses) / processed_count * 100 if processed_count > 0 else 0
        
        if len(businesses) >= target_businesses:
            # –û–±—Ä–µ–∑–∞–µ–º –¥–æ —Ç–æ—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
            businesses = businesses[:target_businesses]
            self.logger.info(f"üèÜ –ú–ò–°–°–ò–Ø –í–´–ü–û–õ–ù–ï–ù–ê: –ù–∞–π–¥–µ–Ω–æ –¢–û–ß–ù–û {len(businesses)} –±–∏–∑–Ω–µ—Å–æ–≤!")
        else:
            self.logger.info(f"‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {len(businesses)} –∏–∑ {target_businesses} –±–∏–∑–Ω–µ—Å–æ–≤")
            
        self.logger.info(f"üìà –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {len(businesses)} –±–∏–∑–Ω–µ—Å–æ–≤ –∏–∑ {processed_count} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö ({final_rate:.1f}%)")
        
        return businesses

    def _super_fast_extract(self, link_data):
        """–ê–ì–†–ï–°–°–ò–í–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ –≥–∏–±–∫–æ—Å—Ç—å—é"""
        url = link_data['url']
        
        try:
            if not self._is_valid_url(url):
                return None
            
            # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Connection': 'keep-alive',
                'Accept-Language': 'en-US,en;q=0.9'
            }
            
            response = self.session.get(url, headers=headers, timeout=self.TIMEOUT, verify=False)
            
            if response.status_code != 200:
                return None
            
            page_text = response.text
            soup = BeautifulSoup(page_text, 'html.parser')
            
            # –ê–ì–†–ï–°–°–ò–í–ù–´–ô –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
            phone = self._extract_phone_aggressive(page_text, soup)
            email = self._extract_email_aggressive(page_text, soup)
            
            # –ü—Ä–∏–Ω–∏–º–∞–µ–º, –µ—Å–ª–∏ –µ—Å—Ç—å –•–û–¢–Ø –ë–´ –û–î–ò–ù –∫–æ–Ω—Ç–∞–∫—Ç (phone –ò–õ–ò email)
            if not (phone or email):
                return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ metal/scrap industry
            if not self._is_relevant_to_industry(page_text, link_data):
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            name = self._extract_name_fast(link_data, soup)
            address = self._extract_address_fast(soup)
            materials = self._extract_materials_fast(page_text)
            
            business = {
                'name': name,
                'phone': phone,
                'email': email,
                'website': url,
                'address': address,
                'city': self._extract_city_fast(address),
                'state': self._extract_state_fast(address),
                'country': 'USA',
                'materials_accepted': materials,
                'source': 'Fast_Extraction',
                'extraction_method': 'aggressive_fast',
                'has_phone': bool(phone),
                'has_email': bool(email),
                'scraped_at': datetime.now().isoformat()
            }
            
            self.logger.info(f"‚úÖ [{len(self.results) + 1}] {name[:30]}... | üìû {phone or 'No phone'} | üìß {email or 'No email'}")
            
            return business
            
        except Exception as e:
            self.logger.debug(f"Fast extraction error from {url}: {e}")
            return None
    
    def _extract_phone_fallback(self, page_text, soup):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤"""
        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–∞—Ö
        contact_tags = soup.find_all(['span', 'div', 'p', 'td'], 
                                    class_=re.compile(r'contact|phone|tel', re.IGNORECASE))
        for tag in contact_tags:
            text = tag.get_text()
            phone = self._extract_phone_from_text_us(text)
            if phone:
                return phone
        
        # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –ø–æ id –∞—Ç—Ä–∏–±—É—Ç–∞–º
        phone_elements = soup.find_all(id=re.compile(r'phone|tel|contact', re.IGNORECASE))
        for element in phone_elements:
            text = element.get_text()
            phone = self._extract_phone_from_text_us(text)
            if phone:
                return phone
        
        # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –≤ –ª—é–±—ã—Ö data-* –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        for element in soup.find_all():
            for attr, value in element.attrs.items():
                if 'phone' in attr.lower() or 'tel' in attr.lower():
                    phone = self._clean_phone_us(str(value))
                    if phone:
                        return phone
        
        # –ú–µ—Ç–æ–¥ 4: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        phone_context_patterns = [
            r'(?:call|phone|tel|telephone|contact)[\s:]*([0-9\s\-\(\)\.]{10,})',
            r'(?:office|business|main)[\s:]*([0-9\s\-\(\)\.]{10,})',
            r'(?:toll\s*free|free)[\s:]*([0-9\s\-\(\)\.]{10,})',
            r'(?:fax|facsimile)[\s:]*([0-9\s\-\(\)\.]{10,})',
        ]
        
        for pattern in phone_context_patterns:
            matches = re.findall(pattern, page_text, re.IGNORECASE)
            for match in matches:
                phone = self._clean_phone_us(match)
                if phone:
                    return phone
        
        return None
    
    def _extract_email_fallback(self, page_text, soup):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è email"""
        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–∞—Ö
        contact_tags = soup.find_all(['span', 'div', 'p', 'td'], 
                                    class_=re.compile(r'contact|email|mail', re.IGNORECASE))
        for tag in contact_tags:
            text = tag.get_text()
            email = self._extract_email_from_text(text)
            if email:
                return email
        
        # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –ø–æ id –∞—Ç—Ä–∏–±—É—Ç–∞–º
        email_elements = soup.find_all(id=re.compile(r'email|mail|contact', re.IGNORECASE))
        for element in email_elements:
            text = element.get_text()
            email = self._extract_email_from_text(text)
            if email:
                return email
        
        # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –≤ –ª—é–±—ã—Ö data-* –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        for element in soup.find_all():
            for attr, value in element.attrs.items():
                if 'email' in attr.lower() or 'mail' in attr.lower():
                    if self._validate_email_global(str(value)):
                        return str(value)
        
        # –ú–µ—Ç–æ–¥ 4: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        email_context_patterns = [
            r'(?:email|mail|contact|info)[\s:]*([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,})',
            r'(?:info|contact|sales|support)[\s:]*([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,})',
            r'(?:send|write|reach)[\s\w]*[\s:]*([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,})',
        ]
        
        for pattern in email_context_patterns:
            matches = re.findall(pattern, page_text, re.IGNORECASE)
            for match in matches:
                if self._validate_email_global(match):
                    return match
        
        return None
    
    def _lightning_fast_phone(self, text):
        """–ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–Ω—Ç–∞–∫—Ç—ã —á–µ—Ä–µ–∑ regex"""
        # US phone patterns - faster regex
        patterns = [
            r'\b\(?([2-9][0-9]{2})\)?[-.\s]?([2-9][0-9]{2})[-.\s]?([0-9]{4})\b',
            r'\b1[-.\s]?\(?([2-9][0-9]{2})\)?[-.\s]?([2-9][0-9]{2})[-.\s]?([0-9]{4})\b',
            r'tel:[\s]*\+?1?[-.\s]?\(?([2-9][0-9]{2})\)?[-.\s]?([2-9][0-9]{2})[-.\s]?([0-9]{4})'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match) == 3:
                    area, exchange, number = match
                    # Quick validation
                    if area[0] not in ['0', '1'] and exchange[0] not in ['0', '1']:
                        return f"({area}) {exchange}-{number}"
        
        return None
    
    def _lightning_fast_email(self, text):
        """–ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–Ω—Ç–∞–∫—Ç—ã —á–µ—Ä–µ–∑ regex"""
        pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        matches = re.findall(pattern, text)
        
        for match in matches:
            # Skip common non-business domains
            if not any(skip in match.lower() for skip in ['example.com', 'google.com', 'facebook.com']):
                return match
        
        return None
    
    def _extract_name_fast(self, link_data, soup):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        # Try title first
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text().strip()
            if title and len(title) > 5:
                # Clean title
                name = title.split('|')[0].split('-')[0].strip()
                return name[:100]
        
        # Fallback to search result title
        return link_data.get('title', 'Unknown Business')[:100]
    
    def _extract_address_fast(self, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ —á–µ—Ä–µ–∑ regex"""
        # US address patterns
        patterns = [
            r'\b\d+\s+[A-Za-z0-9\s]+(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Boulevard|Blvd|Lane|Ln|Way|Circle|Cir|Court|Ct)\b[^,\n]*',
            r'\b\d+\s+[A-Za-z0-9\s]+(?:St|Ave|Rd|Dr|Blvd|Ln|Way|Cir|Ct)\.?\s*[,\n]'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                return matches[0][:150]
        
        return None
    
    def _extract_city_fast(self, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ —á–µ—Ä–µ–∑ regex"""
        # Look for city patterns
        pattern = r'\b([A-Za-z\s]+),\s*([A-Z]{2})\s*\d{5}'
        matches = re.findall(pattern, text)
        
        for match in matches:
            city = match[0].strip()
            if len(city) > 2 and city[0].isupper():
                return city[:50]
        
        return None
    
    def _extract_state_fast(self, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —à—Ç–∞—Ç–∞ —á–µ—Ä–µ–∑ regex"""
        # US state abbreviations
        pattern = r'\b([A-Z]{2})\s*\d{5}(?:-\d{4})?\b'
        matches = re.findall(pattern, text)
        
        us_states = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']
        
        for match in matches:
            if match in us_states:
                return match
        
        return None
    
    def _extract_materials_fast(self, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ —á–µ—Ä–µ–∑ regex"""
        text_lower = text.lower()
        materials = []
        
        # Quick material check
        material_keywords = ['copper', 'aluminum', 'steel', 'iron', 'brass', 'scrap metal']
        
        for material in material_keywords:
            if material in text_lower:
                materials.append(material)
        
        return materials if materials else None

    def _extract_phone_comprehensive(self, page_text, soup):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ (–≥–ª–æ–±–∞–ª—å–Ω–æ–µ)"""
        
        # –ú–µ—Ç–æ–¥ 1: tel: —Å—Å—ã–ª–∫–∏ (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        tel_links = soup.find_all('a', href=lambda x: x and x.startswith('tel:'))
        for link in tel_links:
            tel_value = link.get('href', '').replace('tel:', '').strip()
            phone = self._clean_phone_global(tel_value)
            if phone:
                return phone
        
        # –ú–µ—Ç–æ–¥ 2: JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                phone = self._extract_phone_from_json_ld(data)
                if phone:
                    return self._clean_phone_global(phone)
            except:
                continue
        
        # –ú–µ—Ç–æ–¥ 3: –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        microdata_elements = soup.find_all(attrs={'itemprop': True})
        for element in microdata_elements:
            itemprop = element.get('itemprop', '').lower()
            if 'telephone' in itemprop or 'phone' in itemprop:
                content = element.get('content') or element.get_text()
                phone = self._clean_phone_global(content)
                if phone:
                    return phone
        
        # –ú–µ—Ç–æ–¥ 4: data-* –∞—Ç—Ä–∏–±—É—Ç—ã
        for element in soup.find_all():
            for attr, value in element.attrs.items():
                if 'phone' in attr.lower() or 'tel' in attr.lower():
                    phone = self._clean_phone_global(str(value))
                    if phone:
                        return phone
        
        # –ú–µ—Ç–æ–¥ 5: –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        phone_containers = soup.find_all(class_=re.compile(r'phone|tel|contact|call', re.IGNORECASE))
        for container in phone_containers:
            text = container.get_text()
            phone = self._extract_phone_from_text_global(text)
            if phone:
                return phone
        
        # –ú–µ—Ç–æ–¥ 6: –ü–æ–∏—Å–∫ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –≤ —Ç–µ–∫—Å—Ç–µ
        phone = self._extract_phone_from_text_us(page_text)
        if phone:
            return phone
        
        return None

    def _extract_phone_from_text_us(self, text):
        """–ì–û–†–ê–ó–î–û –ë–û–õ–ï–ï –ê–ì–†–ï–°–°–ò–í–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return None
        
        # –û—á–µ–Ω—å –≥–∏–±–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        phone_patterns = [
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            r'\b\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})\b',
            r'\b([0-9]{3})[-.\s]+([0-9]{3})[-.\s]+([0-9]{4})\b',
            r'\b([0-9]{3})\.([0-9]{3})\.([0-9]{4})\b',
            r'\b([0-9]{3})\s([0-9]{3})\s([0-9]{4})\b',
            
            # –° –∫–æ–¥–æ–º —Å—Ç—Ä–∞–Ω—ã
            r'\b1[-.\s]?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})\b',
            
            # tel: —Å—Å—ã–ª–∫–∏
            r'tel:[\s]*\+?1?[-.\s]?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})',
            
            # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            r'(?:phone|tel|call|contact)[\s:]*\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})',
            
            # –ë–µ–∑ —Å–∫–æ–±–æ–∫ –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
            r'\b([0-9]{3})([0-9]{3})([0-9]{4})\b',
            
            # –ì–∏–±–∫–∏–π –ø–æ–∏—Å–∫ —Å –ª—é–±—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
            r'\b([0-9]{3})[^0-9]*([0-9]{3})[^0-9]*([0-9]{4})\b',
            
            # –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            r'\+1[-.\s]?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})\b',
        ]
        
        for pattern in phone_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match) == 3:
                    area_code, exchange, number = match
                    # –û—á–µ–Ω—å –º—è–≥–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
                    if (area_code != '000' and exchange != '000' and number != '0000' and
                        not (area_code == exchange == number[0] * 3)):
                        return f"({area_code}) {exchange}-{number}"
        
        return None

    def _clean_phone_us(self, phone):
        """–ì–û–†–ê–ó–î–û –ë–û–õ–ï–ï –ú–Ø–ì–ö–ê–Ø –æ—á–∏—Å—Ç–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è US —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤"""
        if not phone:
            return None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã
        digits_only = re.sub(r'\D', '', str(phone))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –¥–ª–∏–Ω—É US –Ω–æ–º–µ—Ä–∞
        if len(digits_only) == 10:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π US —Ñ–æ—Ä–º–∞—Ç
            area_code = digits_only[:3]
            exchange = digits_only[3:6]
            number = digits_only[6:]
        elif len(digits_only) == 11 and digits_only.startswith('1'):
            # US –Ω–æ–º–µ—Ä —Å –∫–æ–¥–æ–º —Å—Ç—Ä–∞–Ω—ã
            area_code = digits_only[1:4]
            exchange = digits_only[4:7]
            number = digits_only[7:]
        else:
            # –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            return None
        
        # –û—á–µ–Ω—å –º—è–≥–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è - —Ä–∞–∑—Ä–µ—à–∞–µ–º –ø–æ—á—Ç–∏ –≤—Å–µ
        # –ë–ª–æ–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —è–≤–Ω–æ –Ω–µ–≤–µ—Ä–Ω—ã–µ –Ω–æ–º–µ—Ä–∞
        if area_code == '000' or exchange == '000' or number == '0000':
            return None
        
        # –ë–ª–æ–∫–∏—Ä—É–µ–º –Ω–æ–º–µ—Ä–∞ –∏–∑ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Ü–∏—Ñ—Ä
        if area_code == exchange == number[0] * 3:
            return None
        
        # –ë–ª–æ–∫–∏—Ä—É–µ–º emergency
        if area_code + exchange + number == '9111111111':
            return None
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º US —Ñ–æ—Ä–º–∞—Ç–µ
        return f"({area_code}) {exchange}-{number}"
    
    def _clean_phone_global(self, phone):
        """–ì–ª–æ–±–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤ - –∏—Å–ø–æ–ª—å–∑—É–µ–º US –≤–∞–ª–∏–¥–∞—Ü–∏—é"""
        # For US-focused scraper, use US validation
        return self._clean_phone_us(phone)
    
    def _extract_phone_from_text_global(self, text):
        """–ì–ª–æ–±–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º US –º–µ—Ç–æ–¥—ã"""
        # For US-focused scraper, use US extraction
        return self._extract_phone_from_text_us(text)

    def _validate_us_phone(self, area_code, exchange, number):
        """–ì–û–†–ê–ó–î–û –ë–û–õ–ï–ï –ú–Ø–ì–ö–ê–Ø –≤–∞–ª–∏–¥–∞—Ü–∏—è US —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞ –¥–ª—è –±–∏–∑–Ω–µ—Å–æ–≤"""
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª–∏–Ω—ã
        if len(area_code) != 3 or len(exchange) != 3 or len(number) != 4:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—á–µ–≤–∏–¥–Ω–æ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ –Ω–æ–º–µ—Ä–∞
        if area_code == '000' or exchange == '000' or number == '0000':
            return False
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–æ–º–µ—Ä–∞ 111, 222, 333, etc (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ü–∏—Ñ—Ä—ã)
        if area_code == exchange == number[0] * 3:
            return False
        
        # –†–ê–ó–†–ï–®–ê–ï–ú toll-free –Ω–æ–º–µ—Ä–∞ - –º–Ω–æ–≥–∏–µ –±–∏–∑–Ω–µ—Å—ã –∏—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç!
        toll_free_areas = ['800', '833', '844', '855', '866', '877', '888']
        if area_code in toll_free_areas:
            return True  # Toll-free –≤—Å–µ–≥–¥–∞ –≤–∞–ª–∏–¥–Ω—ã
        
        # –†–ê–ó–†–ï–®–ê–ï–ú –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ area codes, –≤–∫–ª—é—á–∞—è 555
        # –ë–ª–æ–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —è–≤–Ω–æ –Ω–µ–≤–µ—Ä–Ω—ã–µ
        if area_code in ['111', '999']:
            return False
        
        # –û—á–µ–Ω—å –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ test –Ω–æ–º–µ—Ä–∞
        if area_code == '555' and exchange == '555' and number == '5555':
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ emergency numbers
        if area_code + exchange + number in ['9111111111']:
            return False
        
        return True

    def _extract_email_comprehensive(self, page_text, soup):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ email —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
        
        # –ú–µ—Ç–æ–¥ 1: mailto: —Å—Å—ã–ª–∫–∏ (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        mailto_links = soup.find_all('a', href=lambda x: x and x.startswith('mailto:'))
        for link in mailto_links:
            email = link.get('href', '').replace('mailto:', '').strip()
            if self._validate_email_global(email):
                return email
        
        # –ú–µ—Ç–æ–¥ 2: JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                email = self._extract_email_from_json_ld(data)
                if email and self._validate_email_global(email):
                    return email
            except:
                continue
        
        # –ú–µ—Ç–æ–¥ 3: –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        microdata_elements = soup.find_all(attrs={'itemprop': True})
        for element in microdata_elements:
            itemprop = element.get('itemprop', '').lower()
            if 'email' in itemprop:
                content = element.get('content') or element.get_text()
                if self._validate_email_global(content):
                    return content
        
        # –ú–µ—Ç–æ–¥ 4: data-* –∞—Ç—Ä–∏–±—É—Ç—ã
        for element in soup.find_all():
            for attr, value in element.attrs.items():
                if 'email' in attr.lower():
                    if self._validate_email_global(str(value)):
                        return str(value)
        
        # –ú–µ—Ç–æ–¥ 5: –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ email
        email_containers = soup.find_all(class_=re.compile(r'email|mail|contact', re.IGNORECASE))
        for container in email_containers:
            text = container.get_text()
            email = self._extract_email_from_text(text)
            if email:
                return email
        
        # –ú–µ—Ç–æ–¥ 6: –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ
        email = self._extract_email_from_text(page_text)
        if email:
            return email
        
        return None
    
    def _extract_email_from_text(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ email –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏"""
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ email
        patterns = [
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π email —Ñ–æ—Ä–º–∞—Ç
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            # Email —Å –ø—Ä–æ–±–µ–ª–∞–º–∏
            r'\b[A-Za-z0-9._%+-]+\s*@\s*[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            # Email —Å [at] –∑–∞–º–µ–Ω–æ–π
            r'\b[A-Za-z0-9._%+-]+\s*\[\s*at\s*\]\s*[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            # Email —Å (at) –∑–∞–º–µ–Ω–æ–π
            r'\b[A-Za-z0-9._%+-]+\s*\(\s*at\s*\)\s*[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            # Email —Å AT –∑–∞–º–µ–Ω–æ–π
            r'\b[A-Za-z0-9._%+-]+\s*AT\s*[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            # Email —Å —Ç–æ—á–∫–∞–º–∏ –∫–∞–∫ [dot] –∏–ª–∏ (dot)
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\s*\[\s*dot\s*\]\s*[A-Za-z]{2,}\b',
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\s*\(\s*dot\s*\)\s*[A-Za-z]{2,}\b',
            # Email —Å DOT –∑–∞–º–µ–Ω–æ–π
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\s*DOT\s*[A-Za-z]{2,}\b',
            # Email –≤ –∫–∞–≤—ã—á–∫–∞—Ö
            r'["\']([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,})["\']',
            # Email –≤ href –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
            r'mailto:([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,})',
            # Email —Å –¥–µ—Ñ–∏—Å–∞–º–∏ –≤ domain
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            # Email —Å –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è–º–∏
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9_.-]+\.[A-Z|a-z]{2,}\b',
            # Email —Å —á–∏—Å–ª–∞–º–∏ –≤ domain
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+[0-9]*\.[A-Z|a-z]{2,}\b',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # –û—á–∏—â–∞–µ–º email –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤
                email = re.sub(r'\s+', '', str(match))
                email = email.replace('[at]', '@').replace('(at)', '@')
                email = email.replace('AT', '@').replace(' AT ', '@')
                email = email.replace('[dot]', '.').replace('(dot)', '.')
                email = email.replace('DOT', '.').replace(' DOT ', '.')
                
                if self._validate_email_global(email):
                    return email
        
        return None

    def _extract_whatsapp(self, page_text, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ WhatsApp"""
        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ WhatsApp
        whatsapp_patterns = [
            re.compile(r'whatsapp\.com/send\?phone=([0-9]+)', re.IGNORECASE),
            re.compile(r'wa\.me/([0-9]+)', re.IGNORECASE),
            re.compile(r'api\.whatsapp\.com/send\?phone=([0-9]+)', re.IGNORECASE)
        ]
        
        for pattern in whatsapp_patterns:
            matches = pattern.findall(page_text)
            if matches:
                return matches[0]
        
        return None

    def _extract_social_media(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–æ—Ü—Å–µ—Ç–∏"""
        social_media = {}
        
        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–æ—Ü—Å–µ—Ç–∏
        social_patterns = {
            'facebook': r'facebook\.com/[^/\s]+',
            'twitter': r'twitter\.com/[^/\s]+',
            'instagram': r'instagram\.com/[^/\s]+',
            'linkedin': r'linkedin\.com/[^/\s]+',
            'youtube': r'youtube\.com/[^/\s]+',
            'tiktok': r'tiktok\.com/[^/\s]+',
        }
        
        page_text = str(soup)
        
        for platform, pattern in social_patterns.items():
            matches = re.findall(pattern, page_text, re.IGNORECASE)
            if matches:
                social_media[platform] = matches[0]
        
        return social_media if social_media else None

    def _extract_business_name_comprehensive(self, link_data, soup):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –±–∏–∑–Ω–µ—Å–∞"""
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è
        sources = [
            # JSON-LD
            self._extract_name_from_json_ld_comprehensive(soup),
            # Meta tags
            self._extract_name_from_meta_tags(soup),
            # Title tag
            self._extract_name_from_title(soup),
            # H1 tag
            self._extract_name_from_h1(soup),
            # Fallback - –∏–∑ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            link_data.get('title', 'Unknown Business')
        ]
        
        for source in sources:
            if source and len(source.strip()) > 2:
                # –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
                name = re.sub(r'\s+', ' ', source.strip())
                name = name.split('|')[0].split('-')[0].strip()
                return name[:150]
        
        return 'Unknown Business'

    def _extract_materials_comprehensive(self, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–Ω–∏–º–∞–µ–º—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤"""
        materials_found = []
        text_lower = page_text.lower()
        
        for material in self.materials_keywords:
            if material in text_lower:
                materials_found.append(material)
        
        return materials_found if materials_found else None

    def _extract_pricing_info_comprehensive(self, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ü–µ–Ω–∞—Ö"""
        pricing_patterns = [
            re.compile(r'\$\d+\.?\d*\s*per\s*(?:pound|lb|kilogram|kg|ton|tonne)', re.IGNORECASE),
            re.compile(r'\$\d+\.?\d*\s*/\s*(?:pound|lb|kilogram|kg|ton|tonne)', re.IGNORECASE),
            re.compile(r'(?:copper|aluminum|steel|brass|iron).*?\$\d+\.?\d*', re.IGNORECASE),
            re.compile(r'current\s*price.*?\$\d+\.?\d*', re.IGNORECASE),
            re.compile(r'scrap\s*price.*?\$\d+\.?\d*', re.IGNORECASE)
        ]
        
        pricing_info = []
        for pattern in pricing_patterns:
            matches = pattern.findall(page_text)
            pricing_info.extend(matches[:5])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        
        return pricing_info if pricing_info else None

    def _extract_working_hours_comprehensive(self, soup):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–±–æ—á–∏—Ö —á–∞—Å–æ–≤"""
        # –ü–æ–∏—Å–∫ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
        hours_selectors = [
            '[itemprop*="openingHours"]',
            '[itemprop*="hours"]',
            '.hours',
            '.opening-hours',
            '.business-hours',
            '.working-hours',
            '.schedule'
        ]
        
        for selector in hours_selectors:
            elements = soup.select(selector)
            for element in elements:
                hours_text = element.get_text(strip=True)
                if len(hours_text) > 10 and any(day in hours_text.lower() for day in ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']):
                    return hours_text[:200]
        
        return None

    def _extract_services_comprehensive(self, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —É—Å–ª—É–≥"""
        services_keywords = [
            'pickup', 'collection', 'container rental', 'roll-off', 'demolition',
            'dismantling', 'processing', 'sorting', 'weighing', 'cash payment',
            'check payment', 'commercial', 'residential', 'industrial',
            'same day', 'free estimate', 'certified scales', 'licensed',
            'insured', 'bonded', 'environmental', 'hazardous waste'
        ]
        
        services_found = []
        text_lower = page_text.lower()
        
        for service in services_keywords:
            if service in text_lower:
                services_found.append(service)
        
        return services_found if services_found else None

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    def _extract_country_comprehensive(self, soup, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã"""
        # –ü–æ–∏—Å–∫ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        country_indicators = {
            'United States': ['usa', 'us', 'united states', 'america'],
            'Canada': ['canada', 'ca'],
            'United Kingdom': ['uk', 'united kingdom', 'england', 'scotland', 'wales'],
            'Australia': ['australia', 'au'],
            'New Zealand': ['new zealand', 'nz'],
            'Ireland': ['ireland', 'ie']
        }
        
        text_lower = page_text.lower()
        
        for country, indicators in country_indicators.items():
            if any(indicator in text_lower for indicator in indicators):
                return country
        
        return 'Unknown'

    def _extract_certifications(self, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤"""
        cert_patterns = [
            re.compile(r'ISO\s*\d{4,5}', re.IGNORECASE),
            re.compile(r'certified\s+\w+', re.IGNORECASE),
            re.compile(r'licensed\s+\w+', re.IGNORECASE),
            re.compile(r'EPA\s+\w+', re.IGNORECASE),
            re.compile(r'R2\s+certified', re.IGNORECASE)
        ]
        
        certifications = []
        for pattern in cert_patterns:
            matches = pattern.findall(page_text)
            certifications.extend(matches[:3])
        
        return certifications if certifications else None

    def _extract_years_in_business(self, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–µ—Ç —Ä–∞–±–æ—Ç—ã"""
        years_patterns = [
            re.compile(r'(\d{1,2})\s*\+?\s*years?\s+(?:in\s+)?business', re.IGNORECASE),
            re.compile(r'established\s+(?:in\s+)?(\d{4})', re.IGNORECASE),
            re.compile(r'since\s+(\d{4})', re.IGNORECASE),
            re.compile(r'founded\s+(?:in\s+)?(\d{4})', re.IGNORECASE)
        ]
        
        for pattern in years_patterns:
            matches = pattern.findall(page_text)
            if matches:
                return matches[0]
        
        return None

    def _extract_languages(self, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤"""
        language_patterns = [
            re.compile(r'languages?\s*:?\s*([^.]+)', re.IGNORECASE),
            re.compile(r'speak\s+([^.]+)', re.IGNORECASE),
            re.compile(r'bilingual\s+([^.]+)', re.IGNORECASE)
        ]
        
        for pattern in language_patterns:
            matches = pattern.findall(page_text)
            if matches:
                return matches[0][:50]
        
        return None

    def _extract_payment_methods(self, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–æ–≤ –æ–ø–ª–∞—Ç—ã"""
        payment_keywords = [
            'cash', 'check', 'credit card', 'debit card', 'paypal',
            'wire transfer', 'bank transfer', 'financing', 'terms'
        ]
        
        payment_methods = []
        text_lower = page_text.lower()
        
        for method in payment_keywords:
            if method in text_lower:
                payment_methods.append(method)
        
        return payment_methods if payment_methods else None

    def _extract_additional_services(self, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ª—É–≥"""
        additional_services = []
        
        service_patterns = [
            re.compile(r'we\s+also\s+([^.]+)', re.IGNORECASE),
            re.compile(r'additionally\s+([^.]+)', re.IGNORECASE),
            re.compile(r'other\s+services\s*:?\s*([^.]+)', re.IGNORECASE)
        ]
        
        for pattern in service_patterns:
            matches = pattern.findall(page_text)
            additional_services.extend(matches)
        
        return additional_services[:3] if additional_services else None

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _extract_name_from_json_ld_comprehensive(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ JSON-LD"""
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                name = self._extract_name_from_json_ld(data)
                if name:
                    return name
            except:
                continue
        return None

    def _extract_name_from_meta_tags(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤"""
        meta_tags = [
            soup.find('meta', property='og:site_name'),
            soup.find('meta', property='og:title'),
            soup.find('meta', {'name': 'application-name'}),
            soup.find('meta', {'name': 'twitter:title'})
        ]
        
        for tag in meta_tags:
            if tag:
                content = tag.get('content', '')
                if content and len(content) > 2:
                    return content
        
        return None

    def _extract_name_from_title(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ title"""
        title_tag = soup.find('title')
        if title_tag:
            title_text = title_tag.get_text().strip()
            if title_text:
                return title_text
        return None

    def _extract_name_from_h1(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ H1"""
        h1_tag = soup.find('h1')
        if h1_tag:
            h1_text = h1_tag.get_text().strip()
            if h1_text:
                return h1_text
        return None

    def _extract_description_comprehensive(self, soup):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è"""
        # –ü–æ–∏—Å–∫ –≤ –º–µ—Ç–∞-—Ç–µ–≥–∞—Ö
        meta_descriptions = [
            soup.find('meta', {'name': 'description'}),
            soup.find('meta', property='og:description'),
            soup.find('meta', {'name': 'twitter:description'})
        ]
        
        for meta in meta_descriptions:
            if meta:
                content = meta.get('content', '')
                if content and len(content) > 20:
                    return content[:500]
        
        # –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ
        description_selectors = [
            '.description',
            '.about',
            '.overview',
            '.intro',
            '.summary'
        ]
        
        for selector in description_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text(strip=True)
                if len(text) > 50:
                    return text[:500]
        
        return None

    def _extract_address_comprehensive(self, soup, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞"""
        # JSON-LD
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                address = self._extract_address_from_json_ld(data)
                if address:
                    return address
            except:
                continue
        
        # –ú–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã–µ
        address_elements = soup.find_all(attrs={'itemprop': re.compile(r'address|street', re.IGNORECASE)})
        for element in address_elements:
            address = element.get('content') or element.get_text().strip()
            if len(address) > 10:
                return address[:200]
        
        return None

    def _extract_city_comprehensive(self, soup, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞"""
        # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥—Ä—É–≥–∏–º –º–µ—Ç–æ–¥–∞–º
        city_elements = soup.find_all(attrs={'itemprop': re.compile(r'city|locality', re.IGNORECASE)})
        for element in city_elements:
            city = element.get('content') or element.get_text().strip()
            if city and len(city) > 2:
                return city[:50]
        
        return None

    def _extract_state_comprehensive(self, soup, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —à—Ç–∞—Ç–∞"""
        state_elements = soup.find_all(attrs={'itemprop': re.compile(r'state|region', re.IGNORECASE)})
        for element in state_elements:
            state = element.get('content') or element.get_text().strip()
            if state and len(state) >= 2:
                return state[:20]
        
        return None

    def _extract_zip_comprehensive(self, soup, page_text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ—á—Ç–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        # –ü–æ–∏—Å–∫ –≤ –º–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã—Ö
        zip_elements = soup.find_all(attrs={'itemprop': re.compile(r'postal|zip', re.IGNORECASE)})
        for element in zip_elements:
            zip_code = element.get('content') or element.get_text().strip()
            if zip_code and re.match(r'^\d{5}(-\d{4})?$', zip_code):
                return zip_code
        
        # –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ
        zip_patterns = [
            re.compile(r'\b\d{5}(-\d{4})?\b'),  # US ZIP
            re.compile(r'\b[A-Z]\d[A-Z]\s*\d[A-Z]\d\b'),  # Canada postal code
            re.compile(r'\b[A-Z]{1,2}\d{1,2}\s*\d[A-Z]{2}\b')  # UK postcode
        ]
        
        for pattern in zip_patterns:
            matches = pattern.findall(page_text)
            if matches:
                return matches[0] if isinstance(matches[0], str) else matches[0][0]
        
        return None

    def _extract_latitude(self, soup, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —à–∏—Ä–æ—Ç—ã"""
        # –ü–æ–∏—Å–∫ –≤ –º–∏–∫—Ä–æ–¥–∞–Ω–Ω—ã—Ö –∏ JSON-LD
        lat_elements = soup.find_all(attrs={'itemprop': 'latitude'})
        for element in lat_elements:
            lat = element.get('content') or element.get_text().strip()
            try:
                return float(lat)
            except:
                continue
        
        return None

    def _extract_longitude(self, soup, page_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ª–≥–æ—Ç—ã"""
        lng_elements = soup.find_all(attrs={'itemprop': 'longitude'})
        for element in lng_elements:
            lng = element.get('content') or element.get_text().strip()
            try:
                return float(lng)
            except:
                continue
        
        return None

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è JSON-LD
    def _extract_phone_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            for key in ['telephone', 'phone', 'contactPoint']:
                if key in data:
                    value = data[key]
                    if isinstance(value, str):
                        return value
                    elif isinstance(value, dict) and 'telephone' in value:
                        return value['telephone']
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    result = self._extract_phone_from_json_ld(value)
                    if result:
                        return result
        
        elif isinstance(data, list):
            for item in data:
                result = self._extract_phone_from_json_ld(item)
                if result:
                    return result
        
        return None

    def _extract_email_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ email –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            for key in ['email', 'contactPoint']:
                if key in data:
                    value = data[key]
                    if isinstance(value, str) and '@' in value:
                        return value
                    elif isinstance(value, dict) and 'email' in value:
                        return value['email']
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    result = self._extract_email_from_json_ld(value)
                    if result:
                        return result
        
        elif isinstance(data, list):
            for item in data:
                result = self._extract_email_from_json_ld(item)
                if result:
                    return result
        
        return None

    def _extract_name_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            for key in ['name', 'legalName', 'alternateName']:
                if key in data and isinstance(data[key], str):
                    return data[key].strip()
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    result = self._extract_name_from_json_ld(value)
                    if result:
                        return result
        
        elif isinstance(data, list):
            for item in data:
                result = self._extract_name_from_json_ld(item)
                if result:
                    return result
        
        return None

    def _extract_address_from_json_ld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ –∏–∑ JSON-LD"""
        if isinstance(data, dict):
            if 'address' in data:
                addr = data['address']
                if isinstance(addr, str):
                    return addr
                elif isinstance(addr, dict):
                    parts = []
                    for key in ['streetAddress', 'addressLocality', 'addressRegion', 'postalCode']:
                        if key in addr and addr[key]:
                            parts.append(str(addr[key]))
                    if parts:
                        return ', '.join(parts)
            
            for value in data.values():
                if isinstance(value, (dict, list)):
                    result = self._extract_address_from_json_ld(value)
                    if result:
                        return result
        
        elif isinstance(data, list):
            for item in data:
                result = self._extract_address_from_json_ld(item)
                if result:
                    return result
        
        return None

    def _validate_email_global(self, email):
        """–ì–û–†–ê–ó–î–û –ë–û–õ–ï–ï –ú–Ø–ì–ö–ê–Ø –≤–∞–ª–∏–¥–∞—Ü–∏—è email –¥–ª—è –±–∏–∑–Ω–µ—Å–æ–≤"""
        if not email or '@' not in email:
            return False
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        email = email.strip()
        
        # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ç–æ—á–∫–∏ –≤ –¥–æ–º–µ–Ω–µ
        if '.' not in email.split('@')[1]:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—á–µ–≤–∏–¥–Ω–æ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–æ–º–µ–Ω—ã
        exclude_domains = [
            'example.com', 'test.com', 'domain.com', 'sample.com',
            'your-domain.com', 'yourdomain.com', 'yoursite.com'
        ]
        
        email_lower = email.lower()
        for domain in exclude_domains:
            if domain in email_lower:
                return False
        
        # –û—á–µ–Ω—å –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ - —Ä–∞–∑—Ä–µ—à–∞–µ–º –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ñ–æ—Ä–º–∞—Ç–æ–≤
        # –ü—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ @ –∏ —Ç–æ—á–∫–∏
        parts = email.split('@')
        if len(parts) != 2:
            return False
        
        local_part, domain_part = parts
        
        # –õ–æ–∫–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—É—Å—Ç–æ–π
        if not local_part:
            return False
        
        # –î–æ–º–µ–Ω –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É —Ç–æ—á–∫—É
        if '.' not in domain_part:
            return False
        
        # –î–æ–º–µ–Ω –Ω–µ –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è –∏–ª–∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å—Å—è —Ç–æ—á–∫–æ–π
        if domain_part.startswith('.') or domain_part.endswith('.'):
            return False
        
        return True

    def _calculate_data_completeness(self, business):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö"""
        important_fields = [
            'name', 'phone', 'email', 'website', 'address',
            'city', 'state', 'country', 'working_hours',
            'materials_accepted', 'services', 'description'
        ]
        
        filled_fields = 0
        for field in important_fields:
            value = business.get(field)
            if value and (not isinstance(value, list) or len(value) > 0):
                filled_fields += 1
        
        return int((filled_fields / len(important_fields)) * 100)

    def _is_valid_url(self, url):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL"""
        try:
            parsed = urlparse(url)
            return parsed.scheme in ['http', 'https'] and parsed.netloc
        except:
            return False

    def _deduplicate_links(self, links):
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Å—ã–ª–æ–∫"""
        seen_urls = set()
        unique_links = []
        
        for link in links:
            url = link['url']
            if url not in seen_urls:
                seen_urls.add(url)
                unique_links.append(link)
        
        return unique_links

    def _finalize_comprehensive_results(self, businesses, target_count):
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É –∏ email
        seen_contacts = set()
        unique_businesses = []
        
        for business in businesses:
            phone = business.get('phone', '')
            email = business.get('email', '')
            
            contact_key = f"{phone}|{email}"
            if contact_key not in seen_contacts:
                seen_contacts.add(contact_key)
                unique_businesses.append(business)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ–ª–Ω–æ—Ç–µ –¥–∞–Ω–Ω—ã—Ö
        unique_businesses.sort(key=lambda x: x.get('data_completeness', 0), reverse=True)
        
        return unique_businesses[:target_count]

    def _calculate_contact_percentage(self):
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤"""
        if not self.results:
            return 0
        
        with_contacts = sum(1 for business in self.results 
                           if business.get('phone') or business.get('email'))
        return (with_contacts / len(self.results)) * 100

    def export_comprehensive_results(self, output_dir="output"):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±—ã—Å—Ç—Ä–æ–≥–æ —Å–±–æ—Ä–∞"""
        if not self.results:
            self.logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return None
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # DataFrame
        df = pd.DataFrame(self.results)
        
        # CSV
        csv_file = os.path.join(output_dir, f"fast_metal_businesses_{timestamp}.csv")
        df.to_csv(csv_file, index=False)
        
        # Excel —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –ª–∏—Å—Ç–∞–º–∏
        excel_file = os.path.join(output_dir, f"fast_metal_businesses_{timestamp}.xlsx")
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            # –ì–ª–∞–≤–Ω—ã–π –ª–∏—Å—Ç
            df.to_excel(writer, sheet_name='All Businesses', index=False)
            
            # –õ–∏—Å—Ç —Å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            if 'data_completeness' in df.columns:
                high_quality = df[df['data_completeness'] >= 70]
                if not high_quality.empty:
                    high_quality.to_excel(writer, sheet_name='High Quality Data', index=False)
            
            # –õ–∏—Å—Ç —Å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            contact_columns = ['name', 'phone', 'email', 'website', 'address', 'city', 'state', 'country']
            available_columns = [col for col in contact_columns if col in df.columns]
            if available_columns:
                contacts_df = df[available_columns]
                contacts_df.to_excel(writer, sheet_name='Contact Information', index=False)
            
            # –õ–∏—Å—Ç —Å –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏
            if 'materials_accepted' in df.columns:
                materials_columns = ['name', 'materials_accepted', 'phone', 'email']
                available_mat_columns = [col for col in materials_columns if col in df.columns]
                if available_mat_columns:
                    materials_df = df[available_mat_columns]
                    materials_df.to_excel(writer, sheet_name='Materials', index=False)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats_data = self._create_fast_statistics()
            stats_df = pd.DataFrame(stats_data)
            stats_df.to_excel(writer, sheet_name='Statistics', index=False)
        
        # JSON
        json_file = os.path.join(output_dir, f"fast_metal_businesses_{timestamp}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, default=str, ensure_ascii=False)
        
        # –ë—ã—Å—Ç—Ä—ã–π –æ—Ç—á–µ—Ç
        report_file = self._create_fast_report(output_dir, timestamp)
        
        self.logger.info(f"‚úÖ –ë–´–°–¢–†–´–ï –¥–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã:")
        self.logger.info(f"  ‚Ä¢ CSV: {csv_file}")
        self.logger.info(f"  ‚Ä¢ Excel: {excel_file}")
        self.logger.info(f"  ‚Ä¢ JSON: {json_file}")
        self.logger.info(f"  ‚Ä¢ –û—Ç—á–µ—Ç: {report_file}")
        
        return {
            'csv': csv_file,
            'excel': excel_file,
            'json': json_file,
            'report': report_file,
            'count': len(self.results)
        }

    def _create_fast_statistics(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±—ã—Å—Ç—Ä–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if not self.results:
            return []
        
        total = len(self.results)
        
        stats = [
            {'Metric': 'Total Businesses', 'Count': total, 'Percentage': '100.0%'},
            {'Metric': 'With Phone Numbers', 'Count': sum(1 for b in self.results if b.get('phone')), 'Percentage': f"{sum(1 for b in self.results if b.get('phone'))/total*100:.1f}%"},
            {'Metric': 'With Email Addresses', 'Count': sum(1 for b in self.results if b.get('email')), 'Percentage': f"{sum(1 for b in self.results if b.get('email'))/total*100:.1f}%"},
            {'Metric': 'With Complete Address', 'Count': sum(1 for b in self.results if b.get('address')), 'Percentage': f"{sum(1 for b in self.results if b.get('address'))/total*100:.1f}%"},
            {'Metric': 'With Materials Info', 'Count': sum(1 for b in self.results if b.get('materials_accepted')), 'Percentage': f"{sum(1 for b in self.results if b.get('materials_accepted'))/total*100:.1f}%"},
            {'Metric': 'High Quality Data (>70%)', 'Count': sum(1 for b in self.results if b.get('data_completeness', 0) > 70), 'Percentage': f"{sum(1 for b in self.results if b.get('data_completeness', 0) > 70)/total*100:.1f}%"},
        ]
        
        return stats

    def _create_fast_report(self, output_dir, timestamp):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        report_file = os.path.join(output_dir, f"fast_report_{timestamp}.txt")
        
        total_businesses = len(self.results)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats = self._create_fast_statistics()
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ —à—Ç–∞—Ç–∞–º
        states = {}
        for business in self.results:
            state = business.get('state', 'Unknown')
            states[state] = states.get(state, 0) + 1
        
        # –¢–æ–ø –º–∞—Ç–µ—Ä–∏–∞–ª—ã
        all_materials = []
        for business in self.results:
            materials = business.get('materials_accepted', [])
            if materials:
                all_materials.extend(materials)
        
        material_counts = {}
        for material in all_materials:
            material_counts[material] = material_counts.get(material, 0) + 1
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("üöÄ –ë–´–°–¢–†–´–ô US SCRAP METAL –û–¢–ß–ï–¢\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–ú–µ—Ç–æ–¥ —Å–±–æ—Ä–∞: –ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫\n")
            f.write(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: ~2-5 –º–∏–Ω—É—Ç\n\n")
            
            f.write("üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n")
            f.write("-" * 30 + "\n")
            for stat in stats:
                f.write(f"{stat['Metric']}: {stat['Count']} ({stat['Percentage']})\n")
            f.write("\n")
            
            f.write("üá∫üá∏ –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –®–¢–ê–¢–ê–ú\n")
            f.write("-" * 35 + "\n")
            for state, count in sorted(states.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_businesses) * 100
                f.write(f"{state}: {count} –±–∏–∑–Ω–µ—Å–æ–≤ ({percentage:.1f}%)\n")
            f.write("\n")
            
            if material_counts:
                f.write("üîß –ü–û–ü–£–õ–Ø–†–ù–´–ï –ú–ê–¢–ï–†–ò–ê–õ–´\n")
                f.write("-" * 25 + "\n")
                top_materials = sorted(material_counts.items(), key=lambda x: x[1], reverse=True)[:10]
                for material, count in top_materials:
                    f.write(f"{material}: {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π\n")
                f.write("\n")
            
            f.write("üéØ –ö–õ–Æ–ß–ï–í–´–ï –î–û–°–¢–ò–ñ–ï–ù–ò–Ø\n")
            f.write("-" * 25 + "\n")
            avg_completeness = sum(b.get('data_completeness', 0) for b in self.results) / total_businesses
            f.write(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –ø–æ–ª–Ω–æ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö: {avg_completeness:.1f}%\n")
            f.write(f"‚Ä¢ US –æ—Ö–≤–∞—Ç: {len(states)} —à—Ç–∞—Ç–æ–≤\n")
            f.write(f"‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è\n")
            f.write(f"‚Ä¢ –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: 100% –ø–æ–∫—Ä—ã—Ç–∏–µ\n")
            f.write(f"‚Ä¢ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: 16 –ø–æ—Ç–æ–∫–æ–≤\n\n")
            
            f.write("üöÄ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø OUTREACH\n")
            f.write("-" * 30 + "\n")
            f.write("1. –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–∏–∑–Ω–µ—Å—ã —Å –≤—ã—Å–æ–∫–æ–π –ø–æ–ª–Ω–æ—Ç–æ–π –¥–∞–Ω–Ω—ã—Ö (>70%)\n")
            f.write("2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã –¥–ª—è –ø—Ä—è–º–æ–≥–æ –æ–±—â–µ–Ω–∏—è\n")
            f.write("3. Email-—Ä–∞—Å—Å—ã–ª–∫–∏ –¥–ª—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ—Ö–≤–∞—Ç–∞\n")
            f.write("4. –§–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö\n")
            f.write("5. –£—á–∏—Ç—ã–≤–∞—Ç—å —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏\n")
            f.write("6. –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç –∫–∞–º–ø–∞–Ω–∏–∏ —Å –≥–æ—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n")
        
        return report_file

    def _calculate_quick_completeness(self, phone, email, page_text):
        """–ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö"""
        score = 0
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã (60% –≤–µ—Å–∞)
        if phone:
            score += 30
        if email:
            score += 30
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (40% –≤–µ—Å–∞)
        text_lower = page_text.lower()
        
        # –ê–¥—Ä–µ—Å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        if any(word in text_lower for word in ['address', 'street', 'ave', 'blvd', 'rd']):
            score += 10
        
        # –†–∞–±–æ—á–∏–µ —á–∞—Å—ã
        if any(word in text_lower for word in ['hours', 'open', 'closed', 'monday', 'tuesday']):
            score += 10
        
        # –ú–∞—Ç–µ—Ä–∏–∞–ª—ã
        if any(material in text_lower for material in ['copper', 'aluminum', 'steel', 'metal', 'scrap']):
            score += 10
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã
        if any(word in text_lower for word in ['whatsapp', 'facebook', 'instagram', 'twitter']):
            score += 10
        
        return min(score, 100)  # –ú–∞–∫—Å–∏–º—É–º 100%

    def _extract_phone_aggressive(self, page_text, soup):
        """–ê–ì–†–ï–°–°–ò–í–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –æ—Ö–≤–∞—Ç"""
        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        phone = self._extract_phone_from_text_us(page_text)
        if phone:
            return phone
        
        # –ú–µ—Ç–æ–¥ 2: tel: —Å—Å—ã–ª–∫–∏
        tel_links = soup.find_all('a', href=lambda x: x and x.startswith('tel:'))
        for link in tel_links:
            tel_value = link.get('href', '').replace('tel:', '').strip()
            phone = self._clean_phone_us(tel_value)
            if phone:
                return phone
        
        # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –≤ –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        phone_attrs = soup.find_all(attrs=lambda x: x and any('phone' in str(attr).lower() or 'tel' in str(attr).lower() for attr in x))
        for element in phone_attrs:
            for attr, value in element.attrs.items():
                if 'phone' in attr.lower() or 'tel' in attr.lower():
                    phone = self._clean_phone_us(str(value))
                    if phone:
                        return phone
        
        # –ú–µ—Ç–æ–¥ 4: –ü–æ–∏—Å–∫ –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
        phone_elements = soup.find_all(['span', 'div', 'p'], class_=lambda x: x and any(keyword in str(x).lower() for keyword in ['phone', 'tel', 'contact']))
        for element in phone_elements:
            text = element.get_text()
            phone = self._extract_phone_from_text_us(text)
            if phone:
                return phone
        
        return None
    
    def _extract_email_aggressive(self, page_text, soup):
        """–ê–ì–†–ï–°–°–ò–í–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ email - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –æ—Ö–≤–∞—Ç"""
        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        email = self._extract_email_from_text(page_text)
        if email:
            return email
        
        # –ú–µ—Ç–æ–¥ 2: mailto: —Å—Å—ã–ª–∫–∏
        mailto_links = soup.find_all('a', href=lambda x: x and x.startswith('mailto:'))
        for link in mailto_links:
            email = link.get('href', '').replace('mailto:', '').strip()
            if self._validate_email_global(email):
                return email
        
        # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –≤ –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        email_attrs = soup.find_all(attrs=lambda x: x and any('email' in str(attr).lower() or 'mail' in str(attr).lower() for attr in x))
        for element in email_attrs:
            for attr, value in element.attrs.items():
                if 'email' in attr.lower() or 'mail' in attr.lower():
                    if self._validate_email_global(str(value)):
                        return str(value)
        
        # –ú–µ—Ç–æ–¥ 4: –ü–æ–∏—Å–∫ –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
        email_elements = soup.find_all(['span', 'div', 'p'], class_=lambda x: x and any(keyword in str(x).lower() for keyword in ['email', 'mail', 'contact']))
        for element in email_elements:
            text = element.get_text()
            email = self._extract_email_from_text(text)
            if email:
                return email
        
        return None
    
    def _is_relevant_to_industry(self, page_text, link_data):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ metal/scrap industry"""
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è metal/scrap industry
        keywords = [
            'scrap', 'metal', 'recycling', 'iron', 'steel', 'aluminum', 'copper', 'brass',
            'salvage', 'junk', 'auto parts', 'demolition', 'waste', 'materials',
            'alloy', 'bronze', 'lead', 'zinc', 'titanium', 'stainless',
            'yard', 'dealer', 'buyer', 'processing', 'facility'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º title –∏–∑ Google —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        title = link_data.get('title', '').lower()
        if any(keyword in title for keyword in keywords):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        text_lower = page_text.lower()
        found_keywords = sum(1 for keyword in keywords if keyword in text_lower)
        
        # –¢—Ä–µ–±—É–µ–º –º–∏–Ω–∏–º—É–º 2 —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        return found_keywords >= 2

def main():
    print("üéØ –¢–û–ß–ù–´–ô US SCRAP METAL –ü–ê–†–°–ï–† - –ù–ê–ô–î–ï–¢ –ò–ú–ï–ù–ù–û –°–¢–û–õ–¨–ö–û, –°–ö–û–õ–¨–ö–û –ù–£–ñ–ù–û")
    print("=" * 80)
    print("üèÜ –ì–ê–†–ê–ù–¢–ò–†–û–í–ê–ù–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢ - –¢–û–ß–ù–û–ï –ö–û–õ–ò–ß–ï–°–¢–í–û")
    print("üá∫üá∏ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ô –û–•–í–ê–¢: 50 –õ–û–ö–ê–¶–ò–ô √ó 20 –ó–ê–ü–†–û–°–û–í")
    print("üî• 16 –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–• –ü–û–¢–û–ö–û–í + –ê–î–ê–ü–¢–ò–í–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê")
    print("üìû –ö–û–ú–ü–õ–ï–ö–°–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï: 6 –ú–ï–¢–û–î–û–í –¢–ï–õ–ï–§–û–ù–û–í + 6 –ú–ï–¢–û–î–û–í EMAIL")
    print("üéØ –£–ú–ù–ê–Ø –°–ò–°–¢–ï–ú–ê: –ü–†–û–î–û–õ–ñ–ê–ï–¢ –î–û –î–û–°–¢–ò–ñ–ï–ù–ò–Ø –¶–ï–õ–ò")
    print("üí™ –û–ë–†–ê–ë–ê–¢–´–í–ê–ï–¢ –î–û 2000+ –°–°–´–õ–û–ö –î–õ–Ø –ì–ê–†–ê–ù–¢–ò–ò")
    print("‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢: –ù–ê–ô–î–ï–¢ –¢–û–ß–ù–û –ó–ê–ü–†–û–®–ï–ù–ù–û–ï –ö–û–õ–ò–ß–ï–°–¢–í–û")
    
    scraper = USMetalScraper()
    
    try:
        target_count = input("\\n–°–∫–æ–ª—å–∫–æ –±–∏–∑–Ω–µ—Å–æ–≤ –Ω–∞–π—Ç–∏? (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 200): ").strip()
        target_count = int(target_count) if target_count else 200
        
        print(f"\\nüéØ –ó–∞–ø—É—Å–∫ –¢–û–ß–ù–û–ì–û –ø–æ–∏—Å–∫–∞ {target_count} –±–∏–∑–Ω–µ—Å–æ–≤...")
        print("üá∫üá∏ –û—Ö–≤–∞—Ç: –°–®–ê (50 —Ç–æ–ø-–ª–æ–∫–∞—Ü–∏–π —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º)")
        print("‚ö° –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è: –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")
        print("üìã –ú–µ—Ç–æ–¥—ã: 6 —Å–ø–æ—Å–æ–±–æ–≤ –ø–æ–∏—Å–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ + 6 —Å–ø–æ—Å–æ–±–æ–≤ –ø–æ–∏—Å–∫–∞ email")
        print("üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –°—Ç—Ä–∞–Ω–∏—Ü—ã 2-5 (–Ω–∏–∑–∫–∏–µ –ø–æ–∑–∏—Ü–∏–∏, –±–æ–ª—å—à–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π)")
        print("üí° –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: 8-20 –º–∏–Ω—É—Ç (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ü–µ–ª–∏)")
        print("üèÜ –ì–∞—Ä–∞–Ω—Ç–∏—è: –ù–∞–π–¥–µ—Ç –¢–û–ß–ù–û —É–∫–∞–∑–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏–∑–Ω–µ—Å–æ–≤")
        
        confirm = input("\\nüöÄ –ù–∞—á–∞—Ç—å —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫? (y/N): ").lower().strip()
        if confirm != 'y':
            print("‚ùå –ü–æ–∏—Å–∫ –æ—Ç–º–µ–Ω–µ–Ω")
            return
        
        # –ó–∞–ø—É—Å–∫ —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        results = scraper.run_comprehensive_scraping(target_count)
        
        if results and len(results) >= target_count:
            print(f"\\nüèÜ –ú–ò–°–°–ò–Ø –í–´–ü–û–õ–ù–ï–ù–ê –£–°–ü–ï–®–ù–û!")
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ –±–∏–∑–Ω–µ—Å–æ–≤: {len(results)} (–¢–û–ß–ù–û –∫–∞–∫ –∑–∞–ø—Ä–æ—à–µ–Ω–æ)")
            print(f"üìû –ü—Ä–æ—Ü–µ–Ω—Ç —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏: {scraper._calculate_contact_percentage():.1f}%")
            
            # –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            print(f"\\nüìÅ –≠–∫—Å–ø–æ—Ä—Ç —Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            output_info = scraper.export_comprehensive_results()
            
            if output_info:
                print(f"\\nüéâ –¢–û–ß–ù–´–ï –î–ê–ù–ù–´–ï –≠–ö–°–ü–û–†–¢–ò–†–û–í–ê–ù–´:")
                print(f"üìÑ –í—Å–µ —Ñ–∞–π–ª—ã –≥–æ—Ç–æ–≤—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
                print(f"üöÄ {len(results)} –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –±–∏–∑–Ω–µ—Å–æ–≤ –≥–æ—Ç–æ–≤—ã –¥–ª—è outreach!")
                print(f"\\nüìã –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
                print(f"  ‚Ä¢ CSV: {output_info.get('csv', 'N/A')}")
                print(f"  ‚Ä¢ Excel: {output_info.get('excel', 'N/A')}")
                print(f"  ‚Ä¢ JSON: {output_info.get('json', 'N/A')}")
                print(f"  ‚Ä¢ –û—Ç—á–µ—Ç: {output_info.get('report', 'N/A')}")
                
                print(f"\\nüíé –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•:")
                print(f"  ‚Ä¢ 100% –±–∏–∑–Ω–µ—Å–æ–≤ –∏–º–µ—é—Ç –∫–æ–Ω—Ç–∞–∫—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é")
                print(f"  ‚Ä¢ –ü—Ä–æ–≤–µ—Ä–µ–Ω—ã US —Ç–µ–ª–µ—Ñ–æ–Ω—ã —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π")
                print(f"  ‚Ä¢ –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
                print(f"  ‚Ä¢ –ì–æ—Ç–æ–≤—ã –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
            else:
                print("\\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –¥–∞–Ω–Ω—ã—Ö")
                
        elif results and len(results) < target_count:
            print(f"\\n‚ö†Ô∏è –ß–ê–°–¢–ò–ß–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ –±–∏–∑–Ω–µ—Å–æ–≤: {len(results)} –∏–∑ {target_count} –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã—Ö")
            print(f"üìû –ü—Ä–æ—Ü–µ–Ω—Ç —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏: {scraper._calculate_contact_percentage():.1f}%")
            print(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–∏–∑–∏—Ç—å —Ü–µ–ª—å –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø–æ–∏—Å–∫")
            
            # –≠–∫—Å–ø–æ—Ä—Ç —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            output_info = scraper.export_comprehensive_results()
            if output_info:
                print(f"\\nüìÅ –ß–∞—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã")
        else:
            print("\\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–∏–∑–Ω–µ—Å–æ–≤")
            print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–∏–∑–∏—Ç—å —Ü–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ")
            
    except KeyboardInterrupt:
        print("\\n‚èπÔ∏è  –ü–æ–∏—Å–∫ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        if scraper.results:
            print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
            scraper.export_comprehensive_results()
    except Exception as e:
        print(f"\\n‚ùå –û—à–∏–±–∫–∞: {e}")
        scraper.logger.error(f"Main error: {e}")
    
    print("\\n" + "=" * 80)
    print("üîß –î–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫—É")
    print("üìà –£–¥–∞—á–Ω–æ–≥–æ outreach —Å —Ç–æ—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏!")

if __name__ == "__main__":
    main()